{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c176586",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Covid-19 and the environment, a statistics case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222e4cd",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to discover potential links between the awareness of the environment and the actual state of the environment around the time of the Covid-19 lockdowns.\n",
    "- First, we explore the dataset that was given in the problem statement. We merge, filter and prepare the data for further use.\n",
    "- Then, we present the dataset which we introduced ourselves : more data from Wikipedia (nicknamed \"precise wikipedia) the World Air Quality index and the plastics production statistics from Eurostat.\n",
    "- Then, we analyze the links between wikipedia views with indicator of actual pollution.\n",
    "- Finally, we present a statistical extrapolation of environment data without Covid and compare it to the actual data, to give an idea of what might have happened if Covid hadn't happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d4fb1-9123-44be-bcc8-3043635e6868",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 1 : Coronawiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c7ffe-9e3a-4a1f-9a15-c63034f2b882",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8021f48-467c-4d2a-a3a6-de52eda4ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "from statsmodels.stats import diagnostic\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib.pyplot import figure\n",
    "from dtw import dtw,accelerated_dtw\n",
    "from statsmodels.stats import diagnostic\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "np.random.seed(0)\n",
    "np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d8423-6272-48f8-83fc-19efe72ba8e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82927c-6fa8-4549-8b6c-64e21312b8e3",
   "metadata": {},
   "source": [
    "In this data, each language is mapped to the main country of usage except for English, where the language's usage is very high in multiple countries such that it couldn't be mapped to a single country. As such, for that language, we have most of the data missing.\n",
    "\n",
    "Per country, the pandemic timeline is represented, such as the first registered case, the first death, etc.\n",
    "\n",
    "Note that the paper says that nine languages are spoken in a single language, but we have more than that here (reason for that unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc9a28-881c-431b-84a6-c8523df79edb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "interventions = pd.read_csv(\"Data/interventions.csv\")\n",
    "\n",
    "\n",
    "def transform_column(column_name):\n",
    "    interventions[column_name] = pd.to_datetime(interventions[column_name])\n",
    "\n",
    "\n",
    "for intervention in interventions.columns[1:]:\n",
    "    transform_column(intervention)\n",
    "interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03366e12-a539-407d-9272-bbda666a0152",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Timeseries <a id='timeseries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb90ba-e541-4c6b-919a-62ae6720776e",
   "metadata": {},
   "source": [
    "The most important data we have in this dataset are time series of the Wikipedia views from 2018 to July 2020 for 14 different languages: one part are the total views for all of that language's wikipedia, a second part are the views for the articles that are related to Covid-19, as well as the percentage. Finally, we also have for the same window of time the views for different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac436809-910e-4a33-aa74-8b22ef376770",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = pd.read_json(\"Data/aggregated_timeseries.json.gz\")\n",
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aaa249-b005-43fa-b4f9-1b0189e06555",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f18e28-1959-418a-89e2-d98c6c3fddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_unique = timeseries.columns.map(lambda x: x[:2]).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ace040-710a-40d8-88a1-2c0ebfa74232",
   "metadata": {},
   "source": [
    "Correspondence:\n",
    "- ja -> Japanese\n",
    "- it -> Italian\n",
    "- da -> Danish\n",
    "- tr -> Turkish\n",
    "- no -> Norwegian\n",
    "- en -> English\n",
    "- sr -> Serbian\n",
    "- sv -> Swedish\n",
    "- nl -> Dutch\n",
    "- de -> German\n",
    "- fr -> French\n",
    "- ca -> Catalan\n",
    "- ko -> Korean\n",
    "- fi -> Finnish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8e3c0-e431-4e73-b8ff-7d70e45972bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Splitting the timeseries data into different dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27950baf-10d1-43b1-b5fc-20fc1963b0a7",
   "metadata": {},
   "source": [
    "As we can see, the data's format isn't ideal: for each language, the data is split into 3 Python dictionaries corresponding to the data described above, and it would be nice to separate these pieces of data to be able to read directly for each date, for example, the total number of views accross all languages, instead of having to iterate over each language's dictionnary every time.\n",
    "\n",
    "This will also make the analysis phase easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542af03f-895d-46d1-b7bb-61732030491f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Total sum of views, views of articles related to Covid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9ffea-c295-44b0-8cf0-63512a1c27bf",
   "metadata": {},
   "source": [
    "<a id='extractionformat'></a>\n",
    "In this part of the code we extract two following kind of data, for each date:\n",
    "- For every language's Wikipedia, the total number of views on that particular date\n",
    "- For every language's Wikipedia, the total number of views for articles related to Covid-19 on that particular date\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Every resulting dataframe will have the following format:\n",
    "\n",
    "| Column name          | Description                                                                                                                                                                                       |   |   |   |\n",
    "|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|---|---|\n",
    "| date           | A particular date between January 2018 (inclusive) and July of 2020 (inclusive)                                                                                                                         |   |   |   |                    \n",
    "| language       | The corresponding wikipedia language                                                                |   |   |   |\n",
    "| views  | The number of views (total or only for articles related to Covid-19) for this language and date\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We also extract another dataframe that simply maps for each language the number of articles that were considered in the original experiment. Finally, a last dataframe which is redundant with the two first ones is present: one which gives for each day the percentage of views that went to Covid related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf9536-1647-485f-96a5-74a895de9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_total_sum_dict = {}\n",
    "timeseries_covid_len_dict = {}\n",
    "timeseries_covid_sum_dict = {}\n",
    "timeseries_covid_percent_dict = {}\n",
    "for cn in timeseries.columns:\n",
    "    timeseries_total_sum_dict[cn] = timeseries[cn][\"sum\"]\n",
    "    timeseries_covid_len_dict[cn] = timeseries[cn][\"covid\"][\"len\"]\n",
    "    timeseries_covid_sum_dict[cn] = timeseries[cn][\"covid\"][\"sum\"]\n",
    "    timeseries_covid_percent_dict[cn] = timeseries[cn][\"covid\"][\"percent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ea2ab-bd0c-4547-973e-aacb1461f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df = pd.DataFrame.from_dict(timeseries_total_sum_dict, orient=\"index\").T\n",
    "covid_len_data_df = pd.DataFrame.from_dict(\n",
    "    timeseries_covid_len_dict, orient=\"index\", columns=[\"len\"]\n",
    ").T\n",
    "covid_sum_data_df = pd.DataFrame.from_dict(timeseries_covid_sum_dict, orient=\"index\").T\n",
    "covid_percent_data_df = pd.DataFrame.from_dict(\n",
    "    timeseries_covid_percent_dict, orient=\"index\"\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd46995-1d02-4a56-9554-70af621979a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse the .m and the normal columns together, and separate the data\n",
    "new_sum_data_df = pd.DataFrame()\n",
    "new_covid_sum_data_df = pd.DataFrame()\n",
    "for language in languages_unique:\n",
    "    country_sum_data = pd.DataFrame()\n",
    "    country_sum_data[\"views\"] = sum_data_df[language] + sum_data_df[language + \".m\"]\n",
    "    country_sum_data[\"language\"] = language\n",
    "    new_sum_data_df = pd.concat([new_sum_data_df, country_sum_data], axis=0)\n",
    "\n",
    "    country_covid_sum_data = pd.DataFrame()\n",
    "    country_covid_sum_data[\"views\"] = (\n",
    "        covid_sum_data_df[language] + covid_sum_data_df[language + \".m\"]\n",
    "    )\n",
    "    country_covid_sum_data[\"language\"] = language\n",
    "    new_covid_sum_data_df = pd.concat(\n",
    "        [new_covid_sum_data_df, country_covid_sum_data], axis=0\n",
    "    )\n",
    "\n",
    "sum_data_df = new_sum_data_df\n",
    "covid_sum_data_df = new_covid_sum_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3c28d-9212-4216-a0e2-6a2348cd73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.index = covid_sum_data_df.index = pd.to_datetime(sum_data_df.index)\n",
    "covid_percent_data_df.index = pd.to_datetime(covid_percent_data_df.index)\n",
    "sum_data_df[\"date\"] = covid_sum_data_df[\"date\"] = sum_data_df.index\n",
    "covid_percent_data_df[\"date\"] = covid_percent_data_df.index\n",
    "# covid_sum_data_df = covid_sum_data_df[new_column_order]\n",
    "# covid_percent_data_df = covid_percent_data_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89733873-085e-47ad-899e-da398e509ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4468b5-4221-43cb-95b8-691178ee630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_percent_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7aa174-d5f8-4469-876f-af5a4054541f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Checking for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ed79e-fcdc-4444-b9ff-62d7f95326ab",
   "metadata": {},
   "source": [
    "Before continuing further, let us check for missing data in the timeseries; this will help us avoid bad surprises later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08f737-22a1-43f2-a0a5-e2ee043a5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.isnull().any().any(), covid_sum_data_df.isnull().any().any(), covid_percent_data_df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd509e-308f-447d-bb84-8f93fa212d31",
   "metadata": {},
   "source": [
    "There appears to be some missing data in the percentage dataframe; let's check by language where that missing data is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049c1eb-bb14-43e8-bfe8-7d1b6667493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_language = covid_percent_data_df.isnull().any(axis=0)\n",
    "missing_data_language[missing_data_language]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2634257-7894-42fe-8a39-a026461bc3ee",
   "metadata": {},
   "source": [
    "Looking at the paper, this corresponds to Swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffd134-d643-4336-84e3-15de9fd336bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomment Run this cell if you want to see how the missing data can be seen in the original timeseries\n",
    "# timeseries.loc[:,'sv']['sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c46177-cc59-4d6d-b8db-8ffc2d81a3d4",
   "metadata": {},
   "source": [
    "It appears that, for desktop devices, the views for the Swedish Wikipedia haven't been collected for the whole year 2018. The reason for that is unknown, as a quick search tells us that this version has existed since 2001. The mobile data, however, is available. We will need to take this into account when doing our analysis in the future, for example by not taking into account 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8164a0-ad9d-42f1-913c-c2ce1fb1f9d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Topics data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aebf05-3d59-4b91-a892-af40b76e1654",
   "metadata": {},
   "source": [
    "Now we will extract for each language, all the views per topic in such a way that the data becomes more usable. In the original data, all topic-related information was in a single dictionnary; we're gonna separate them in a way that each column will correspond to a different topic, with each row being a different language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5379a73c-8eba-4be8-9fcc-408dd902529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_topics = {}\n",
    "for cn in timeseries.columns:\n",
    "    country_to_topics[cn] = timeseries[cn][\"topics\"]\n",
    "topics_df = pd.DataFrame.from_dict(country_to_topics, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9c567-d699-4f00-98ca-119fb05e21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_topics_len = {}\n",
    "countries_to_topics_sum = {}\n",
    "countries_to_topics_percent = {}\n",
    "for country in topics_df.index:\n",
    "    countries_to_topics_len[country] = {}\n",
    "    countries_to_topics_sum[country] = {}\n",
    "    countries_to_topics_percent[country] = {}\n",
    "    for topic in topics_df.columns:\n",
    "        countries_to_topics_len[country][topic] = topics_df.loc[country, topic][\"len\"]\n",
    "        countries_to_topics_sum[country][topic] = topics_df.loc[country, topic][\"sum\"]\n",
    "        countries_to_topics_percent[country][topic] = topics_df.loc[country, topic][\n",
    "            \"percent\"\n",
    "        ]\n",
    "countries_to_topics_len_df = pd.DataFrame.from_dict(\n",
    "    countries_to_topics_len, orient=\"index\"\n",
    ")\n",
    "countries_to_topics_sum_df = pd.DataFrame.from_dict(\n",
    "    countries_to_topics_sum, orient=\"index\"\n",
    ")\n",
    "countries_to_topics_percent_df = pd.DataFrame.from_dict(\n",
    "    countries_to_topics_percent, orient=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2c508-4d20-4f6c-aba1-eaf1653ab36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_topics_sum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445b369-e933-4066-871a-107fda961e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomment and run this cell to see all available topics.\n",
    "# countries_to_topics_sum_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca346a-d223-44b4-b3c2-a53019a88d66",
   "metadata": {},
   "source": [
    "However, we are not be interested in all available topics. As a matter of fact, for our project, it is only useful to isolate the data about articles related to the environment. Examining the columns, the topic is available in only one of them, so we will extract only that topic in two dataframes that have the same format as [here](#extractionformat) (only difference is that we change the name *views* to *environment_views*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e0f89-3d72-47b6-b41a-82e5261c3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df = countries_to_topics_sum_df[\"STEM.Earth and environment\"]\n",
    "percent_environment_df = countries_to_topics_percent_df[\"STEM.Earth and environment\"]\n",
    "country_to_env_data_sum = {}\n",
    "country_to_env_data_percent = {}\n",
    "for country in sum_environment_df.index:\n",
    "    country_to_env_data_sum[country] = sum_environment_df[country]\n",
    "    country_to_env_data_percent[country] = percent_environment_df[country]\n",
    "sum_environment_df = pd.DataFrame.from_dict(country_to_env_data_sum, orient=\"index\").T\n",
    "percent_environment_df = pd.DataFrame.from_dict(\n",
    "    country_to_env_data_percent, orient=\"index\"\n",
    ").T\n",
    "percent_environment_df.index = pd.to_datetime(percent_environment_df.index)\n",
    "\n",
    "new_sum_environment_df = pd.DataFrame()\n",
    "for language in languages_unique:\n",
    "    country_env_sum_data = pd.DataFrame()\n",
    "\n",
    "    country_env_sum_data[\"environment_views\"] = (\n",
    "        sum_environment_df[language] + sum_environment_df[language + \".m\"]\n",
    "    )\n",
    "    country_env_sum_data[\"language\"] = language\n",
    "    new_sum_environment_df = pd.concat(\n",
    "        [new_sum_environment_df, country_env_sum_data], axis=0\n",
    "    )\n",
    "sum_environment_df = new_sum_environment_df\n",
    "sum_environment_df.index = pd.to_datetime(sum_environment_df.index)\n",
    "sum_environment_df[\"date\"] = sum_environment_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4652a93-39bf-46cd-bc06-449608fbc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdc080-773f-45a2-a72d-a71991ce2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really useful piece of data\n",
    "percent_environment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cea968-f37a-4ecf-a2dc-12d28614f4ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377675ad-6e85-4d2d-83f0-a7a5c3f5ad5f",
   "metadata": {},
   "source": [
    "In the following part, we save the following type of plots:\n",
    "\n",
    "- per language on Wikipedia, three separate line plots showing the evolution of the total number of views, the number of views of articles related to Covid-19, and the number of views of environment articles.\n",
    "- per language on Wikipedia, three separate histograms representing the number of views per day, for the same categories as in the point above.\n",
    "\n",
    "These plots can be found in the 'Figures' folder that will be created after the launch of all cells in this part, but we will plot some examples in the notebook to try and detect some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59cb19-698a-40b9-9ce8-f74029c7e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_path = \"./Figures/\"\n",
    "timeseries_path = \"timeseries/\"\n",
    "hists_path = \"hists/\"\n",
    "total_views_path = \"all_views/\"\n",
    "covid_views_path = \"covid_views/\"\n",
    "topic_views_path = \"topic_views/\"\n",
    "\n",
    "\n",
    "def make_sub_dirs(main_dir):\n",
    "    os.mkdir(main_dir + total_views_path)\n",
    "    os.mkdir(main_dir + covid_views_path)\n",
    "    os.mkdir(main_dir + topic_views_path)\n",
    "    os.mkdir(main_dir + topic_views_path + total_views_path)\n",
    "    os.mkdir(main_dir + topic_views_path + covid_views_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc76d5-72f1-491e-9d60-b432d4c8c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(figures_path):\n",
    "    os.mkdir(figures_path)\n",
    "    os.mkdir(figures_path + timeseries_path)\n",
    "    os.mkdir(figures_path + hists_path)\n",
    "    make_sub_dirs(figures_path + timeseries_path)\n",
    "    make_sub_dirs(figures_path + hists_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c906b-6c79-4e62-94f5-5e029477e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_language_views_timeseries(data, country, covid_views=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = data[data[\"language\"] == country]\n",
    "    x = filtered_data.index\n",
    "    y = filtered_data.views\n",
    "    g = sns.lineplot(x=x, y=y)\n",
    "    plt.xticks(fontsize=8)\n",
    "    g.set(xlabel=\"Dates\")\n",
    "    if covid_views:\n",
    "        title = \"Wikipedia page views for articles related to Covid-19 for {}\".format(\n",
    "            country\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia page views for {}\".format(country)\n",
    "    g.set(ylabel=\"Page views\", title=title)\n",
    "    # ax.set(xscale=\"log\")\n",
    "    if covid_views:\n",
    "        plt.savefig(figures_path + timeseries_path + covid_views_path + title + \".jpg\")\n",
    "    else:\n",
    "        plt.savefig(figures_path + timeseries_path + total_views_path + title + \".jpg\")\n",
    "    plt.close(fig)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9726f-611b-4c25-9067-30754e264587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_language_views(data, country, covid_views=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = data[data[\"language\"] == country]\n",
    "    g = sns.histplot(data=filtered_data, x=\"views\", bins=50)\n",
    "    if covid_views:\n",
    "        title = (\n",
    "            \"Wikipedia views distribution for articles related to Covid-19 {}\".format(\n",
    "                country\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia views distribution for {}\".format(country)\n",
    "    g.set(title=title)\n",
    "    if covid_views:\n",
    "        plt.savefig(figures_path + hists_path + covid_views_path + title + \".jpg\")\n",
    "    else:\n",
    "        plt.savefig(figures_path + hists_path + total_views_path + title + \".jpg\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5d437-eea7-4aba-9236-02418e1939ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_code in sum_data_df.language.unique():\n",
    "    lineplot_language_views_timeseries(sum_data_df, country_code)\n",
    "    hist_language_views(sum_data_df, country_code)\n",
    "    lineplot_language_views_timeseries(covid_sum_data_df, country_code, True)\n",
    "    hist_language_views(covid_sum_data_df, country_code, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb8cbb-434c-4745-817a-6bf11bc9903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_topic_views_timeseries(\n",
    "    topic_data, country, covid_views=False, topic=\"environment\"\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = topic_data[topic_data[\"language\"] == country]\n",
    "    x = filtered_data.index\n",
    "    y = filtered_data.environment_views\n",
    "    g = sns.lineplot(x=x, y=y)\n",
    "    plt.xticks(fontsize=8)\n",
    "    g.set(xlabel=\"Dates\")\n",
    "    if covid_views:\n",
    "        title = \"Wikipedia page views for articles related to Covid-19 for {0} for the {1} topic\".format(\n",
    "            country, topic\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia page views for {0} for the {1} topic\".format(country, topic)\n",
    "    g.set(ylabel=\"Page views\", title=title)\n",
    "    if covid_views:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + timeseries_path\n",
    "            + topic_views_path\n",
    "            + covid_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "    else:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + timeseries_path\n",
    "            + topic_views_path\n",
    "            + total_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2434775-3321-43ed-8b16-7f5a8bd0462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_topic_views(topic_data, country, covid_views=False, topic=\"environment\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = topic_data[topic_data[\"language\"] == country]\n",
    "    g = sns.histplot(data=filtered_data, x=\"environment_views\", bins=50)\n",
    "    if covid_views:\n",
    "        title = \"Wikipedia views distribution for articles related to Covid-19 for {0} for the {1} topic\".format(\n",
    "            country, topic\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia views distribution for {0} for the {1} topic\".format(\n",
    "            country, topic\n",
    "        )\n",
    "    g.set(title=title)\n",
    "    if covid_views:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + hists_path\n",
    "            + topic_views_path\n",
    "            + covid_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "    else:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + hists_path\n",
    "            + topic_views_path\n",
    "            + total_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520c7db-53e6-413d-9f0d-afd2761d55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_code in sum_environment_df.language.unique():\n",
    "    lineplot_topic_views_timeseries(sum_environment_df, country_code)\n",
    "    hist_topic_views(sum_environment_df, country_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd093d5-420c-4116-a14b-68fd950f831f",
   "metadata": {},
   "source": [
    "Let us plot some of the environment views timeseries here, to try and see if some interesting patterns can be seen already. We will choose the Serbian and Danish languages for this part. Note that we're not comparing the values themselves but whether or not there are patterns in either of them, so sharing the y-axis isn't really needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f99e3-523f-4fab-a84d-26a9bb750597",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_chosen = [\"sr\", \"da\"]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(countries_chosen), figsize=(20, 10), dpi=100)\n",
    "for i in range(len(countries_chosen)):\n",
    "    filename = \"Wikipedia page views for {} for the environment topic.jpg\".format(\n",
    "        countries_chosen[i]\n",
    "    )\n",
    "    ax[i].imshow(\n",
    "        plt.imread(fname=\"./Figures/timeseries/topic_views/all_views/\" + filename)\n",
    "    )\n",
    "    ax[i].axis(\"off\")\n",
    "# _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bc131-2736-40cf-be9a-28cc0ee58289",
   "metadata": {},
   "source": [
    "We can see a slight difference in behavior between these languages around the beginning of 2020:\n",
    "- first, we can see that while the environment views are pretty low before 2020 for the serbian wikipedia, these views go up quickly around the end of March 2020, which when looking at the intervention data seems to coincide with the first serbian lockdown. It is still a bit early however to affirm that one of these elements caused the other.\n",
    "- moving to the danish wikipedia however, there doesn't seem to be a particular rise or fall in the number of environment views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c0e25-ea7a-40a9-baa0-6a0b3d5bb8fa",
   "metadata": {},
   "source": [
    "We can also see however that for both languages, there seems to be a pattern of evolution of views between the years: for serbian, there is clear decrease between June and Septembre of both 2018 and 2019, which seems to have happened in 2020 as well if we look at the left plot. For danish the same phenomena can be observed as well, this time between July and August of both 2018 and 2019; again, this seems to have happened in 2020 as well if we were to look at the right plot. Looking at the other languages, patterns can also be found accross years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6ffe4-50b4-4c4d-8848-d4eb27da638b",
   "metadata": {},
   "source": [
    "It appears that indeed, the number of views for environmental articles did change in the same timeframe as when Covid-19 first arrived in some of the countries. It's equally important, however, to state that some don't display a change in pattern, and that at this point in the analysis **we can't say** that Covid-19 indeed caused more or less environment awareness on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e1331-aa61-443b-890b-268478b3dc3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Statistically testing if there was indeed a difference between 2019 and 2020 <a id='firststat'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d1548-c952-4b5a-9a8b-d2712e429ded",
   "metadata": {},
   "source": [
    "Let us now do another, more statistical analysis; we will test the hypothesis that, in average, and for every language, the average number of environmental views is the same between 2019 (pre-Covid) and 2020 (during the first, biggest wave of Covid). We will test these hypotheses using the $\\alpha = 0.05 $ significance level, as well as using the Bonferonni correction $\\alpha_{c} = \\frac{\\alpha}{n}$, with n = 14 .\n",
    "\n",
    "We match the same dates between 2019 and 2020 (up to the 31st of July, as that's when the Coronawiki data ends) for every language, and conduct a t-test for the null hypothesis described above. Note that we consider the samples to be related, as they come from the same language and for the same range of dates. In this setup, a negative t-statistic means that for that language, there are more environment views on average in 2020, and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf1fd7-d5e1-4f5a-bb59-c4510b9c71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_covid_env_topic_views = sum_environment_df[\n",
    "    (sum_environment_df.date < \"2020-01-01\") & (sum_environment_df.date >= \"2019-01-01\")\n",
    "]\n",
    "during_covid_env_topic_views = sum_environment_df[\n",
    "    sum_environment_df.date >= \"2020-01-01\"\n",
    "]\n",
    "for language in sum_environment_df.language.unique():\n",
    "    language_before = before_covid_env_topic_views[\n",
    "        before_covid_env_topic_views.language == language\n",
    "    ].copy()\n",
    "    language_during = during_covid_env_topic_views[\n",
    "        during_covid_env_topic_views.language == language\n",
    "    ].copy()\n",
    "\n",
    "    language_before.date = language_before.date.apply(\n",
    "        lambda date: str(date.month) + \"-\" + str(date.day)\n",
    "    )\n",
    "    language_during.date = language_during.date.apply(\n",
    "        lambda date: str(date.month) + \"-\" + str(date.day)\n",
    "    )\n",
    "    matching = pd.merge(\n",
    "        language_before,\n",
    "        language_during,\n",
    "        on=[\"date\", \"language\"],\n",
    "        suffixes=[\"_before\", \"_during\"],\n",
    "    )\n",
    "    stat, pvalue = stats.ttest_rel(\n",
    "        matching[\"environment_views_before\"], matching[\"environment_views_during\"]\n",
    "    )\n",
    "    print(\"p-value for {0}: {1}\".format(language, pvalue))\n",
    "    if pvalue >= 0.05:\n",
    "        print(\"Don't reject null when alpha = 0.05\")\n",
    "    if pvalue >= (0.05 / 14):\n",
    "        print(\"Don't reject null when using the banferoni correction\")\n",
    "    print(\"Ttest statistic value for {0}: {1}\".format(language, stat))\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539eca0-095b-4fd3-8b4f-601d075cbf71",
   "metadata": {},
   "source": [
    "It appears that for 11 out of 14 languages, there is indeed a change in how people visit these pages; the null hypothesis is rejected even after applying the Bonferonni correction, seeing how small their p-values are.. Out of these 11 languages, 6 observe a negative t-statistic, i.e. an average increase in 2020, while the other 5 observe a positive t-statistic, i.e an average decrease in 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83187a78-0a5c-44eb-8674-771bda26cc30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df91463-378c-4aa8-9195-4595befa7d9c",
   "metadata": {},
   "source": [
    "The second type of data we have are mobility data that come from two different sources. The first one is from Apple, who stopped giving out the data in April 2022, and the second one is from Google, which is still available, and more up-to-date (17th of October)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8910d-489d-4116-94b0-84969d4de6f6",
   "metadata": {},
   "source": [
    "### Apple mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb622bf-aee7-4556-80d4-fd1bde23fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_mobility = pd.read_csv(\"Data/applemobilitytrends-2020-04-20.csv.gz\")\n",
    "apple_mobility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4f513-88b1-44e0-a0ef-a6d24248aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apple_mobility.transportation_type.unique())  # Three types of transportation\n",
    "print(apple_mobility.geo_type.unique())  # Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c722dfdd-1244-46fb-a4e2-6497281e952a",
   "metadata": {},
   "source": [
    "The mobility data from Apple we have begins in mid-January 2020, and ends that same year in April. This isn't a big time window, and it doesn't appear that there is earlier data as it has been collected specifically for Covid-19 mobility tracking.\n",
    "\n",
    "Three types of transportation have been tracked here: driving, walking, and transit. We also have two different granularities about the collected data: either country/world region level, or city level, which are often country capitals.\n",
    "\n",
    "Per day and region, we have the pourcentage of the usage of every transportation mode according to some pre-pandemic baseline computed in early 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18fe82-cc3b-4749-8e1a-20156baeb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_mobility.isnull().any().any()  # There doesn't appear to be null data, but verifying shows that some countries don't have all the transportation types (transit, mostly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6485a88-c489-4345-b364-3086b7f963f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates strings to date times\n",
    "time_columns = pd.to_datetime(apple_mobility.columns[3:])\n",
    "apple_mobility.columns = apple_mobility.columns[:3].append(time_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75b980-0cfc-45f5-9e8c-414a53250562",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Global mobility from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7f124-a17e-4679-87cf-1747ab417ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mobility_report = pd.read_csv(\"Data/Global_Mobility_Report.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ae10a-93d2-4f0d-8bf6-a3edca11161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    min(global_mobility_report.date.unique()), max(global_mobility_report.date.unique())\n",
    ")\n",
    "global_mobility_report.date = pd.to_datetime(global_mobility_report.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22582c1f-b1cb-46fe-a24e-276fe123b0de",
   "metadata": {},
   "source": [
    "The mobility data from Google we have begins in mid-February 2020, and ends that same year in August. This is more than the given Apple data, despite the fact that both collections happened in the context of Covid-19.\n",
    "\n",
    "There are more levels of granularity with this data: for example, for the United Arab Emirates, we might simply talk about the whole country, or it could be specified in the column *sub_region_1* that the row is actually focused on the city of Abu Dhabi. This granularity can be made finer with *sub_region_2*.\n",
    "\n",
    "Per day and region, we have the **difference** in pourcentage usage of various location types (workplaces, etc) according to some pre-pandemic baseline computed in the early weeks of 2020. The baseline was computed *per day* , as people can have different behaviors depending on whether it's the weekend or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc014df-d018-4920-9a11-5d306f268bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mobility_report.isnull().sum() / global_mobility_report.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68afcd-f2d3-4f45-be8e-7e94f68e39e5",
   "metadata": {},
   "source": [
    "As expected, we have more coarse grained data (no missing data) than finer grained (many sub_region_1 fields are null, and even more sub_region_2 as well). The metropolitan area is very rarely defined, as almost 99.4% of the field is empty values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040e08d-1660-4d55-b229-2beeb9abaffe",
   "metadata": {},
   "source": [
    "Looking at the differences from baseline, we remark scarcity as well; apart from workplace locations which has a missing rate of only around 5.04%, others go from 36.4% (for retail) to 53.7% (for parks).\n",
    "\n",
    "We can look in more details at the entries which have the missing values for the differences from baseline; let's check the intersection of these missing values, to see for example if the absence of one field implies the absence of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136877fc-3e2e-4eb5-9901-8ef27d8b9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_missing = (\n",
    "    global_mobility_report.retail_and_recreation_percent_change_from_baseline.isnull()\n",
    ")\n",
    "grocery_pharmecy_missing = (\n",
    "    global_mobility_report.grocery_and_pharmacy_percent_change_from_baseline.isnull()\n",
    ")\n",
    "park_missing = global_mobility_report.parks_percent_change_from_baseline.isnull()\n",
    "transit_stations_missing = (\n",
    "    global_mobility_report.transit_stations_percent_change_from_baseline.isnull()\n",
    ")\n",
    "workplace_missing = (\n",
    "    global_mobility_report.workplaces_percent_change_from_baseline.isnull()\n",
    ")\n",
    "residential_missing = (\n",
    "    global_mobility_report.residential_percent_change_from_baseline.isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b24b73-99b8-4c81-86b0-2d139ec9b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing = (\n",
    "    retail_missing\n",
    "    & grocery_pharmecy_missing\n",
    "    & park_missing\n",
    "    & transit_stations_missing\n",
    "    & workplace_missing\n",
    "    & residential_missing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fcccb-a677-4d32-bc0e-50c102d260ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177e7b3-d53f-466d-8886-c65456dd17a6",
   "metadata": {},
   "source": [
    "From the above result, we can conclude that there doesn't appear to be a feature such that if that one is missing, then all the others are missing; this also means that for each entry, there's always at least one feature available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a1765-6e13-412e-ada3-714bbedbad70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9202aa6-1b81-48e3-a598-b2c326b6e876",
   "metadata": {},
   "source": [
    "Simply maps each considered article to the topics it is related to. A single article can be mapped to multiple topics. The number of articles per topic can be found in the [timeseries](#timeseries) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51d692-7594-4ecd-9c35-e9b7d727fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data takes a lot of time, so decomment if you want to run this cell.\n",
    "# topics_linked = pd.read_csv(\"topics_linked.csv.xz\")\n",
    "# topics_linked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6cab65-3276-4f8a-8ae4-da8effadb7aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6996c-3272-40e0-8689-a274243f8e33",
   "metadata": {},
   "source": [
    "The statistical testing [here](#firststat) showed us that indeed, there was a difference in environment views between 2019 and 2020. However, some questions remain: \n",
    "- are these differences correlated with the rest of the Coronawiki data, such as precise mobility changes (walking, driving, etc)?\n",
    "- can we explain how these views evolve depending on different time periods for each language?\n",
    "\n",
    "In order to answer these questions, we will proceed with two different regression analyses, both trying to explain the logarithm of the environment views. We are taking the logarithm because the range of pageviews vary greatly between languages. This also makes the models multiplicative, making comparaison easier between languages (a coefficient will now correspond to a multiplicative effect on the page views). The two regression models used are the following:\n",
    "- first, we will try to see if using mainly the mobility data alone (without the context of a time period) is enough to explain the (log)pageviews. There isn't a concept of treatment or control group here, it is simply a regression task.\n",
    "- the second model used is more refined: here, we take two different time periods (based on the mobility data) over 2019 and 2020, effectively creating a treatment and control group according to the time period, to see if we can detect a shift in pageviews volume based on the year and the time periods. More details will be given in the [second subsection](#diffndiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d4e21-6b0c-4ffb-9c73-347145548414",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df['log_pages_views'] = np.log(sum_environment_df.environment_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ac015-34e5-4170-af1e-c31c8f863149",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### First regression: using the mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028027a1-7f46-4e34-83e7-54208148d2ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Data merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cd8bc-3cc0-4208-a8d0-6d86df5855ce",
   "metadata": {},
   "source": [
    "The data preparation phase in the first regression is more intensive as in the second, as we have to merge/standardize the format of multiple dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc835f1-054e-4113-85cb-0ef0d288aefa",
   "metadata": {},
   "source": [
    "First of all, let us keep from both mobility datasets only the countries that were considered in the original timeseries. We mapped the english language to Washington DC, the catalan language to Barcelona, and the Korean language both to Seoul and South Korea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a22711-8e40-4fcf-aa1e-5ce54c131348",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "country_to_wiki_code = {\n",
    "    \"Japan\": \"ja\",\n",
    "    \"Italy\": \"it\",\n",
    "    \"Denmark\": \"da\",\n",
    "    \"Turkey\": \"tr\",\n",
    "    \"Norway\": \"no\",\n",
    "    \"Serbia\": \"sr\",\n",
    "    \"Sweden\": \"sv\",\n",
    "    \"Netherlands\": \"nl\",\n",
    "    \"Germany\": \"de\",\n",
    "    \"France\": \"fr\",\n",
    "    \"Barcelona\": \"ca\",\n",
    "    \"South Korea\": \"ko\",\n",
    "    \"Finland\": \"fi\",\n",
    "    \"District of Columbia\": \"en\",\n",
    "    \"Washington DC\": \"en\",\n",
    "    \"Seoul\": \"ko\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d873d06-a4d4-434e-ae64-5dfd250324f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def map_countries_to_fine_grained(x):\n",
    "    if x == \"Spain\":\n",
    "        return \"ca\"\n",
    "    if x == \"United States\":\n",
    "        return \"en\"\n",
    "    if x == \"Seoul\":\n",
    "        return \"ko\"\n",
    "    return country_to_wiki_code[x]\n",
    "\n",
    "\n",
    "filtered_global_mobility_report = global_mobility_report[\n",
    "    global_mobility_report[\"country_region\"].isin(country_to_wiki_code)\n",
    "    | global_mobility_report[\"sub_region_1\"].isin(country_to_wiki_code)\n",
    "    | global_mobility_report[\"sub_region_2\"].isin(country_to_wiki_code)\n",
    "].copy()\n",
    "filtered_global_mobility_report.loc[\n",
    "    :, \"country_region_code\"\n",
    "] = filtered_global_mobility_report.country_region.apply(\n",
    "    lambda x: map_countries_to_fine_grained(x)\n",
    ")\n",
    "filtered_global_mobility_report.index = range(len(filtered_global_mobility_report))\n",
    "filtered_global_mobility_report.country_region.unique()  # There is data about all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae52741-6afa-498c-a532-bfc0ad5e280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_global_mobility_language_info(cities_global_mobility_report, granularity, place, is_ankara = False):\n",
    "    if not(is_ankara):\n",
    "        return pd.concat(\\\n",
    "        [\n",
    "            cities_global_mobility_report,\n",
    "            filtered_global_mobility_report[\n",
    "                filtered_global_mobility_report[granularity] == place\n",
    "            ]\n",
    "        ])\n",
    "    return pd.concat(\\\n",
    "        [\n",
    "            cities_global_mobility_report,\n",
    "            filtered_global_mobility_report[\n",
    "                (filtered_global_mobility_report[granularity] == place) & (filtered_global_mobility_report[\"sub_region_2\"].isnull())\n",
    "            ]\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f492ee-0858-479b-a440-dfec130ff106",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_global_mobility_report = pd.DataFrame()\n",
    "zipped = [(\"sub_region_1\", 'Tokyo'), (\"sub_region_2\", 'Copenhagen Municipality'), (\"sub_region_1\", 'Berlin'), (\"sub_region_1\", 'Oslo'), (\"sub_region_2\", 'Barcelona'),\n",
    "          (\"sub_region_2\", 'Helsinki'), (\"sub_region_2\", 'Paris'), (\"sub_region_2\", 'Metropolitan City of Rome'), (\"metro_area\", 'Seoul Metropolitan Area'),\n",
    "          (\"sub_region_2\", 'Government of Amsterdam'), (\"metro_area\", 'Belgrade Metropolitan Area'), (\"sub_region_2\", 'Stockholm Municipality'), \n",
    "          (\"sub_region_1\", 'Ankara'), (\"sub_region_2\", 'Barcelona'), (\"sub_region_1\", 'District of Columbia')]\n",
    "\n",
    "for pair in zipped:\n",
    "    if pair[1] == 'Ankara':\n",
    "        cities_global_mobility_report = add_global_mobility_language_info(cities_global_mobility_report, pair[0], 'Ankara', True)\n",
    "    else:\n",
    "        cities_global_mobility_report = add_global_mobility_language_info(cities_global_mobility_report, pair[0], pair[1])\n",
    "cities_global_mobility_report = cities_global_mobility_report[\n",
    "    [\n",
    "        \"country_region_code\",\n",
    "        \"date\",\n",
    "        \"retail_and_recreation_percent_change_from_baseline\",\n",
    "        \"grocery_and_pharmacy_percent_change_from_baseline\",\n",
    "        \"parks_percent_change_from_baseline\",\n",
    "        \"transit_stations_percent_change_from_baseline\",\n",
    "        \"workplaces_percent_change_from_baseline\",\n",
    "        \"residential_percent_change_from_baseline\",\n",
    "    ]\n",
    "]\n",
    "cities_global_mobility_report = cities_global_mobility_report.rename(\n",
    "    columns={\"country_region_code\": \"language\"}\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98278b13-43cd-4644-bc90-8ff19b69c032",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filtered_apple_mobility = apple_mobility[\n",
    "    apple_mobility[\"region\"].isin(country_to_wiki_code)\n",
    "].copy()\n",
    "print(filtered_apple_mobility.region.unique())\n",
    "print(\"-----\")\n",
    "filtered_apple_mobility.region = filtered_apple_mobility.region.apply(\n",
    "    lambda x: country_to_wiki_code[x]\n",
    ")\n",
    "filtered_apple_mobility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ade63-7722-4607-a0e0-67688bc47218",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_apple_mobility = pd.DataFrame()\n",
    "for language in sum_data_df.language.unique():\n",
    "    tmp = (\n",
    "        filtered_apple_mobility[filtered_apple_mobility[\"region\"] == language]\n",
    "        .T.iloc[2:]\n",
    "        .copy()\n",
    "    )\n",
    "    tmp.columns = tmp.iloc[0]\n",
    "    tmp = tmp.iloc[1:]\n",
    "    tmp[\"language\"] = language\n",
    "    tmp[\"date\"] = tmp.index\n",
    "    new_apple_mobility = pd.concat([new_apple_mobility, tmp], axis=0)\n",
    "\n",
    "tmp_merge1 = sum_data_df.merge(new_apple_mobility, on=[\"language\", \"date\"])\n",
    "tmp_merge2 = tmp_merge1.merge(sum_environment_df, on=[\"language\", \"date\"])\n",
    "final_merge = tmp_merge2.merge(cities_global_mobility_report, on=[\"date\", \"language\"])\n",
    "final_merge[[\"driving\", \"transit\", \"walking\"]] = final_merge[\n",
    "    [\"driving\", \"transit\", \"walking\"]\n",
    "].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5620a21-f008-433f-8cb1-0c63cda9a4c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92437c-947d-4e3f-a4bc-63bd8049f695",
   "metadata": {},
   "source": [
    "There is still some null data however, so we have to manage these features, either with interpolation or filling these values with the mean of the corresponding countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3b0e4-7a23-4029-8e4a-e46b782e54d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_merge.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87d988-adf5-4631-9639-09e1ff74b868",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_merge[final_merge.transit.isnull()].groupby(\"language\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b587b5d-4dbd-474d-8bbb-29180ec6aea9",
   "metadata": {},
   "source": [
    "It appears that transit data from Apple is totally missing for the Korean, Serbian and Turkish parts; as such there is no way to replace them. Let us check for the other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265bf3b-0118-4e8f-8662-e3a54f16c70d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_merge[final_merge.grocery_and_pharmacy_percent_change_from_baseline.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03210f99-86b5-40ee-b6d5-7692f936ba60",
   "metadata": {},
   "source": [
    "We will simply interpolate these 2 missing values with the nearest value; because it's both on the same day, and that day isn't the last one per language, the nearest value will be one for the same language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a32089-9a64-4a14-9d7a-ef18cde3aa0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_merge[[\"grocery_and_pharmacy_percent_change_from_baseline\"]] = final_merge[\n",
    "    [\"grocery_and_pharmacy_percent_change_from_baseline\"]\n",
    "].interpolate(method=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8c245-56a4-4d31-a8c9-fee8869c8ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f6244-dd0d-431e-bf63-d2d6a68e832a",
   "metadata": {},
   "source": [
    "We can now do regression analysis, with the logarithm of the number of environment views as output, and using the columns of *final_merge* as covariates, to see if we can fit some hyperplane to this data. First, let us transform the categorical language data using dummy variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd29937-657b-470a-8afc-5170d4ef973d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_merge = pd.get_dummies(final_merge, drop_first=True)\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cc7e2-f2e7-436b-b150-bc96a706e37e",
   "metadata": {},
   "source": [
    "Then, we will normalize the input variables, so that each column has mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a8e86-a1c3-4acc-a3d7-4edf8f2fd199",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data_columns = list(final_merge.columns[2:])\n",
    "all_data_columns.remove(\"environment_views\")\n",
    "#all_data_columns.remove(\"year\")\n",
    "all_data_columns.remove(\"log_pages_views\")\n",
    "for column in all_data_columns:\n",
    "    final_merge[column] = (\n",
    "        final_merge[column] - final_merge[column].mean()\n",
    "    ) / final_merge[column].std()\n",
    "\n",
    "#final_merge[\"environment_views\"] = (\n",
    "#    final_merge[\"environment_views\"] - final_merge[\"environment_views\"].mean()\n",
    "#) / final_merge[\"environment_views\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c818fd-bf02-4e5d-920d-6ea22f4e843b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "formula = \"\"\n",
    "for covariate in all_data_columns:\n",
    "    formula = covariate + \"+\" + formula\n",
    "\n",
    "\n",
    "formula = formula[: len(formula) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3646f8-279c-4862-928c-7b9258f9063e",
   "metadata": {},
   "source": [
    "First we will use all covariates as predictors, with no interactions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71585f0a-3b7f-46c9-973a-9f017dab61b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=\"\"\"log_pages_views ~ \"\"\" + formula, data=final_merge)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2ec0b-dedc-492a-a00f-949d952d511e",
   "metadata": {},
   "source": [
    "We're being told in the warnings that there either, there are strong multicollinearity problems, or the design matrix isn't invertible. This might be because of the \"dummy variable trap\" where, after using dummy encoding, many of the created features can be explained by the others. The condition number is also pretty high, which means that the matrix is easily perturbed by variation in the input data. We will try to fit a different model with less of the dummy variables to see if we can get rid of this issue.\n",
    "\n",
    "We also use less data points than we have; these correspond to the three languages that don't have the Apple transit data, i.e Turkish, Serbian and Korean. We will first try to get rid of that feature for the next regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145526d0-2662-4e79-a8f8-388117605cc4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_to_exclude = [\n",
    "    \"transit\", \n",
    "]\n",
    "data_columns = list(set(all_data_columns) - set(features_to_exclude))\n",
    "formula = \"\"\n",
    "for covariate in data_columns:\n",
    "    formula = covariate + \"+\" + formula\n",
    "\n",
    "\n",
    "formula = formula[: len(formula) - 1]\n",
    "# formula = formula[120:]\n",
    "# formula = formula[72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1866b8-52a8-4efd-8ea6-d821eb8c9e0e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=\"\"\"log_pages_views ~ \"\"\" + formula, data=final_merge)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836a986-3612-405e-af4c-9b37966a2cbb",
   "metadata": {},
   "source": [
    "Here, we got rid of the Apple transit feature, which makes us able to use all the data points we have. This gets rid of the warning that we used to have, meaning that we don't have multicollinearity problems anymore, and the data matrix is invertible. We can then now try to interpret the model.\n",
    "\n",
    "The R-squared is nearly the same as before, i.e the part of explained variance is equal to 97.7% . Some of the features are not singificant, as their p-value is bigger than 0.05, so one can try to fit the model without these features.\n",
    "\n",
    "Looking at the covariates themselves, we see that it is useful to have the language data, as obviously the range of views from one country to another can change significantly as Wikipedia's importance isn't the same everywhere.\n",
    "\n",
    "From the Apple mobility data, it appears that the remaining features (Walking and driving) aren't statistically significant, and so it may not be suitable to use/interpret them.\n",
    "\n",
    "For the global mobility, all but one feature (*parks_percent_change_from_baseline*) are statistically significant; however, it is quiet hard to interpret some of them. For example, it makes sense to say that an increase in the use of transit stations results in less environment views (coefficient is -0.2785) as people are using more polluting modes of transportation. Similarly, an increase in how much one stays home (the residential change) seems to indicate an increase in environment views.\n",
    "\n",
    "However, on the flip side, other features aren't as interpretable, or appear counter-intuitive: for example, it's not really easy to explain why an increase in the visits to workplaces would cause an increase in environment views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479c30a-8072-4db7-a481-53f017bbe486",
   "metadata": {},
   "source": [
    "Performing a regression analysis using the mobility data from Apple and Google appears to explain the data quiet well; however, because it is lacking more contextualisation, it is not clear if this analysis is enough. To get more precise, we will now conduct the difference-in-difference regressions, to check more clearly the differences between not only 2019 and 2020, but also specific time periods in each year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977a7dd-ecaa-4fa2-8b51-7fcdb632f482",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Difference in Difference regression <a id='diffndiff'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac639c-183c-438a-aee5-f6e2048567e1",
   "metadata": {},
   "source": [
    "The second type of analysis we will now conduct is a difference-in-differences regression; as explained in the original paper, it follows the same logic as observational data analysis, as we can define a concept of treatment and control if we were to use certain time periods as points of reference.\n",
    "\n",
    "First, we have to prepare the data; as in the paper, we will define certain time windows of (up to) 5 weeks around both the change of mobility dates, and the return to normalcy dates.  We have to do this separately for each language, and keep the same dates in 2019. \n",
    "\n",
    "Note that we lack the Turkish data, as by the time Coronawiki was created, Turkey still didn't have even a lockdown (which happened in April 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbecd3-b464-429c-bd63-3a946ed25d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thirty_five_days = timedelta(days = 35)\n",
    "## For each date, calculate the 5 week time window preceeding and following it\n",
    "## We keep only the day and months so that when we define the time periods later, we can also do it for 2019.\n",
    "interventions['Mobility_35_days_before'] = (interventions['Mobility'] - thirty_five_days).dt.strftime('%m-%d')\n",
    "interventions['Mobility_35_days_after'] = (interventions['Mobility'] + thirty_five_days).dt.strftime('%m-%d')\n",
    "interventions['Mobility'] = interventions['Mobility'].dt.strftime('%m-%d')\n",
    "interventions['Normalcy_35_days_before'] = (interventions['Normalcy'] - thirty_five_days).dt.strftime('%m-%d')\n",
    "interventions['Normalcy_35_days_after'] = (interventions['Normalcy'] + thirty_five_days).dt.strftime('%m-%d')\n",
    "interventions['Normalcy'] = interventions['Normalcy'].dt.strftime('%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3de94-7bb5-4557-b4ba-670f99d6581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean variables that define the time periods; note that for attributes with prefix 'before_', this means that the date is in a 5 weeks time window before\n",
    "# the corresponding date for a given language; for the prefix 'after_', this means that it is in the 5 weeks time window after the date for that language\n",
    "sum_environment_df['before_mobility'] = 0\n",
    "sum_environment_df['after_mobility'] = 0\n",
    "sum_environment_df['before_normalcy'] = 0\n",
    "sum_environment_df['after_normalcy'] = 0\n",
    "sum_environment_df['year'] = sum_environment_df['date'].dt.year\n",
    "sum_environment_df['date_without_year'] =  sum_environment_df['date'].dt.strftime('%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cae013-c2bb-44b1-aaf8-58139aeeed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_values_for_language(mask, date_1, date_2, column_name):\n",
    "    #For a given boolean variable, set to 1 if the dates are in the given time window denoted by date_1 and date_2, otherwise it's 0\n",
    "    sum_environment_df.loc[(mask) & \\\n",
    "                        (sum_environment_df['date_without_year'] >= date_1) & \\\n",
    "                        (sum_environment_df['date_without_year'] < date_2), column_name] = 1\n",
    "\n",
    "    \n",
    "\n",
    "for language in languages_unique:\n",
    "    if language == 'tr': #ignore the turkish language as we don't have mobility data for it\n",
    "        continue\n",
    "    \n",
    "    constant_mask_interventions = (interventions['lang'] == language)\n",
    "    intervention_for_that_language = interventions[constant_mask_interventions]\n",
    "    \n",
    "    #Keep the 3 mobility-related dates for that language\n",
    "    before_mobility_date = intervention_for_that_language['Mobility_35_days_before'].item()\n",
    "    mobility_date = intervention_for_that_language['Mobility'].item()\n",
    "    after_mobility_date = intervention_for_that_language['Mobility_35_days_after'].item()\n",
    "    \n",
    "    #Keep the 3 normalcy-related dates for that language\n",
    "    before_normalcy_date = intervention_for_that_language['Normalcy_35_days_before'].item()\n",
    "    normalcy_date = intervention_for_that_language['Normalcy'].item()\n",
    "    after_normalcy_date = intervention_for_that_language['Normalcy_35_days_after'].item()\n",
    "    \n",
    "    constant_mask_env = (sum_environment_df['language'] == language)\n",
    "    set_values_for_language(constant_mask_env, before_mobility_date, mobility_date, 'before_mobility')\n",
    "    set_values_for_language(constant_mask_env, mobility_date, after_mobility_date, 'after_mobility')\n",
    "    set_values_for_language(constant_mask_env, before_normalcy_date, normalcy_date, 'before_normalcy')\n",
    "    set_values_for_language(constant_mask_env, normalcy_date, after_normalcy_date, 'after_normalcy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537f133-ee65-4f10-99aa-e35e8d9b4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for sanity check, no use to run it\n",
    "_ = \"\"\"for language in languages_unique:\n",
    "    print(language)\n",
    "    for c in ['before_mobility', 'after_mobility', 'before_normalcy', 'after_normalcy']:\n",
    "        print(c)\n",
    "        print(len(sum_environment_df[(sum_environment_df['language'] == language)&(sum_environment_df[c] == 1) & (sum_environment_df['year'] == 2020)]))\n",
    "    print(\"------\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864ced2-bec5-49bb-b133-97051610a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df = sum_environment_df.drop(labels = 'date_without_year', axis = 1) # We don't need that column anymore\n",
    "sum_environment_df_2019_2020 = sum_environment_df[sum_environment_df['year'] >= 2019].copy() #Keep only data from 2019 and 2020\n",
    "sum_environment_df_2019_2020['is_2020'] = (sum_environment_df_2019_2020.year == 2020).astype(int) #Create a boolean variable to denote the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9793e77-9baa-4c03-962b-f2eab087177e",
   "metadata": {},
   "source": [
    "Now, we will define the 3 regression models that we will build. For all of these, the data around certain dates from 2020 is considered to be the treated one, while the 2019 one is the control :\n",
    "- for the first, we take the mobility change date as our only reference, by using the 10 week data around that date for each language. The goal is to quantify the immediate change after the mobility restriction\n",
    "- for the second , we take the data before mobility restriction and the data after normalcy, to see if having the same mobility means having exactly the same visits to the environment pages\n",
    "- for the third and last, we take the data after the mobility change and the data after the return to normalcy, to see if by returning to normal, the environment visits also changed, and how much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abccb2-642b-4ebf-beb6-1dedc54295ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before and after mobility change\n",
    "sum_environment_df_2019_2020_before_after_mobility = sum_environment_df_2019_2020[(sum_environment_df_2019_2020.before_mobility == 1) |\\\n",
    "                             (sum_environment_df_2019_2020.after_mobility == 1)].drop(labels = ['before_normalcy', 'after_normalcy'], axis = 1)\n",
    "sum_environment_df_2019_2020_before_after_mobility['period'] = (sum_environment_df_2019_2020_before_after_mobility.after_mobility ==1).astype(int)\n",
    "\n",
    "# before mobility and after normalcy\n",
    "# note that for this dataframe and the next, we have to exclude the ca language, as it doesn't have a return to normacly date.\n",
    "sum_environment_df_2019_2020_before_mobility_after_normalcy = sum_environment_df_2019_2020[(sum_environment_df_2019_2020.before_mobility == 1) |\\\n",
    "                             (sum_environment_df_2019_2020.after_normalcy == 1)].drop(labels = ['after_mobility', 'before_normalcy'], axis = 1).copy()\n",
    "sum_environment_df_2019_2020_before_mobility_after_normalcy['period'] = (sum_environment_df_2019_2020_before_mobility_after_normalcy.after_normalcy ==1).astype(int)\n",
    "sum_environment_df_2019_2020_before_mobility_after_normalcy = sum_environment_df_2019_2020_before_mobility_after_normalcy[sum_environment_df_2019_2020_before_mobility_after_normalcy.language != 'ca']\n",
    "\n",
    "# after mobility and after normalcy\n",
    "sum_environment_df_2019_2020_after_mobility_after_normalcy = sum_environment_df_2019_2020[(sum_environment_df_2019_2020.after_mobility == 1) |\\\n",
    "                             (sum_environment_df_2019_2020.after_normalcy == 1)].drop(labels = ['before_mobility', 'before_normalcy'], axis = 1).copy()\n",
    "sum_environment_df_2019_2020_after_mobility_after_normalcy['period'] = (sum_environment_df_2019_2020_after_mobility_after_normalcy.after_mobility ==1).astype(int)\n",
    "sum_environment_df_2019_2020_after_mobility_after_normalcy = sum_environment_df_2019_2020_after_mobility_after_normalcy[sum_environment_df_2019_2020_after_mobility_after_normalcy.language != 'ca']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb95a23-3310-498d-ae60-56a1b88970c4",
   "metadata": {},
   "source": [
    "For the 3 regressions, the regression formula used is the same: $log\\_views \\; \\text{~}\\;is\\_2020 * period * language  $, where :\n",
    "- is_2020 is a binary variable that determines whether it is 2019 or 2020\n",
    "- period is a binary variable that determines whether we're in a certain time period for each test (for example, it is equal to 1 in the first regression if the data point is in the 5 week window after the mobility for the corresponding language, otherwise it is before). When both this variable is equal to 1, and the year is 2020, then we are in the treated group.\n",
    "- language simply corresponds to that data point's language; lets the model adapt to the different languages pageview volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4dd60-f458-472f-aa27-7a29d7b860de",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Difference in difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa40089-5beb-4a39-8759-f52880dda04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_languages = sum_environment_df_2019_2020_before_mobility_after_normalcy.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09ef36-3771-4ae1-8799-59b88f7b6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_error_sum(results, covariates):\n",
    "    '''\n",
    "    #95CI is approximated with +- 2 sum_variance_standard_error\n",
    "    '''\n",
    "    # get the variance covariance matrix\n",
    "    # print(covariates)\n",
    "    vcov = results.cov_params() \\\n",
    "        .loc[covariates, covariates].values\n",
    "\n",
    "    # calculate the sum of all pair wise covariances by summing up off-diagonal entries\n",
    "    off_dia_sum = np.sum(vcov)\n",
    "    # variance of a sum of variables is the square root\n",
    "    return np.sqrt(off_dia_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c90287-1ced-439f-ba45-52c7c8450c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_n_diff(df, careful_da = False):\n",
    "    mod = smf.ols(formula=\"\"\"log_pages_views ~ is_2020* period * C(language) \"\"\" , data=df)\n",
    "    res = mod.fit()\n",
    "    #print(res.params)\n",
    "    res = res.get_robustcov_results(cov_type='HC0')\n",
    "    res = statsmodels.regression.linear_model.RegressionResultsWrapper(res)\n",
    "    print(\"R2 for this regression: {}\".format(res.rsquared))\n",
    "    df_list = []\n",
    "    for lang in all_languages:\n",
    "        if careful_da and lang == 'da':\n",
    "            val = res.params['is_2020:period']\n",
    "\n",
    "            std = get_standard_error_sum(res, ['is_2020:period'])\n",
    "        else:\n",
    "            val = res.params['is_2020:period'] + \\\n",
    "                    res.params['is_2020:period:C(language)[T.{}]'.format(lang)]\n",
    "\n",
    "            std = get_standard_error_sum(res, ['is_2020:period',\n",
    "                                               'is_2020:period:C(language)[T.{}]'.format(lang)])\n",
    "        tmp_dict = {\n",
    "                \"lang\": lang,\n",
    "                \"low\": val - 2 * std,\n",
    "                \"high\": val + 2 * std,\n",
    "                \"val\": val,\n",
    "                \"pval\": (val - 2 * std > 0) or (val + 2 * std < 0),\n",
    "                \"std\": std\n",
    "            }\n",
    "\n",
    "        df_list.append(tmp_dict)\n",
    "    print(\"==============================================\")\n",
    "    print(pd.DataFrame(df_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddabe682-b1b8-40cd-a1dc-a434e0263e32",
   "metadata": {},
   "source": [
    "We will now have a look at the results from the 3 different regressions separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a2a06-13b4-46d0-931c-aa561d7f48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-vs-post mobility change effects on environment views\n",
    "diff_n_diff(sum_environment_df_2019_2020_before_after_mobility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a009a34-5ebc-4ef0-8d8f-37616a2aad4d",
   "metadata": {},
   "source": [
    "Let us now try to interpret the results we have for the first regression:\n",
    "- First, notice that for the logarithmic pre-vs.-post mobility effect isn't significant for all languages; only the Japanese, Italian, Serbian, Dutch and Finnish observe a statistically meaningful change. We also notice that for the languages where the effect isn't significant, the coefficients either go in the negatives (i.e there is a diminution of environment views after the change of mobility), or in very small positive values (i.e, small multiplicative effect).\n",
    "- Second, for every statistically significant change, there is an increase that can go from around 116% of normal pageviews for the Japanese Wikipedia ($exp(val)\\; \\text{} 1.16$ ), to around 224 % for the Serbian one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc603e1-53ca-4e93-aa7b-7644af94a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre mobility vs Post normalcy effects on environment views\n",
    "diff_n_diff(sum_environment_df_2019_2020_before_mobility_after_normalcy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ceae7-ba38-454c-89fe-7c6ca8741c27",
   "metadata": {},
   "source": [
    "For this second regression, we notice that:\n",
    "- for a majority of languages, the change is statistically significant, i.e there is indeed a difference between how people visit environment related pages before the mobility changes, and after the return to normalcy.\n",
    "\n",
    "- for most of these meaningful changes, we see a decrease, i.e people visit these pages less after returning to normal mobility. There are some outliers for which there is an increase (Serbian and Japanese), but for the decreases can range from 87% (Italian) of before-mobility pageviews to 74% (Danish), which is the most significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e608aa-db4b-4372-9a0c-a79ee132527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post mobility vs Post normalcy effects on environment views\n",
    "diff_n_diff(sum_environment_df_2019_2020_after_mobility_after_normalcy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05955ac0-0195-4466-a327-4429faf716f2",
   "metadata": {},
   "source": [
    "For the final results, this is what we have:\n",
    "- First, for all statistically significant changes, we observe an increase after the mobility changes compared to the post-return to normalcy window. We also see that whereas the change for some languages wasn't significant in the pre-vs-post mobility regression, it became so in this one (Danish and French for example). \n",
    "\n",
    "- Relating these results to the two above regressions, we see that it makes sense: according to the first analysis, views generally increased post-mobility compared to the pre-mobility, and according to the second, views generally decreased after the return to normality compared to before the mobility change. Therefore by transitivity is it expected that on average, there would be even bigger changes when comparing post-mobility to post-normalcy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a57c3c-a18a-4b75-8a5c-14aac74de1bb",
   "metadata": {},
   "source": [
    "After this analysis, we can conclude that depending on the considered time period and year, a general increase of environment views is clear after the mobility change compared to both after a return to normalcy, and before that same mobility change for every language.\n",
    "\n",
    "Now, this doesn't mean that people are necessarily in the topic of environment itself; as such, the next step of our analysis will be to compare the evolution of these views with the whole of Wikipedia for every language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f44fc5-e243-4d7d-86bb-a540554b32de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Comparaison with rest of Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa688b4-4f0e-4651-bac0-870f20332504",
   "metadata": {},
   "source": [
    "First, we will start with a simple plot, to gather some first-order intuition: we will aggregate, for every language, all the views from all topics one hand, and the environment views on the other. We will then plot the trends using a period of 14 days and see if the overall evolutions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627500a-5b07-47ae-8e7a-b3d83a8a02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df['year'] = sum_environment_df.date.dt.year\n",
    "sum_environment_df_2020 = sum_environment_df[sum_environment_df['year'] > 2019][['environment_views', 'language', 'date']]\n",
    "sum_environment_df_2020_views = sum_environment_df_2020.groupby('date').sum()\n",
    "sum_data_df_all_2020_views = sum_data_df[sum_data_df.date.dt.year > 2019].groupby('date').sum()\n",
    "#sum_data_df_all_2020_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17974e-7194-451e-b467-22404d255261",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_env = seasonal_decompose(sum_environment_df_2020_views, model='additive', period=14).trend\n",
    "result_all = seasonal_decompose(sum_data_df_all_2020_views, model='additive', period=14).trend\n",
    "# To put them both on the same scale from 0 to 1, we normalize by the maximum of every trend\n",
    "result_env = result_env / result_env.max()\n",
    "result_all = result_all/ result_all.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb0ada-0530-4d49-9cf4-5cd959b4d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.plot(result_env, label = \"Environment views\")\n",
    "plt.plot(result_all, label = \"All views\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f13ea2-d3fa-463c-b661-98ba2658fde8",
   "metadata": {},
   "source": [
    "As we can see in this first analysis, it appears that the maximum of the environment views happen in the beginning of 2020, during a time where the Wikipedia views in general are declining; after that, in the first part of March, the general views are in constant increase while the environment ones decline until near the end of March. After that, both timeseries appear to be in sync. \n",
    "\n",
    "In order to be more fine-grained about our analysis, we will propose two methods : Dynamic Time Warping first, then Time Lagged Cross-Correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab3dfc-400d-4671-8293-d3599fea1e52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dynamic time warping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df545-f4bf-4b55-bab2-7521f1bff579",
   "metadata": {},
   "source": [
    "Dynamic time warping (DTW) is a way to compare two usually temporal sequences that do not sync up perfectly. It is a method to calculate the optimal matching between two sequences. Its commonly used to measure the distance between two time-series.\n",
    "\n",
    "Our goal here is to build some intuition on whenever the increase in popularity of the environement pages is simply due to the fact that people read wikipedia more or if people are really more interested in them. In order to do that we will study the similarity between the wikipedia views of environement related topics with the total wikipedia views both as a function of time. We only keep the views data from 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29eec7a-23a1-4b16-80f3-c36dacec7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df_2020=sum_data_df[sum_data_df['date']>='2020-01-01']\n",
    "sum_environment_df_2020=sum_environment_df[sum_environment_df['date']>='2020-01-01']\n",
    "language=set(sum_data_df_2020['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb55ca6-2b13-4b71-ab4d-9489fbe20127",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig, axs = plt.subplots(7, 2)\n",
    "fig.set_figheight(35)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "for code in language:\n",
    "    ax = axs[i % 7, i // 7 ]\n",
    "    \"\"\"\n",
    "     We decided to use min max scaling as it makes the distance easier to interpret.\n",
    "     Scaling is important as both timeseries don't have the same range of values, and so this way both timeseries are in the [0, 1] range which makes \n",
    "     it possible to compare them.\n",
    "    \"\"\"\n",
    "    sum_data_df_2020_normalized = sum_data_df_2020[(sum_data_df_2020['views'].notnull()) & (sum_data_df_2020['language']==code)].copy()\n",
    "    sum_data_df_2020_normalized['views'] = (sum_data_df_2020_normalized['views']-sum_data_df_2020_normalized['views'].min())/sum_data_df_2020_normalized['views'].max()\n",
    "    \n",
    "    sum_environment_df_2020_normalized = sum_environment_df_2020[sum_environment_df_2020['environment_views'].notnull()& (sum_data_df_2020['language']==code)].copy()\n",
    "    sum_environment_df_2020_normalized['environment_views']=(sum_environment_df_2020_normalized['environment_views']-sum_environment_df_2020_normalized['environment_views'].min())/sum_environment_df_2020_normalized['environment_views'].max()\n",
    "    \n",
    "    d, cost_matrix, acc_cost_matrix, path = accelerated_dtw(sum_data_df_2020_normalized['views'].values,sum_environment_df_2020_normalized['environment_views'].values,dist='euclidean')\n",
    "\n",
    "    ax.imshow(acc_cost_matrix.T, origin='lower', cmap='plasma', interpolation='nearest')\n",
    "    ax.plot(path[0], path[1], 'w')\n",
    "    ax.set_xlabel('global views '+code)\n",
    "    ax.set_ylabel('environment_views '+code)\n",
    "    ax.set_title(f'DTW Minimum Path with minimum distance: {np.round(d,2)}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad9aca-cd1e-49aa-818b-461f42ccc14a",
   "metadata": {},
   "source": [
    "We can see here that for Italian, Norwegian, English and Dutch, the time series are really close to each others, and because the shortest path is really close to the matrix diagonal, we can say that the time series are behaving similarly (up to the scales of the values and time dilation). This may indicate that the evolution in environment views isn't due to a sudden ecological awareness.\n",
    "\n",
    "For some of the other languages (Korean or Swedish for example), the distances on one hand are higher than average, and the plots are very far from being lines. This means that DTW didn't find a mapping that is even close to one-to-one for a majority of the points; i.e, the environment views and the total Wikipedia evolution for these languages are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b392729-0a73-4638-8ee2-b6c7dc57aaac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time lagged cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a803bee-f235-4b2a-939b-205f29730ae3",
   "metadata": {},
   "source": [
    "Another similarity study we decided to use as a complement to the DTW is the Time lagged cross correlation which study the behavior of $\\mathbf{corr}[X_tY_{t+\\tau}]$ where $\\tau$ is a time lag. The goal of this analysis is see for which value of $\\tau$ the correlation is maximal in order to determine if one series is a shifted version of the other or at least has a non zero correlation of time lagged version of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0a673-f963-41a4-bba5-cc6f3d8df6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(datax, datay, lag=0, wrap=False):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Shifted data filled with NaNs \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    if wrap:\n",
    "        shiftedy = datay.shift(lag)\n",
    "        shiftedy.iloc[:lag] = datay.iloc[-lag:].values\n",
    "        return datax.corr(shiftedy)\n",
    "    else: \n",
    "        return datax.corr(datay.shift(lag))\n",
    "for code in language:\n",
    "    sum_data_df_2020_normalized = sum_data_df_2020[(sum_data_df_2020['views'].notnull()) & (sum_data_df_2020['language']==code)].copy()\n",
    "    sum_data_df_2020_normalized['views'] = (sum_data_df_2020_normalized['views']-sum_data_df_2020_normalized['views'].min())/sum_data_df_2020_normalized['views'].max()\n",
    "\n",
    "    sum_environment_df_2020_normalized = sum_environment_df_2020[sum_environment_df_2020['environment_views'].notnull()& (sum_data_df_2020['language']==code)].copy()\n",
    "    sum_environment_df_2020_normalized['environment_views']=(sum_environment_df_2020_normalized['environment_views']-sum_environment_df_2020_normalized['environment_views'].min())/sum_environment_df_2020_normalized['environment_views'].max()\n",
    "\n",
    "\n",
    "\n",
    "    d1 = sum_data_df_2020_normalized['views']\n",
    "    d2 = sum_environment_df_2020_normalized['environment_views']\n",
    "    seconds = 5\n",
    "    fps = 30\n",
    "    rs = [crosscorr(d1,d2, lag) for lag in range(-int(seconds*fps),int(seconds*fps+1))]\n",
    "    offset = np.floor(len(rs)/2)-np.argmax(rs)\n",
    "    f,ax=plt.subplots(figsize=(14,3))\n",
    "    ax.plot(rs)\n",
    "    ax.axvline(np.ceil(len(rs)/2),color='k',linestyle='--',label='Center')\n",
    "    ax.axvline(np.argmax(rs),color='r',linestyle='--',label='Peak synchrony')\n",
    "    ax.set(title=f'Offset = {offset} frames\\nS1 leads <> S2 leads for '+code,ylim=[-1,1],xlim=[0,301], xlabel='Offset',ylabel='Pearson r')\n",
    "    ax.set_xticks([0, 50, 100, 151, 201, 251, 301])\n",
    "    ax.grid()\n",
    "    ax.set_xticklabels([-150, -100, -50, 0, 50, 100, 150]);\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb7747-6eea-44f9-8d03-4415e0553fb0",
   "metadata": {},
   "source": [
    "An interesting case here is German for which an offset of 140 has a Pearson coefficient of 0.75 meaning that the wikipedia views at time $t+140$ almost overlaps with environment views at time t. This might indicate that some increase in the views of the environement related topics might not be due to an increase in wikipedia views.\n",
    "Meanwhile, for Serbian, Catalan, French, Italian and Norwegian, the offset that maximizes the cross correlation is 0. As such, we cannot really conclude much as the wikipedia views and environement views behave very similarly, so any increase in the environement views appears to be due to just an increase in Wikipedia views in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb9c59-8e4d-4d7a-bd68-6d418be9b021",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Study of the evolution of the rank of the environement topic as a function of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848ddcf-8172-4f17-883b-a2c0d05acce9",
   "metadata": {},
   "source": [
    "The previous analysis showed that a more in depth study of the Wikipedia views is required to truly see if the people's attention shifted towards or away from the environment pages during the first Covid wave. To that end, we decided to study the ranking of the views of the environment compared to the rest, to see whether or not it is one of the most important topics. A ranking of 1 means that it is the most viewed subject, while a ranking of 64 means the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06231e96-30ef-4dda-8ca6-d91d513245d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = set(countries_to_topics_sum_df.columns)\n",
    "df_per_topic=dict()\n",
    "\n",
    "for topic in topics:\n",
    "    sum_topic_df = countries_to_topics_sum_df[topic]\n",
    "    country_to_topic_data_sum = {}\n",
    "    for country in sum_topic_df.index:\n",
    "        country_to_topic_data_sum[country] = sum_topic_df[country]\n",
    "    sum_topic_df = pd.DataFrame.from_dict(country_to_topic_data_sum, orient=\"index\").T\n",
    "\n",
    "    new_sum_topic_df = pd.DataFrame()\n",
    "    for language in languages_unique:\n",
    "        country_topic_sum_data = pd.DataFrame()\n",
    "\n",
    "        country_topic_sum_data[topic+\"_views\"] = (\n",
    "            sum_topic_df[language] + sum_topic_df[language + \".m\"]\n",
    "        )\n",
    "        country_topic_sum_data[\"language\"] = language\n",
    "        new_sum_topic_df = pd.concat(\n",
    "            [new_sum_topic_df, country_topic_sum_data], axis=0\n",
    "        )\n",
    "    sum_topic_df = new_sum_topic_df\n",
    "    sum_topic_df.index = pd.to_datetime(sum_topic_df.index)\n",
    "    sum_topic_df[\"date\"] = sum_topic_df.index\n",
    "    sum_topic_df[\"topic\"] = topic\n",
    "    sum_topic_df = sum_topic_df[sum_topic_df.date.dt.year > 2018]\n",
    "    sum_topic_df = sum_topic_df.rename(columns = {topic+'_views':'views'})\n",
    "    df_per_topic.update([(topic,sum_topic_df)])\n",
    "multi_language_save_topic_data = copy.deepcopy(df_per_topic)\n",
    "    \n",
    "# we sum the views of all languages together\n",
    "\n",
    "for topic in topics:\n",
    "    df_per_topic[topic]=df_per_topic[topic].groupby('date').sum()\n",
    "    \n",
    "for topic in topics:\n",
    "    df_per_topic[topic]['topic']=topic\n",
    "    df_per_topic[topic].rename(columns = {topic+'_views':'views'}, inplace = True)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for topic in topics:\n",
    "    df = pd.concat([df, df_per_topic[topic]])   \n",
    "df['date']=df.index   \n",
    "df['Month_Year'] = df['date'].dt.to_period('M')\n",
    "df2=pd.DataFrame(df.groupby(['topic','Month_Year'])['views'].sum())\n",
    "periods=set(df['Month_Year'])\n",
    "df2['Month_Year']=df2.index.get_level_values('Month_Year')\n",
    "df2['topic']=df2.index.get_level_values('topic')\n",
    "environement_rank=pd.DataFrame()\n",
    "for period in periods:\n",
    "    temp=df2[df2['Month_Year']==period].copy()\n",
    "    temp['topic']=temp.index.get_level_values('topic')\n",
    "    temp['rank']=temp['views'].rank(ascending=False)\n",
    "    temp[temp['topic']=='STEM.Earth and environment']['rank']\n",
    "    environement_rank=pd.concat([environement_rank,temp[temp['topic']=='STEM.Earth and environment']['rank']])\n",
    "environement_rank.index=environement_rank.index.map( lambda  x:x[1])\n",
    "environement_rank=environement_rank.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea091cd7-e9fc-462f-bbd0-d757819a8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "environement_rank.plot.line(figsize = (16,5))\n",
    "plt.title('Evolution of the ranking of the views of the environement topic per month')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Environment rank')\n",
    "plt.legend([\"Rank\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131376a-81ac-4069-98b8-0a6969933b21",
   "metadata": {},
   "source": [
    "The results here aren't the best; the fact is that the rank of the environment topic (aggregated accross all available languages) doesn't go above 44, and towards June/July, it actually goes above the 50 rank threshold.\n",
    "\n",
    "To try and see if this is the result of some languages dominating others in terms of views (English for example), and thus shifting the ranks, we will do the same ranking plot but for every language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21a2aa-e4f9-4efe-bee4-1bbe7d592838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language_to_env_ranking = {} #will hold the ranking of env for each languages\n",
    "periods = set()\n",
    "for language in languages_unique:\n",
    "    df_language_all_topics = pd.DataFrame()\n",
    "    for topic in topics:\n",
    "        language_topic_data =  multi_language_save_topic_data[topic]\n",
    "        language_topic_data = language_topic_data[language_topic_data.language == language]\n",
    "        df_language_all_topics = pd.concat([df_language_all_topics, language_topic_data])\n",
    "    #print(df_language_all_topics)\n",
    "    df_language_all_topics['year_month'] = df_language_all_topics['date'].dt.to_period('M')\n",
    "    if(periods == set()):\n",
    "        periods = sorted(set(df_language_all_topics['year_month']))\n",
    "    \n",
    "    grouped = pd.DataFrame(df_language_all_topics.groupby(['year_month', 'topic'])['views'].sum())\n",
    "    grouped['year_month'] = grouped.index.get_level_values('year_month')\n",
    "    grouped['topic'] = grouped.index.get_level_values('topic')\n",
    "    #print(grouped.year_month)\n",
    "    #grouped\n",
    "    result = []\n",
    "    result_language = pd.DataFrame()\n",
    "    for period in periods:\n",
    "        tmp = grouped[grouped.year_month == period].copy()\n",
    "        tmp['rank'] = tmp['views'].rank(ascending = False)\n",
    "        tmp = pd.concat([result_language, tmp[tmp.topic == 'STEM.Earth and environment'][['year_month', 'rank']]])\n",
    "        tmp.index = tmp.year_month\n",
    "        result_language = tmp\n",
    "        #result.append(tmp[tmp.topic == 'STEM.Earth and environment']['rank'])\n",
    "    language_to_env_ranking[language] = result_language\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ad62c-8325-4633-ae67-d710cd414acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,5))\n",
    "plt.gca().set_prop_cycle(None)\n",
    "cmap = plt.get_cmap('jet_r')\n",
    "for idx,language in enumerate(languages_unique):\n",
    "    color = cmap(float(idx)/len(languages_unique))\n",
    "    language_to_env_ranking[language] = language_to_env_ranking[language].rename(columns = {'rank': language + \"_rank\"})\n",
    "    ax = language_to_env_ranking[language][language + \"_rank\"].plot.line(color = color)\n",
    "plt.title('Evolution of the ranking of the views of the environement topic per month and per language')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rank')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b165e-023d-47ab-8ddf-f48523bb1b0f",
   "metadata": {},
   "source": [
    "Here, the results aren't looking great either: it appears that a majority of the languages, the didn't really change their focus towards environment when the first Covid wave hit in March/April 2020. One language in particular that appears to have that behavior is Serbian, but even that shift may have not been caused by Covid; other changes seem to be seasonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b675aaa-b498-4451-be16-3519535c5b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Excluding the data loading/wrangling, this first part serves as a preliminary analysis of the Coronawiki dataset content related to environmental topics. Firstly, we saw that statistically speaking, for a majority of the available countries, there was indeed a difference between the visits to environmental topics between the years of 2019 and 2020; this difference wasn't always an increase for all languages.\n",
    "\n",
    "Using that as a basis, we asked ourselves whether we can predict these views using only the changes in transportations mode/visits to certain places, for every language. We saw that it appears to be possible, but not always interpretable, and there may be a lack of contextualisation, i.e it's not certain that if we had the same data but from another year (for example 2019), we would have the same results. In order to do that, we resort to difference-in-difference regression.\n",
    "\n",
    "Depending on the considered time period and year, a general increase of environment views is clear after the mobility change compared to both after a return to normalcy, and before that same mobility change for every language. Note that in this case, we did use the 2019 data as well, so we see that the fact that there is an increase in 2020 is indeed related to Covid-related restrictions. However, we asked ourselves another question at this point: yes, there is an increase happening during the first Covid wave, but this translate directly into environmental awareness?\n",
    "\n",
    "To attempt to answer that, we decided to study the similarities between the evolution of the global wikipedia views, and the evolution of wikipedia views related to the environement.\n",
    "The DTW analysis showed that for a lot of languages, both of the corresponding timeseries are really close to each others in distance and behave very similarly. This seems to indicate that an increase in environment views was most of the time, because of an increase Wikipedia usage in general. \n",
    "\n",
    "In order to complement this analysis, we decided to look at the time lagged cross correlation. Unfortunatly, other than for German for which time lagged cross correlation attains its maximun of 0.75 with a lag of 140 compared to the unlagged correlation of 0.5, most of the languages achieve maximum cross correlation for a lag of 0 or 1, and therefore both of their time series are almost superposed.\n",
    "\n",
    "More over, since the environement views and wikipedia views behave the same, we decided to study the ranking of the environement topic compared to other topics, either by separating the languages or aggregating them all together. The results we have seems to indicate that people were more preoccupied with other topics than the environment during the first Covid wave.\n",
    "\n",
    "Thus it seems that for now, the increase in environement views in 2020 isn't necessarily due to a sudden awarness as its rankind drops to 51, but to a increase in wikipedia views. However, it doesn't mean that more awareness wasn't there; our data does stop in July 2020 after all; plus, the Wikipedia views alone aren't enough to confirm a change in awareness, we need to have a better look at the general behavior of people. In order to complement our analysis, we add the usage in the followning part of external datasets related to pollution, and even more Wikipedia data to see the evolution of environment related views after 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23f73e-ba2d-4cac-83d8-cdbfa29dad7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 2 : The external datasets which we added by ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91089e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Precise Wikipedia Views Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c13ea7",
   "metadata": {},
   "source": [
    "The goal of this section is to precisely analyse the views of different wikipedia pages (Air pollution, Plastic pollution, and plastic production) from 01/01/2019 to 01/09/2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16ac57",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We restrict the page to the language studied in coronawiki\n",
    "languages = [\n",
    "    \"ja\",\n",
    "    \"it\",\n",
    "    \"da\",\n",
    "    \"tr\",\n",
    "    \"no\",\n",
    "    \"en\",\n",
    "    \"sr\",\n",
    "    \"sv\",\n",
    "    \"nl\",\n",
    "    \"de\",\n",
    "    \"fr\",\n",
    "    \"ca\",\n",
    "    \"ko\",\n",
    "    \"fi\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_langviews_data(path, columns_to_keep=None):\n",
    "    \"\"\"\n",
    "    Convert the csv generated by langviews for a particular wikipedia page into a usable dataframe keeping only the pages in the specified languages.\n",
    "    The resulting dataframe has a column per language and a new total column giving the total number of views across all the studied languages.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(path).transpose()\n",
    "    data.columns = data.iloc[0]\n",
    "    titles = data.iloc[1]\n",
    "    data.drop([\"Language\", \"Title\", \"Badges\"], inplace=True)\n",
    "\n",
    "    if columns_to_keep is None:\n",
    "        columns_to_keep = data.columns\n",
    "\n",
    "    columns = data.columns.intersection(columns_to_keep + [\"date\"])\n",
    "    data = data[columns]\n",
    "\n",
    "    data[\"total\"] = data.sum(axis=1)\n",
    "    data[\"date\"] = data.index\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    data[\"year\"] = data.apply(lambda x: x.date.year, axis=1)\n",
    "\n",
    "    return  titles ,data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64b7da",
   "metadata": {},
   "source": [
    "#### Air Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a203e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "airpol_titles, airpol_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-airpol.csv\", languages\n",
    ")\n",
    "airpol_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173be22",
   "metadata": {},
   "source": [
    "#### Plastic Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plastin_titles, plastin_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-plastin.csv\", languages\n",
    ")\n",
    "plastin_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5cf02",
   "metadata": {},
   "source": [
    "#### Plastic Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plastpol_titles, plastpol_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-plastpol.csv\", languages\n",
    ")\n",
    "plastpol_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc8125",
   "metadata": {},
   "source": [
    "### Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a562875",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_titles, covid_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-covid19.csv\", languages\n",
    ")\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc300fc3-877a-4eb8-9286-5dbd8d87e5d7",
   "metadata": {},
   "source": [
    "### Total Wikipedia Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ab968-2f09-4106-a073-a4a15df16475",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_views = pd.read_csv('Data/Additional/langviews/final/wikipedia_global_views.csv')\n",
    "global_views['date'] = pd.to_datetime(global_views['date'])\n",
    "global_views['total'] = global_views.drop(\"date\", axis=1).sum(axis=1)\n",
    "global_views['year'] = global_views.apply(lambda x: x.date.year, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042b51b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e878f",
   "metadata": {},
   "source": [
    "### Air Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972468db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=airpol_data.total, x=airpol_data.year)\n",
    "plt.title(\"Air Pollution page views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dea3e0",
   "metadata": {},
   "source": [
    "It seems that during and after COVID, interest in air pollution decreases significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56328239",
   "metadata": {},
   "source": [
    "### Plastic Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac211b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=plastin_data.total, x=plastin_data.year)\n",
    "plt.title(\"Plastic Industry page views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa769f",
   "metadata": {},
   "source": [
    "There are not enough views to conclude anything about awareness on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c35db",
   "metadata": {},
   "source": [
    "### Plastic Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=plastpol_data.total, x=plastpol_data.year)\n",
    "plt.title(\"Plastic Pollution page views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd9b67-3d54-45eb-8a13-9b2b5ae3a1af",
   "metadata": {},
   "source": [
    "### Total Wikipedia Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e357848-7c9d-4024-b2c4-7dc1bd7822a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=global_views.total, x=global_views.year)\n",
    "plt.title(\"Total Wikipedia views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3e6bd-c89c-4b10-b844-eec5aa1a5c98",
   "metadata": {},
   "source": [
    "Globally, it seems that interest in wikipedia increases during Covid and significatively decreases after the crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd977e-a76c-45c0-b1c4-cbe77a2c6c7c",
   "metadata": {},
   "source": [
    "### Environment topic expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449493f-0696-4ff2-828e-17dfec4ee33c",
   "metadata": {},
   "source": [
    "The coronawiki dataset contains aggregated views per day of all articles related to different topics. Unfortunately, these values are only listed for the years 2018 to 2020. This is not usable to do a proper analysis, so we used the langviews website to acquire views data for every article in the environment topic and aggregate them. To see how we precisely did, you can see the *wikipedia_views_extension* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755b1ca-314c-42aa-b5c7-2e5ce802fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_views = pd.read_csv('Data/Additional/langviews/final/aggregated_views.csv')\n",
    "aggregated_views['date'] = pd.to_datetime(aggregated_views['date'])\n",
    "aggregated_views['year'] = aggregated_views.apply(lambda x: x.date.year, axis=1)\n",
    "aggregated_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454f9bb-d78c-474d-99bc-7600285174ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=aggregated_views.total, x=aggregated_views.year)\n",
    "plt.title(\"Environment topic views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc009965-f01e-4d87-b0ce-1ca1411c227a",
   "metadata": {},
   "source": [
    "Globally, after the Covid crisis, interest in articles concerning the environment topic decreases. We now need to determine wether this decrease follow the general loss of interest in wikipedia or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485199d-8edc-4f8e-aed8-957a8b111a2b",
   "metadata": {},
   "source": [
    "This concludes the extra data from Wikipedia that we add to the original Coronawiki dataset.\n",
    "Let us now present the next one : NO2 air pollution data by country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddb8bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Analysis of nitrogen dioxide levels by capital city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69346f8",
   "metadata": {},
   "source": [
    "Nitrogen dioxide is a toxic molecule that is typically emitted by industrial activities and car engines. It is therefore a very good indicator of human-induced air pollution.\n",
    "\n",
    "Here, we will focus on understanding the link between Covid and NO2 air pollution, i.e. between Covid and human-induced air pollution. To do so, we will make the following assumptions :\n",
    "- 2019 is the year that represents the \"usual activities\" before Covid.\n",
    "- The year of lockdowns and restrictions is then obviously 2020. We also consider that 2021-22 are the beginning of efforts towards going back to the normal, pre-Covid life. We do not assume that 2021 and 2022 have succeeded at being \"back to normal\".\n",
    "\n",
    "This might be a simplification of reality, but it has the two following advantages :\n",
    "- Covid began spreading worldwide during the very beginning of 2020. We can exploit this to our advantage by splitting years at the beginning of January, which is easy and fits our assumptions well.\n",
    "- It makes for very easy cross-coutries, cross-years comparisons.\n",
    "- Using instead lockdown data instead of year-by-year separations would weaken the possibility of comparing pollution by country, as pollution emissions are very dependent on the period of the year.\n",
    "\n",
    "The goal of this section is then to establish (or not) the fact that during Covid, the air got significantly cleaner, and to quantify the margin of improvement. We also consider the recovery phase after the initial Covid wave, i.e. after 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98c700",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd29fc",
   "metadata": {},
   "source": [
    "We use air pollution data from the different capitals from the World Air Quality Index dataset for the Covid-19 period (https://aqicn.org/data-platform/covid19/). This is a reliable dataset that was used in various large-scale studies about worldwide air pollution. This dataset is fairly lage, it includes the data for the years 2018 to 2022 for many cities of the world, and for various air polluting molecules.\n",
    "\n",
    "We reduce this to NO2 and only the fourteen capital cities in the notebook \"air_quality_by_capital.ipynb\", reducing the dataset to a much more manageable size, but not to the point of losing all interesting information.\n",
    "\n",
    "Note : we don't consider country-wide pollution because the dataset does not provide us with a way to merge all cities into one large blob for the country : we don't have the city size, the geographic proportion of the city in the country, etc. We therefore study capitals only, which the dataset certainly provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_datasets = \"Data/Additional/waqi/no2_capital/\"\n",
    "capitals = [\n",
    "    \"tokyo\",\n",
    "    \"rome\",\n",
    "    \"copenhagen\",\n",
    "    \"ankara\",\n",
    "    \"oslo\",\n",
    "    \"washington\",\n",
    "    \"stockholm\",\n",
    "    \"belgrade\",\n",
    "    \"amsterdam\",\n",
    "    \"berlin\",\n",
    "    \"paris\",\n",
    "    \"barcelona\",\n",
    "    \"seoul\",\n",
    "    \"helsinki\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_datasets = []\n",
    "for capital in capitals:\n",
    "    full_datasets.append((capital, pd.read_csv(path_to_datasets + capital + \".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_datasets[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc18fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2019 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    # keep year 2019\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2019\")].copy()\n",
    "    # remove year from date string to allow for inner merge on date with other years later on\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    # remove all other columns\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2019.append((capital, ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing as above, but for 2020\n",
    "ds_2020 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2020\")].copy()\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2020.append((capital, ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bce8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2021 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2021\")].copy()\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2021.append((capital, ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cef886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2022 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2022\")].copy()\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2022.append((capital, ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb1bfc",
   "metadata": {},
   "source": [
    "## Data viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c038b",
   "metadata": {},
   "source": [
    "We will then plot the SO3 measurements in all 14 cities every day for the years 2019, 2020 and 2021.\n",
    "\n",
    "We could also easily plot the current data for 2022, but we found that the resulting graph was fairly overloaded.\n",
    "\n",
    "We use a log scale because these cities have strong differences in air quality, and we want to show the full detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(14, sharex=True, sharey=True)\n",
    "fig.set_size_inches(18.5, 35.5)\n",
    "for i in range(14):\n",
    "    ds1 = ds_2019[i][1]\n",
    "    ds2 = ds_2020[i][1]\n",
    "    ds3 = ds_2021[i][1]\n",
    "    # not 2022, see above\n",
    "\n",
    "    ax[i].set_yscale(\"log\")\n",
    "    ax[i].set_title(ds_2019[i][0])\n",
    "    # always 2020 because we want all three graphs on one year, and 2020 is a leap year\n",
    "    (p2019,) = ax[i].plot_date(\n",
    "        matplotlib.dates.datestr2num(\"2020-\" + ds1[\"yearlessDate\"]),\n",
    "        ds1[\"median\"],\n",
    "        tz=\"UTC+1\",\n",
    "        fmt=\"-\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    (p2020,) = ax[i].plot_date(\n",
    "        matplotlib.dates.datestr2num(\"2020-\" + ds2[\"yearlessDate\"]),\n",
    "        ds2[\"median\"],\n",
    "        tz=\"UTC+1\",\n",
    "        fmt=\"-\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    (p2021,) = ax[i].plot_date(\n",
    "        matplotlib.dates.datestr2num(\"2020-\" + ds3[\"yearlessDate\"]),\n",
    "        ds3[\"median\"],\n",
    "        tz=\"UTC+1\",\n",
    "        fmt=\"-\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    ax[i].legend([p2019, p2020, p2021], [\"2019\", \"2020\", \"2021\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191d1d9",
   "metadata": {},
   "source": [
    "Some trends emerge :\n",
    "- 2019 (in blue) is usually a little bit above the others. When considering that this is a log scale, this is actually a fairly impressive difference.\n",
    "- 2020 is typically lower. We can almost always find a drop in March 2020, where the international community initially reacted to the virus.\n",
    "- 2020 and 2021 are somewhat more difficult to discern. It could be that these years are similar in terms of NO2 pollution.\n",
    "\n",
    "We can also check the evolution of mean pollution per country per year :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means2019 = [df[\"median\"].mean() for (capital, df) in ds_2019]\n",
    "means2020 = [df[\"median\"].mean() for (capital, df) in ds_2020]\n",
    "means2021 = [df[\"median\"].mean() for (capital, df) in ds_2021]\n",
    "means2022 = [df[\"median\"].mean() for (capital, df) in ds_2022]\n",
    "\n",
    "\n",
    "plt.title(\"Yearly average of daily pollution in capital cities\")\n",
    "\n",
    "for i in range(len(capitals)):\n",
    "    plt.plot(\n",
    "        [\"2019\", \"2020\", \"2021\", \"2022\"],\n",
    "        [means2019[i], means2020[i], means2021[i], means2022[i]],\n",
    "        linewidth=0.8,\n",
    "        label=capitals[i],\n",
    "    )\n",
    "\n",
    "plt.legend(loc=(1.05, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd77a05",
   "metadata": {},
   "source": [
    "Perhaps surprisingly, the graph is not U-shaped at all ! This means that on average, air pollution in the capital cities goes down steadily year after year. There is a caveat : the year 2022 is not over as we write these words. This means that the end of autumn and beginning of winter 2022 are not accounted for in this graph. If winter is more polluted than summer in 2022, then the graph above underestimates the pollution average for 2022.\n",
    "\n",
    "We can then quantify this evolution exactly, by using statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df659987",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf77fd",
   "metadata": {},
   "source": [
    "The testing we will perform consists of the following :\n",
    "- First, we merge the pollution data of two years\n",
    "- Then, for each capital city, we do a paired test comparing pairs of (day, day) of both years\n",
    "- The two days are always on the same date, except for the year. This enables us to compare pollution day by day, ignoring the effects of recurring seasonal pollution. This analysis works for 2022, as the missing data for the end of the year will be ignored in the other dataset, possibly making the compromise of increasing the variance of test statistics.\n",
    "\n",
    "We first merge years we want to compare. We will compare the following pairs : (2019-2020) to check if covid had an impact against the 2019 baseline, (2020-2021) to see if 2021 was a rebounce/drop from Covid, and (2020, 2022) for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697eb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged19_20 = []\n",
    "for index in range(len(capitals)):\n",
    "    (capital1, df1) = ds_2019[index]\n",
    "    (capital2, df2) = ds_2020[index]\n",
    "    assert capital1 == capital2\n",
    "    mergeTwoYears = pd.merge(df1, df2, on=\"yearlessDate\", how=\"inner\")\n",
    "    merged19_20.append((capitals[index], mergeTwoYears))\n",
    "\n",
    "merged20_21 = []\n",
    "for index in range(len(capitals)):\n",
    "    (capital2, df2) = ds_2020[index]\n",
    "    (capital3, df3) = ds_2021[index]\n",
    "    assert capital2 == capital3\n",
    "    mergeTwoYears = pd.merge(df2, df3, on=\"yearlessDate\", how=\"inner\")\n",
    "    merged20_21.append((capitals[index], mergeTwoYears))\n",
    "\n",
    "merged20_22 = []\n",
    "for index in range(len(capitals)):\n",
    "    (capital2, df2) = ds_2020[index]\n",
    "    (capital4, df4) = ds_2022[index]\n",
    "    assert capital2 == capital4\n",
    "    mergeTwoYears = pd.merge(df2, df4, on=\"yearlessDate\", how=\"inner\")\n",
    "    merged20_22.append((capitals[index], mergeTwoYears))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f6140",
   "metadata": {},
   "source": [
    "We can then do the daily paired test for the year pairs :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad37aa1",
   "metadata": {},
   "source": [
    "2019 to 2020 comparison :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_capital = []\n",
    "for capital, merger in merged19_20:\n",
    "    ttest = stats.ttest_rel(merger.median_x, merger.median_y, alternative=\"greater\")\n",
    "    alone_sig = ttest.pvalue < 0.05\n",
    "    bonf_sig = ttest.pvalue < (0.05 / 14)\n",
    "    evolution = (\n",
    "        \"{:+.2f}\".format(\n",
    "            ((np.mean(merger.median_y) / np.mean(merger.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "\n",
    "    test_by_capital.append((capital, len(merger), evolution, alone_sig, bonf_sig))\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_by_capital,\n",
    "    columns=[\n",
    "        \"Capital\",\n",
    "        \"numpoints\",\n",
    "        \"pollution_evolution\",\n",
    "        \"better_during_covid_significant\",\n",
    "        \"bonferroni_significant\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcae5e6",
   "metadata": {},
   "source": [
    "Here, we learn that for almost every capital city, the air was significantly cleaner in during Covid times than before. There is an exception for Ankara in Turkey, which is the only capital city that polluted more during Covid than before. Tokyo is not Bonferroni-significant, but also shows a drop in average pollution during Covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_capital = []\n",
    "for capital, merger in merged20_21:\n",
    "    # NOTE : we switch to two-sided testing because we are interested in both kinds of changes.\n",
    "    # Before, we were only interested in knowing whether covid was an improvement or not\n",
    "    ttest = stats.ttest_rel(merger.median_x, merger.median_y, alternative=\"two-sided\")\n",
    "    alone_sig = ttest.pvalue < 0.05\n",
    "    bonf_sig = ttest.pvalue < (0.05 / 14)\n",
    "    evolution = (\n",
    "        \"{:.2f}\".format(\n",
    "            ((np.mean(merger.median_y) / np.mean(merger.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "\n",
    "    test_by_capital.append((capital, len(merger), evolution, alone_sig, bonf_sig))\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_by_capital,\n",
    "    columns=[\n",
    "        \"Capital\",\n",
    "        \"numpoints\",\n",
    "        \"pollution_evolution\",\n",
    "        \"alone_significant\",\n",
    "        \"bonferroni_significant\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71ca84",
   "metadata": {},
   "source": [
    "Here, the results are much less significant, as the test could not manage to find a lot of cities where 2020 and 2021 were somehow different. We only have Belgrade (Serbia) and Helsinki (Finland) that showed a significant boost in pollution between 2020 and 2021. In this sense, the years 2020 and 2021 and very much alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_capital = []\n",
    "for capital, merger in merged20_22:\n",
    "    ttest = stats.ttest_rel(merger.median_x, merger.median_y, alternative=\"two-sided\")\n",
    "    alone_sig = ttest.pvalue < 0.05\n",
    "    bonf_sig = ttest.pvalue < (0.05 / 14)\n",
    "    evolution = (\n",
    "        \"{:.2f}\".format(\n",
    "            ((np.mean(merger.median_y) / np.mean(merger.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "\n",
    "    test_by_capital.append((capital, len(merger), evolution, alone_sig, bonf_sig))\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_by_capital,\n",
    "    columns=[\n",
    "        \"Capital\",\n",
    "        \"numpoints\",\n",
    "        \"pollution_evolution\",\n",
    "        \"alone_significant\",\n",
    "        \"bonferroni_significant\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965904f",
   "metadata": {},
   "source": [
    "This is perhaps the surprise of this study. We find that between 2020 and 2022, the only significant changes show that the cities are doing better in 2022 in terms of air pollution, the only exception being Barcelona (Spain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec46bc",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c0d7d",
   "metadata": {},
   "source": [
    "Let us then conclude about the NO2 air pollution. We have established the following :\n",
    "- Air pollution typically goes down with time in the fourteen capital cities we studied.\n",
    "- The year 2020 is special, as it shows a massive drop worldwide. We attribute this to Covid.\n",
    "- The air pollution typically does not go back up after Covid. This is surprising, as one might expect air pollution to go back up after the lockdowns are finished. This is not the case.\n",
    "\n",
    "There are a few limitations in this study :\n",
    "- We do not study pollution by country, but by capital city. If we want to draw nationwide conclusions, we need to assume that the pollution of the capital city is a good proxy for the country. This may or may not be the case.\n",
    "- We assumed that we could split the timeline (before, during, after) as (2019, 2020, 2021+). This is a decent compromise in order to be able to compare the evolution country-wise, but it can be a bit rough.\n",
    "\n",
    "This concludes this study about air pollution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3f0f4-42c9-400a-95e5-42ed4bd7c91b",
   "metadata": {},
   "source": [
    "## Plastic pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_plastic = pd.read_csv(\"Data/Additional/plastic/global-plastics-production.csv\")\n",
    "# source : https://ourworldindata.org/plastic-pollution\n",
    "global_plastic = global_plastic.rename(\n",
    "    columns={\"Global plastics production\": \"Global_plastics_production\"}\n",
    ")\n",
    "global_plastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb19e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd = pd.read_excel(\"Data/Additional/plastic/oecd_source.xlsx\")\n",
    "# source: https://www.oecd.org/newsroom/plastic-pollution-is-growing-relentlessly-as-waste-management-and-recycling-fall-short.htm\n",
    "oecd = oecd.dropna().rename(\n",
    "    columns={\n",
    "        \"Unnamed: 0\": \"Region\",\n",
    "        \"Unnamed: 1\": \"2020\",\n",
    "        \"Unnamed: 2\": \"2019\",\n",
    "        \"Unnamed: 3\": \"pre-COVID 2020 projection\",\n",
    "        \"Unnamed: 4\": \"Absolute 2020 versus 2019\",\n",
    "    }\n",
    ")\n",
    "oecd = oecd.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102583f-7859-45f6-90f9-50be03d7d751",
   "metadata": {},
   "source": [
    "## Looking at the difference between the amount of plastic produced in 2020 and 2019 per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f22e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=oecd, x=\"Absolute 2020 versus 2019\", y=\"Region\").set(title=\"difference between the amount of plastic produced in 2020 and 2019 per region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718608e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont have 2020 data here\n",
    "sns.lineplot(data=global_plastic, x=\"Year\", y=\"Global_plastics_production\").set(title=\"global plastic production per year from 1950 to 2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dc183-bca8-47b0-a8c2-df2ffd20cb9d",
   "metadata": {},
   "source": [
    "Now we want to fit a regression model in order to predict what will be the global plastic production in 2020 given the data from 1950 to 2019, the goal is to see how much higher that number would have been if covid was not there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(\n",
    "    formula=\"Global_plastics_production ~ +np.exp(Year/1950) +Year \",\n",
    "    data=global_plastic,\n",
    ")\n",
    "model = mod.fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69712b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = global_plastic[\"Year\"]\n",
    "plt.plot(X, model.predict(X), color=\"k\", label=\"Regression model\")\n",
    "\n",
    "plt.xlabel('year') \n",
    "plt.ylabel('model prediction') \n",
    "plt.title('The regression model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00041b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(Year=2020)\n",
    "\n",
    "d2020 = pd.DataFrame(data, index=[0])\n",
    "plastic_prediction_2020_if_not_covid = model.predict(d2020)\n",
    "real_plastic_prodction_2020=oecd['2020'][49]*10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poucentage_de_reduction=((real_plastic_prodction_2020 - plastic_prediction_2020_if_not_covid) / plastic_prediction_2020_if_not_covid) * 100\n",
    "print(poucentage_de_reduction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9f13b-e8db-45dd-a89a-ae2942228169",
   "metadata": {},
   "source": [
    "We can see that compared to our prediction the amount of plastic produced in 2020 is 0.06% smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244540ad-3007-47f2-add6-358b4601f89c",
   "metadata": {},
   "source": [
    "## Studying the plastic production in the EU\n",
    "In the first part we study the plastic produced for food packaging, Then the whole amount of plastic produced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_plastic_packaging = pd.read_csv(\n",
    "    \"Data/Additional/plastic/sts_inpr_m__custom_3782744_linear.csv\"\n",
    ")\n",
    "# source= https://ec.europa.eu/eurostat/databrowser/view/STS_INPR_M/default/table?lang=en production of plastic packages in europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_nace_r2_c222 = eu_plastic_packaging[eu_plastic_packaging[\"nace_r2\"] == \"C2222\"].sort_values(\"TIME_PERIOD\")\n",
    "\n",
    "for region in set(eu_nace_r2_c222[\"geo\"]):\n",
    "    df = eu_nace_r2_c222[eu_nace_r2_c222[\"geo\"] == region]\n",
    "    if len(df[\"OBS_VALUE\"].dropna()) == 0:\n",
    "        continue\n",
    "\n",
    "    df.plot.line(\n",
    "        x=\"TIME_PERIOD\", y=\"OBS_VALUE\", title=\"plastic produced for packaging in \" +region, rot=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51cd4b-6e74-4d82-8b92-62954f12b322",
   "metadata": {},
   "source": [
    "Notice the drop around 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9083727",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = eu_plastic_packaging[\n",
    "    (eu_plastic_packaging[\"nace_r2\"] == \"C2222\")\n",
    "    & (eu_plastic_packaging[\"TIME_PERIOD\"] >= \"2020-01\")\n",
    "]\n",
    "old = eu_plastic_packaging[\n",
    "    (eu_plastic_packaging[\"nace_r2\"] == \"C2222\")\n",
    "    & (eu_plastic_packaging[\"TIME_PERIOD\"] < \"2020-01\")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ae4ce50",
   "metadata": {},
   "source": [
    "EL is greece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb3ba9-4948-4e8c-9a55-013b7fbcd664",
   "metadata": {},
   "source": [
    "for each region we compare the production before and after covid to see how the pandemic affected the production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in set(\n",
    "    eu_plastic_packaging[eu_plastic_packaging[\"nace_r2\"] == \"C2222\"][\"geo\"]\n",
    "):\n",
    "    if region=='IE' or region=='UK' or region=='EU28': # we have no data for these \n",
    "        continue\n",
    "    print(\n",
    "        \"difference in the mean ammount of plastic produced before and after covid in \"+region,\n",
    "        stats.ttest_ind(\n",
    "            new[new[\"geo\"] == region][\"OBS_VALUE\"].dropna(),\n",
    "            old[old[\"geo\"] == region][\"OBS_VALUE\"].dropna(),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a81d79-7a37-4d35-90e0-9561aca4def8",
   "metadata": {},
   "source": [
    "it seems for France and Germany the data shows a significant drop in production, while for spain Greece and turkey we see an increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacture of plastics products by month by country\n",
    "full_plastic_production = pd.read_csv(\n",
    "    \"Data/Additional/plastic/sts_inpr_m__custom_3857183_linear.csv\"\n",
    ")\n",
    "# source: https://ec.europa.eu/eurostat/databrowser/view/STS_INPR_M__custom_3857183/default/table?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eceb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_plastic_production = full_plastic_production[\n",
    "    full_plastic_production[\"TIME_PERIOD\"] >= \"2018-01\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cda3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_c222 = full_plastic_production[full_plastic_production[\"nace_r2\"] == \"C222\"].sort_values(\"TIME_PERIOD\")\n",
    "for region in set(r2_c222[\"geo\"]):\n",
    "    df = r2_c222[r2_c222[\"geo\"] == region]\n",
    "    if len(df[\"OBS_VALUE\"].dropna()) == 0:\n",
    "        continue\n",
    "\n",
    "    df.plot.line(x=\"TIME_PERIOD\", y=\"OBS_VALUE\", title=\"plastic produced in \" +region, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56478d4",
   "metadata": {},
   "source": [
    "notice how the drop during 2020 usually wider than the others and slightly deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff860e-300b-457c-bab2-39ed755b585d",
   "metadata": {},
   "source": [
    "## Study of the recycling rates and recovery rates\n",
    "The goal here is to see how the recycling and recovery rates were affected by covid, since if we see a drop in plastic production it would be hard to state that it would lead to a drop in plastic pollution without studying first how covid impacted plastic management facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b9cff",
   "metadata": {},
   "source": [
    "source: https://ec.europa.eu/eurostat/databrowser/view/TEN00063__custom_3793752/default/table?lang=en\n",
    "\n",
    "Recycling rates for packaging waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eee22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recycling_rates_eu = pd.read_csv(\n",
    "    \"Data/Additional/plastic/ten00063__custom_3793752_linear.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62242c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we filter for plastic waste\n",
    "Recycling_rates_eu = Recycling_rates_eu[Recycling_rates_eu[\"waste\"] == \"W150102\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(\n",
    "    Recycling_rates_eu[Recycling_rates_eu[\"TIME_PERIOD\"] == 2020][\"OBS_VALUE\"],\n",
    "    Recycling_rates_eu[Recycling_rates_eu[\"TIME_PERIOD\"] == 2019][\"OBS_VALUE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fdb7c-b351-4bbb-bc4c-4206f8983469",
   "metadata": {},
   "source": [
    "The data shows that the recycling rates are unaffected by covid, thus we can conclude that the recycling rates did not change from 2019 to 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c787dc",
   "metadata": {},
   "source": [
    "Rate of recovery or incineration at waste incineration plants with energy recovery for the purposes of Article 6(1) of Directive 94/62/EC means the total quantity of packaging waste recovered or incinerated at waste incineration plants with energy recovery, divided by the total quantity of generated packaging waste https://ec.europa.eu/eurostat/databrowser/view/ten00062/default/table?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_rates_eu = pd.read_csv(\"Data/Additional/plastic/ten00062_linear.csv\")\n",
    "recovery_rates_eu = recovery_rates_eu[recovery_rates_eu[\"waste\"] == \"W150102\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(\n",
    "    recovery_rates_eu[recovery_rates_eu[\"TIME_PERIOD\"] == 2020][\"OBS_VALUE\"],\n",
    "    recovery_rates_eu[recovery_rates_eu[\"TIME_PERIOD\"] == 2019][\"OBS_VALUE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360170d-3e64-467f-9052-46c641decfda",
   "metadata": {},
   "source": [
    "The data shows that the recovery rates are unaffected by covid, thus we can conclude that the recovery rates did not change from 2019 to 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c9bb3-86e7-4669-b92d-3ed036c2a01a",
   "metadata": {},
   "source": [
    "## Study of the waste generated by households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste = pd.read_csv(\"Data/Additional/plastic/ten00110_linear.csv\")\n",
    "# https://ec.europa.eu/eurostat/databrowser/view/TEN00110/default/table?lang=en&category=env.env_was.env_wasgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste = eu_waste[eu_waste[\"waste\"] == \"TOTAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste[eu_waste[\"TIME_PERIOD\"] == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=eu_waste, x=\"TIME_PERIOD\", y=\"OBS_VALUE\").set(title=\"Household waste generated per year in the EU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56945501-9d5f-447d-b4b6-e799ccf54535",
   "metadata": {},
   "source": [
    "We can see a drop of 0.5 in 2020, however, due to the low amount of data the confidence intervals are quite large, so it would be hard to perform statistical tests and get a meaningful result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65314c1c-173c-481c-bf60-7bd034953900",
   "metadata": {},
   "source": [
    "## Study of the total amount of waste generated by households and businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste = pd.read_csv(\"Data/Additional/plastic/ten00108_linear.csv\")\n",
    "eu_waste = eu_waste[eu_waste[\"waste\"] == \"TOTAL\"]\n",
    "# https://ec.europa.eu/eurostat/databrowser/view/TEN00108/default/table?lang=en&category=env.env_was.env_wasgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=eu_waste, x=\"TIME_PERIOD\", y=\"OBS_VALUE\").set(title=\"Total amount of waste generated by the EU and buisnesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337db5f-5e58-4f9a-b0de-ef481ceb4e1b",
   "metadata": {},
   "source": [
    "We can see a drop of almost 1 in 2020, however, due to the low amount of data the confidence intervals are quite large, so it would be hard to perform statistical tests and get a meaningful result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0055a1b-b8a1-4f9d-b591-4289279adddb",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "We saw that in some countries the plastic production experienced a significant decrease during covid, especially around 2020 we saw quite a drop probably due to lockdown. However, recycling rates and recovery rates did not experience significant change, More over The amount of waste generated decreased between 2018 and 2020 although one might question the validity of this due to a low amount of data.\n",
    "\n",
    "Therefore with those results one might conclude that since less plastic was produced and with no impact on the recycling and recovery rates this might suggest that covid caused a drop plastic polution in the EU.\n",
    "\n",
    "\n",
    "This analysis can be improved since one main issue is that the EU data lacks the data for some countries like Switzerland Poland and so on.\n",
    "This causes some confidence intervals to inflate and thus makes some hypothesis hard to test.\n",
    "We mainly focused on the EU and therefore we can improve it by adding other regions of the world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bf5e7-faef-4a4b-a267-cdc16201d5e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 3 : Studying the awareness versus the actual pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849de983-5e1d-4856-89de-81f53f0562a5",
   "metadata": {},
   "source": [
    "The goal is to establish whether there is a link between awareness (i.e. Wikipedia views) and actual ground measurements about pollution. We will perform two experiments for each country : \n",
    "- **intervention analysis** : we find the peak of wikipedia views for a given wikipedia subject or page in 2020 (the peak of awareness, which we call the intervention) and check whether this peak translates to a significant change in empirical pollution. \n",
    "- **granger causality testing** : we test whether a given timeseries (wikipedia views) can be used to linearly predict the future of another timeseries (say, pollution). This gives us a hint about the temporal relationship between two observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbe441-1fb2-4344-a4e9-5471f247944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = sum_environment_df.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2986f1-4322-4082-858a-02c956405922",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e34ac-0504-417e-82d7-4ebad4aa8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_language_to_dataset = {\n",
    "    'ja' : full_datasets[0], \n",
    "    'it' : full_datasets[1], \n",
    "    'da' : full_datasets[2],\n",
    "    'tr' : full_datasets[3], \n",
    "    'no' : full_datasets[4], \n",
    "    'en' : full_datasets[5], \n",
    "    'sr' : full_datasets[7], #careful : we swap indices here \n",
    "    'sv' : full_datasets[6], \n",
    "    'nl' : full_datasets[8], \n",
    "    'de' : full_datasets[9], \n",
    "    'fr' : full_datasets[10], \n",
    "    'ca' : full_datasets[11], \n",
    "    'ko' : full_datasets[12], \n",
    "    'fi' : full_datasets[13]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f39d1c-4348-426e-86ed-d78de5289874",
   "metadata": {},
   "outputs": [],
   "source": [
    "{(k, v[0]) for (k, v) in wikipedia_language_to_dataset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4829d-d7c8-46bc-8eca-34926074c233",
   "metadata": {},
   "source": [
    "First, we will perform intervention analysis. The first thing we need is to find the peaks of awareness in all fourteen countries in the environment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ac122-2064-4d2a-8049-d557f7bb014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak2020 = {}\n",
    "for code in country_codes:\n",
    "    datapoints = sum_environment_df[sum_environment_df.language == code]\n",
    "    datapoints = datapoints[datapoints.date.dt.year == 2020]\n",
    "    peak = datapoints.environment_views.max()\n",
    "    peakline = datapoints[datapoints.environment_views == peak].iloc[0]\n",
    "    peak2020[code] = peakline['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38a1bf-de5a-479b-9e27-cbd9ad578bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d00ca-1c2c-4361-961b-22f227a39903",
   "metadata": {},
   "source": [
    "Now that we have all peaks, we can look at the 365 previous days and the 365 following years in air pollution and analyze whether there is a significant difference in air quality. This will give us an idea of the relationship between awareness of environment problems and the actual state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf3e1c-5ff9-4cc6-840a-8190f58acb34",
   "metadata": {},
   "source": [
    "#### Can wikipedia awareness be used to model an intervention in the air pollution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e35ce4-9a81-4996-9cf4-5826e84d51e9",
   "metadata": {},
   "source": [
    "At first, let us visualize the data which we analyze : we want to see whether there is a significant difference of air quality before and after the wikipedia views peak of 2020. We will look at both the full timeseries with a change of color at the date of the peak, along with a boxplot to show the difference in quartile-based statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d420217-5ff8-408c-b3db-fec04cac55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for code in country_codes:\n",
    "    dataset = wikipedia_language_to_dataset[code][1]\n",
    "    dataset[\"dt\"] = pd.to_datetime(dataset.Date)\n",
    "    dataset[\"yearless\"] = dataset.Date.str[5:]\n",
    "    peak = peak2020[code]\n",
    "    \n",
    "    before = dataset[(dataset[\"dt\"] >= peak - datetime.timedelta(weeks=52)) & (dataset[\"dt\"] < peak)][['Date', 'median']]\n",
    "    after  = dataset[(dataset[\"dt\"] < peak + datetime.timedelta(weeks=52)) & (dataset[\"dt\"] >= peak)][['Date', 'median']]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(code)\n",
    "    fig.set_size_inches(13, 5)\n",
    "    ax1.plot_date(matplotlib.dates.datestr2num(before[\"Date\"]), before['median'], tz=\"UTC+1\", fmt=\"-\")\n",
    "    ax1.plot_date(matplotlib.dates.datestr2num(after[\"Date\"]), after['median'], tz=\"UTC+1\", fmt=\"-\")\n",
    "    \n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    ax2.boxplot([before[\"median\"], after[\"median\"]])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f2ec0-f827-4991-a1d9-8eedc8a82c66",
   "metadata": {},
   "source": [
    "Here's a few interesting results by themselves : \n",
    "- First of all, most countries visibly have less air pollution after the peak of their environment Wikipedia page of 2020. We will analyze this further in the next cell. That alone is however not enough to explain the whole story : both periods of 365 days have strong similarities\n",
    "- Analyzing in that direction, there seems to be a group of countries that stand out : Japan, Italy, Turkey, Norway, the U.S., Serbia, the Netherlands, Norway, Korea and Finland all have a air pollution that is U-shaped, meaning they are highly seasonal and binary (summer = no pollution, winter = strong pollution). France and Germany also show a similar behavior, but with a seemingly higher variance. This is hardly the case for the last two : Denmark and Sweden are less well-behaved in terms a pollution seasonality. Looking at it globally, we can conclude that there is a strong seasonality of pollution wordlwide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d424d-85f2-4344-8f18-eff683960ab2",
   "metadata": {},
   "source": [
    "Then, we can dig into the significance of these results : is it generally true that the peak of awareness marks a significant difference in air quality in a given country ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5d5e2-3876-4c22-aa32-197122187446",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for code in country_codes:\n",
    "    dataset = wikipedia_language_to_dataset[code][1]\n",
    "    dataset[\"dt\"] = pd.to_datetime(dataset.Date)\n",
    "    dataset[\"yearless\"] = dataset.Date.str[5:]\n",
    "    peak = peak2020[code]\n",
    "    \n",
    "    before = dataset[(dataset[\"dt\"] >= peak - datetime.timedelta(weeks=52)) & (dataset[\"dt\"] < peak)][['yearless', 'median']]\n",
    "    after  = dataset[(dataset[\"dt\"] < peak + datetime.timedelta(weeks=52)) & (dataset[\"dt\"] >= peak)][['yearless', 'median']]\n",
    "    \n",
    "    yearComp = before.merge(after, on='yearless')\n",
    "    pval = stats.ttest_rel(yearComp['median_x'], yearComp['median_y']).pvalue\n",
    "    evolution = (\n",
    "        \"{:+.2f}\".format(\n",
    "            ((np.mean(yearComp.median_y) / np.mean(yearComp.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "    sig_alone = pval < 0.05\n",
    "    sig_bonf = pval < 0.05/14\n",
    "    results.append((code, pval, evolution, sig_alone, sig_bonf))\n",
    "    \n",
    "pd.DataFrame(results, columns = ['country', 'pvalue', 'evolution', 'sig_alone', 'sig_bonferonni'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f52b60-0492-4cc0-88e1-5d858fb50841",
   "metadata": {},
   "source": [
    "These results are in line with the rest of the air pollution analysis earlier : countries significantly reduced their air pollution between these two periods. The only exception is again Turkey. This [Wikipedia page](https://en.wikipedia.org/wiki/Air_pollution_in_Turkey#Nitrogen_oxides) gives a bit of an explanation : NOx car pollution and lack of pollution regulation are a large part of the problem. We however could not explain why the pollution goes up instead of stagnating, for example. Serbia also shows a slight increase in air pollution, but this is not a significant change from before the peak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b66c7-0db1-4d28-85b8-1ecdd4006e6b",
   "metadata": {},
   "source": [
    "#### What can we conclude from this analysis of view peak vs. air pollution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c27fc3-f9a0-49ec-951e-53ff880aa845",
   "metadata": {},
   "source": [
    "We can establish that for the capitals of most countries studied here, **air pollution can reasonably be used as an intervention in air pollution**. However, this is not a causal analysis : we cannot really claim that awareness made the pollution go down, because of two related remarks. First, there is a strong confounder : Covid probably made people read the wikipedia page about air pollution, and at the same time made them pollute less. Second, we cannot establish causality in such sequential data, as we would need to also witness an alternative universe where covid did not happen. It is however not out of the question that awareness causes a drop in pollution : being locked down during Covid, people may have had more time to learn and think about their environment.\n",
    "\n",
    "Bad news aside, we did learn some facts : air pollution gets significantly better after people are most aware about the problem of air pollution in their respective country. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613edef-2e0b-472b-8324-cb1d6f5d55f7",
   "metadata": {},
   "source": [
    "### Daily views vs actual pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53e94e-89d2-427c-adb3-4b9b0640a8ae",
   "metadata": {},
   "source": [
    "We can also ask a related but different question : is it true that the value of wikipedia views of the environment topic can linearly predict the air pollution ? \n",
    "\n",
    "Here, the Granger test can help us. The [Granger causality test](https://en.wikipedia.org/wiki/Granger_causality) is a hypothesis testing tool that takes two time-series as input and checks whether the first one is a good linear predictor for the values of the next. It enables us to do some lag analysis, for example verifying whether the past of the first time series can predict the value of the other at a future day. Here's the implementation we will use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd453ee7-d55c-469c-8355-35f51633f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "grangercausalitytests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dd9e7-2af7-4c43-b96d-ee8b7c64ad78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "802e5aad-2ed6-4e67-9d05-2acb80d2f6d8",
   "metadata": {},
   "source": [
    "Since we are concerned with data that goes beyond what the original Coronawiki dataset gives us, we can load the extended version that goes further than 2020.\n",
    "\n",
    "We loaded all wikipedia views on the environment topic from 2015-07 to the end of 2022. This extends the data in both directions, giving us more datapoints to merge with the air quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c602fa-b123-46db-9328-45ff447d8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_extended = pd.read_csv(\"Data/Additional/langviews/final/aggregated_views.csv\")\n",
    "sum_environment_extended.date = pd.to_datetime(sum_environment_extended.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341c43b-984c-4c0f-960c-bf5455c37d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8015487-dc75-4e68-b5ff-b7a41062d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for code in country_codes:\n",
    "    \n",
    "    env = sum_environment_extended[[code, 'date']]\n",
    "    air = wikipedia_language_to_dataset[code][1]\n",
    "    merged = env.merge(air, left_on=\"date\", right_on=\"dt\")\n",
    "    \n",
    "    print(code)\n",
    "                                                           #jesus the access patterns are annoying\n",
    "    pval = grangercausalitytests(merged[['median', code]], [1])[1][0]['ssr_chi2test'][1]\n",
    "    alone_sig = pval < 0.05\n",
    "    bonf_sig = pval < 0.05/14\n",
    "    \n",
    "    res.append((code, pval, alone_sig, bonf_sig))\n",
    "    \n",
    "    print(\"\\n\\n===================\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d5d90-fbbf-4740-bb35-e8b0c2f44102",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_poll_granger = pd.DataFrame(res)\n",
    "air_poll_granger.columns = [\"code\", \"pvalue\", \"alone_sig\", \"bonf_sig\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1595f-c846-45c8-aaa9-8584b9dff4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_poll_granger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2cd53-2072-47ee-b8fe-13643aacda21",
   "metadata": {},
   "source": [
    "The p-values are very binary : either something is not significant alone, or it is significant even under the Bonferonni correction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d42c0-905b-479f-b400-7e0b9c244574",
   "metadata": {},
   "outputs": [],
   "source": [
    "(air_poll_granger.alone_sig == air_poll_granger.bonf_sig).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2034f-ebaf-4d02-8c83-957d0554f5c1",
   "metadata": {},
   "source": [
    "In other words, there is no need for a finer correction like Benjamin-Hochberg FDR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee2285-d99b-4a49-8e67-b2f319ed3520",
   "metadata": {},
   "source": [
    "We find that for most countries, past wikipedia views make for a good linear predictor of the future of air pollution. This even holds for Turkey where the air pollution got worse during Covid.\n",
    "\n",
    "An interesting case is that of Japan and South Korea which have very insignificant p-values (>.2), suggesting that day-to-day linear prediction is not very convincing for these two countries. We note that these are the only Eastern-Asian countries in the dataset. An interesting extension to this project could be to check whether this extends further to other countries in the area.\n",
    "\n",
    "The odd one out is then Serbia, with a p-value of 0.21. There is an explanation which we find satisfying : while air pollution follows the same patterns as similar European countries, the wikipedia views on the environment topic in Serbia are fairly unusual when compared to neighboring countries. Indeed, there is a massive spike around 2020-04, the time at which Covid hit the country. Previously, the article had significnatly fewer views. For most other countries, the process was much more continuous. We see no particular reason as to why the awareness differs, but this explains why the Granger test sees no significant use of wikipedia views for predicting the air pollution.\n",
    "\n",
    "For all others, the linear prediction works out fine. The model is confident that the past of the wikipedia views is a useful tool to predict the air pollution of the next day in the capital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c63e5-095a-4bee-b31a-b1c829b7db67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc4f5c91-b457-452e-ae1a-1389a0d3bcf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 4 : What if ? and what happens next ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184b6a8-2329-48c9-9c97-cd580eac0872",
   "metadata": {},
   "source": [
    "In this final part, we want to create a hypothetical scenario of 2022 using statistical forecasting without the data from that year. The idea is to show whether the direction air pollution is taking is predictable, and where it is headed.\n",
    "\n",
    "For the statistical forecasting, we will use the [SARIMA model](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average#Variations_and_extensions) which enables the prediction of the future of a timeseries by using the previous data points and accounting for seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1fbd1-348a-4c4f-9434-e3a9a8a12b84",
   "metadata": {},
   "source": [
    "We will analyze the data in the following way : we will predict whether the air pollution in the period of 2022 can be meaningfully predicted from the previous years.\n",
    "Ideally, the plan was to attempt to predict what would have heppened by training the model on air data from 2015 to 2019 and predicting 2020-2022, in order to compare with what actually happened.\n",
    "It turns out, the air data is not available so long in the past. We had to modify the analysis to analyze from 2019-2021 and predict 2022, to give the model data on which it could be trained.\n",
    "This modifies a bit the intent of the analysis. Instead of seeing whether there is a downwards perturbance in air pollution (which we already know happened), we now ask a slightly different question : \"does the air pollution in 2022 significantly change from the trends of 2019-2021, and if so, what is the direction of the change?\". This will give us insights into what the different countries have \"learned\". For example, a year 2022 that is unpredictably low in terms of air pollution means that the country has (for now) learned that it could survive without as much pollution.\n",
    "\n",
    "There could be other explanations (the country never economically recovered from Covid, ...) but there is little we can do to account for this in time. Besides, speaking only in terms of air pollution, the conclusion is the same : the country is for the foreseeable future on its way to have a better air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e0184-f67c-4eab-96ef-687adb4ab358",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_list = []\n",
    "for country in country_codes : \n",
    "    try:\n",
    "        print(\"=========================================================\")\n",
    "        print(country, \"\\n\") \n",
    "        \n",
    "        dataset = wikipedia_language_to_dataset[country][1].copy()\n",
    "        dataset = dataset[dataset[\"dt\"].dt.year <= 2021]\n",
    "        dataset = dataset.set_index(\"dt\")\n",
    "        dataset = dataset[\"median\"]\n",
    "        decomposition = sm.tsa.seasonal_decompose(dataset.resample('MS').mean(), model='additive')\n",
    "        fig = decomposition.plot()\n",
    "        plt.show()\n",
    "\n",
    "        mod = sm.tsa.statespace.SARIMAX(dataset.resample('MS').mean(),\n",
    "                                        order=(0, 1, 1),\n",
    "                                        seasonal_order=(1, 1, 1, 12))\n",
    "        results = mod.fit(method = 'powell')\n",
    "\n",
    "        pred = results.get_prediction(start=pd.to_datetime('2022-01-01'), end = pd.to_datetime('2023-01-01'), dynamic=False)\n",
    "        pred.predicted_mean.plot()\n",
    "        dataset = wikipedia_language_to_dataset[country][1].copy()\n",
    "        dataset = dataset.set_index(\"dt\")\n",
    "        dataset = dataset[\"median\"]\n",
    "        dataset = dataset.rename(\"actual_mean\")\n",
    "        dataset.resample('MS').mean().plot()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        dataset_2022 = wikipedia_language_to_dataset[country][1]\n",
    "        dataset_2022 = dataset_2022[dataset_2022[\"dt\"].dt.year > 2021]\n",
    "        dataset_2022 = dataset_2022.set_index(\"dt\")\n",
    "        dataset_2022 = dataset_2022[\"median\"].resample('MS').mean()\n",
    "        \n",
    "        joined_true_and_pred = pd.concat([pred.predicted_mean, dataset_2022], axis = 1, join = 'inner')\n",
    "        mae = mean_absolute_error(joined_true_and_pred[\"median\"], joined_true_and_pred[\"predicted_mean\"])\n",
    "        \n",
    "        expectedMean = dataset_2022.mean()\n",
    "        predMean = pred.predicted_mean.mean()\n",
    "        relative_diff = (\n",
    "            \"{:+.2f}\".format(\n",
    "                ((predMean / expectedMean) - 1) * 100\n",
    "            )+ \"%\"\n",
    "        )\n",
    "        sarima_list.append((country, expectedMean, predMean, relative_diff, mae))\n",
    "        print(\"\\n\\n\\n\")\n",
    "    except:\n",
    "        print(\"not enough data for sarima!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77927fbb-1d4c-4163-8ee3-415996d6c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimaDf = pd.DataFrame(sarima_list, columns = [\"Country\", \"expected\", \"prediction\", \"relative_diff\", \"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f613e71-97bd-4841-aed5-cfdaf1e06ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimaDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac39c75-15ec-4882-8cb5-da5c4a1d7585",
   "metadata": {},
   "source": [
    "Here, we have fairly binary results : \n",
    "- Either the country has an actual pollution that is lower than the prediction (Japan, Turkey, Norway, the U.S., Germany, France, Korea, Finland). The country that has the largest MAE is in this category: it is Turkey, which has a massive drop in air pollution in 2022, while its trend was increasing before that year.\n",
    "- Or it has an actual pollution that is higher than the prediction (Italy, Serbia, Sweden, the Netherlands, Catalonia). In this case, the largest difference between the actual pollution is Sweden, which is explained by noticing that the values for Sweden are usually really low, and that there is an small, unexplained peak in the air pollution in 2022. This is seen in the Sarima trends graph of the country. Most countries in this category behave this way.\n",
    "\n",
    "We also note that Denmark is the only country with a full month of data missing, so it is not included in this study. \n",
    "\n",
    "We can conclude in the following way :\n",
    "For most countries, it holds that either the model predicts a higher pollution than reality for 2022 or the country emissions were already fairly low. This, in turn, suggests that humans were indeed taught a lesson by Covid in terms of climate change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
