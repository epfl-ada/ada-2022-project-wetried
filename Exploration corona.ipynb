{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c176586",
   "metadata": {},
   "source": [
    "# Covid-19 and the environment, a statistics case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222e4cd",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to discover potential links between the awareness of the environment and the actual state of the environment around the time of the Covid-19 lockdowns.\n",
    "- First, we explore the dataset that was given in the problem statement. We merge, filter and prepare the data for further use.\n",
    "- Then, we present the dataset which we introduced ourselves : more data from Wikipedia (nicknamed \"precise wikipedia) the World Air Quality index and the plastics production statistics from Eurostat.\n",
    "- Then, we analyze the links between wikipedia views with indicator of actual pollution.\n",
    "- Finally, we present a statistical extrapolation of environment data without Covid and compare it to the actual data, to give an idea of what might have happened if Covid hadn't happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab15c9f-53eb-4e86-8b6b-809388067dc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1 : Coronawiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd50edc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99dcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statsmodels.stats import diagnostic\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf09fb3",
   "metadata": {},
   "source": [
    "## Timeseries <a id='timeseries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e3f55",
   "metadata": {},
   "source": [
    "The most important data we have in this dataset are time series of the Wikipedia views from 2018 to July 2020 for 14 different languages: one part are the total views for all of that language's wikipedia, a second part are the views for the articles that are related to Covid-19, as well as the percentage. Finally, we also have for the same window of time the views for different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49233ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = pd.read_json(\"Data/aggregated_timeseries.json.gz\")\n",
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e32552",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_unique = timeseries.columns.map(lambda x: x[:2]).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f8e9c",
   "metadata": {},
   "source": [
    "Correspondence:\n",
    "- ja -> Japanese\n",
    "- it -> Italian\n",
    "- da -> Danish\n",
    "- tr -> Turkish\n",
    "- no -> Norwegian\n",
    "- en -> English\n",
    "- sr -> Serbian\n",
    "- sv -> Swedish\n",
    "- nl -> Dutch\n",
    "- de -> German\n",
    "- fr -> French\n",
    "- ca -> Catalan\n",
    "- ko -> Korean\n",
    "- fi -> Finnish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9d53f",
   "metadata": {},
   "source": [
    "### Splitting the timeseries data into different dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed536c66",
   "metadata": {},
   "source": [
    "As we can see, the data's format isn't ideal: for each language, the data is split into 3 Python dictionaries corresponding to the data described above, and it would be nice to separate these pieces of data to be able to read directly for each date, for example, the total number of views accross all languages, instead of having to iterate over each language's dictionnary every time.\n",
    "\n",
    "This will also make the analysis phase easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03be51e",
   "metadata": {},
   "source": [
    "### Total sum of views, views of articles related to Covid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fcd7a8",
   "metadata": {},
   "source": [
    "<a id='extraction_format'></a>\n",
    "In this part of the code we extract two following kind of data, for each date:\n",
    "- For every language's Wikipedia, the total number of views on that particular date\n",
    "- For every language's Wikipedia, the total number of views for articles related to Covid-19 on that particular date\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Every resulting dataframe will have the following format:\n",
    "\n",
    " Column name          | Description                                                                                                                                                                                       |\n",
    "|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|---|---|\n",
    "| date           | A particular date between January 2018 (inclusive) and July of 2020 (inclusive)                                                                                                                                             |\n",
    "| language       | The corresponding wikipedia language                                                                |\n",
    " |   |   |\n",
    "| views  | The number of views (total or only for articles related to Covid-19) for this language and date\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We also extract another dataframe that simply maps for each language the number of articles that were considered in the original experiment. Finally, a last dataframe which is redundant with the two first ones is present: one which gives for each day the percentage of views that went to Covid related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_total_sum_dict = {}\n",
    "timeseries_covid_len_dict = {}\n",
    "timeseries_covid_sum_dict = {}\n",
    "timeseries_covid_percent_dict = {}\n",
    "for cn in timeseries.columns:\n",
    "    timeseries_total_sum_dict[cn] = timeseries[cn][\"sum\"]\n",
    "    timeseries_covid_len_dict[cn] = timeseries[cn][\"covid\"][\"len\"]\n",
    "    timeseries_covid_sum_dict[cn] = timeseries[cn][\"covid\"][\"sum\"]\n",
    "    timeseries_covid_percent_dict[cn] = timeseries[cn][\"covid\"][\"percent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b22797",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df = pd.DataFrame.from_dict(timeseries_total_sum_dict, orient=\"index\").T\n",
    "covid_len_data_df = pd.DataFrame.from_dict(\n",
    "    timeseries_covid_len_dict, orient=\"index\", columns=[\"len\"]\n",
    ").T\n",
    "covid_sum_data_df = pd.DataFrame.from_dict(timeseries_covid_sum_dict, orient=\"index\").T\n",
    "covid_percent_data_df = pd.DataFrame.from_dict(\n",
    "    timeseries_covid_percent_dict, orient=\"index\"\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse the .m and the normal columns together, and separate the data\n",
    "new_sum_data_df = pd.DataFrame()\n",
    "new_covid_sum_data_df = pd.DataFrame()\n",
    "for language in languages_unique:\n",
    "    country_sum_data = pd.DataFrame()\n",
    "    country_sum_data[\"views\"] = sum_data_df[language] + sum_data_df[language + \".m\"]\n",
    "    country_sum_data[\"language\"] = language\n",
    "    new_sum_data_df = pd.concat([new_sum_data_df, country_sum_data], axis=0)\n",
    "\n",
    "    country_covid_sum_data = pd.DataFrame()\n",
    "    country_covid_sum_data[\"views\"] = (\n",
    "        covid_sum_data_df[language] + covid_sum_data_df[language + \".m\"]\n",
    "    )\n",
    "    country_covid_sum_data[\"language\"] = language\n",
    "    new_covid_sum_data_df = pd.concat(\n",
    "        [new_covid_sum_data_df, country_covid_sum_data], axis=0\n",
    "    )\n",
    "\n",
    "sum_data_df = new_sum_data_df\n",
    "covid_sum_data_df = new_covid_sum_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4302bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.index = covid_sum_data_df.index = pd.to_datetime(sum_data_df.index)\n",
    "covid_percent_data_df.index = pd.to_datetime(covid_percent_data_df.index)\n",
    "sum_data_df[\"date\"] = covid_sum_data_df[\"date\"] = sum_data_df.index\n",
    "covid_percent_data_df[\"date\"] = covid_percent_data_df.index\n",
    "# covid_sum_data_df = covid_sum_data_df[new_column_order]\n",
    "# covid_percent_data_df = covid_percent_data_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_sum_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e71aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_percent_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde94fb5",
   "metadata": {},
   "source": [
    "### Checking for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e05d8",
   "metadata": {},
   "source": [
    "Before continuing further, let us check for missing data in the timeseries; this will help us avoid bad surprises later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data_df.isnull().any().any(), covid_sum_data_df.isnull().any().any(), covid_percent_data_df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ab332",
   "metadata": {},
   "source": [
    "There appears to be some missing data in the percentage dataframe; let's check by language where that missing data is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fab93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_language = covid_percent_data_df.isnull().any(axis=0)\n",
    "missing_data_language[missing_data_language]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19b08c",
   "metadata": {},
   "source": [
    "Looking at the paper, this corresponds to Swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d61272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomment Run this cell if you want to see how the missing data can be seen in the original timeseries\n",
    "# timeseries.loc[:,'sv']['sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262891f",
   "metadata": {},
   "source": [
    "It appears that, for desktop devices, the views for the Swedish Wikipedia haven't been collected for the whole year 2018. The reason for that is unknown, as a quick search tells us that this version has existed since 2001. The mobile data, however, is available. We will need to take this into account when doing our analysis in the future, for example by not taking into account 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511651c2",
   "metadata": {},
   "source": [
    "### Topics data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2b75a",
   "metadata": {},
   "source": [
    "Now we will extract for each language, all the views per topic in such a way that the data becomes more usable. In the original data, all topic-related information was in a single dictionnary; we're gonna separate them in a way that each column will correspond to a different topic, with each row being a different language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_topics = {}\n",
    "for cn in timeseries.columns:\n",
    "    country_to_topics[cn] = timeseries[cn][\"topics\"]\n",
    "topics_df = pd.DataFrame.from_dict(country_to_topics, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_topics_len = {}\n",
    "countries_to_topics_sum = {}\n",
    "countries_to_topics_percent = {}\n",
    "for country in topics_df.index:\n",
    "    countries_to_topics_len[country] = {}\n",
    "    countries_to_topics_sum[country] = {}\n",
    "    countries_to_topics_percent[country] = {}\n",
    "    for topic in topics_df.columns:\n",
    "        countries_to_topics_len[country][topic] = topics_df.loc[country, topic][\"len\"]\n",
    "        countries_to_topics_sum[country][topic] = topics_df.loc[country, topic][\"sum\"]\n",
    "        countries_to_topics_percent[country][topic] = topics_df.loc[country, topic][\n",
    "            \"percent\"\n",
    "        ]\n",
    "countries_to_topics_len_df = pd.DataFrame.from_dict(\n",
    "    countries_to_topics_len, orient=\"index\"\n",
    ")\n",
    "countries_to_topics_sum_df = pd.DataFrame.from_dict(\n",
    "    countries_to_topics_sum, orient=\"index\"\n",
    ")\n",
    "countries_to_topics_percent_df = pd.DataFrame.from_dict(\n",
    "    countries_to_topics_percent, orient=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_topics_sum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomment and run this cell to see all available topics.\n",
    "# countries_to_topics_sum_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b371c",
   "metadata": {},
   "source": [
    "However, we are not be interested in all available topics. As a matter of fact, for our project, it is only useful to isolate the data about articles related to the environment. Examining the columns, the topic is available in only one of them, so we will extract only that topic in two dataframes that have the same format as [here](#extraction_format) (only difference is that we change the name *views* to *environment_views*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d71373",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df = countries_to_topics_sum_df[\"STEM.Earth and environment\"]\n",
    "percent_environment_df = countries_to_topics_percent_df[\"STEM.Earth and environment\"]\n",
    "country_to_env_data_sum = {}\n",
    "country_to_env_data_percent = {}\n",
    "for country in sum_environment_df.index:\n",
    "    country_to_env_data_sum[country] = sum_environment_df[country]\n",
    "    country_to_env_data_percent[country] = percent_environment_df[country]\n",
    "sum_environment_df = pd.DataFrame.from_dict(country_to_env_data_sum, orient=\"index\").T\n",
    "percent_environment_df = pd.DataFrame.from_dict(\n",
    "    country_to_env_data_percent, orient=\"index\"\n",
    ").T\n",
    "percent_environment_df.index = pd.to_datetime(percent_environment_df.index)\n",
    "\n",
    "new_sum_environment_df = pd.DataFrame()\n",
    "for language in languages_unique:\n",
    "    country_env_sum_data = pd.DataFrame()\n",
    "\n",
    "    country_env_sum_data[\"environment_views\"] = (\n",
    "        sum_environment_df[language] + sum_environment_df[language + \".m\"]\n",
    "    )\n",
    "    country_env_sum_data[\"language\"] = language\n",
    "    new_sum_environment_df = pd.concat(\n",
    "        [new_sum_environment_df, country_env_sum_data], axis=0\n",
    "    )\n",
    "sum_environment_df = new_sum_environment_df\n",
    "sum_environment_df.index = pd.to_datetime(sum_environment_df.index)\n",
    "sum_environment_df[\"date\"] = sum_environment_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f25fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really useful piece of data\n",
    "percent_environment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c98143",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc821f",
   "metadata": {},
   "source": [
    "In the following part, we save the following type of plots:\n",
    "\n",
    "- per language on Wikipedia, three separate line plots showing the evolution of the total number of views, the number of views of articles related to Covid-19, and the number of views of environment articles.\n",
    "- per language on Wikipedia, three separate histograms representing the number of views per day, for the same categories as in the point above.\n",
    "\n",
    "These plots can be found in the 'Figures' folder that will be created after the launch of all cells in this part, but we will plot some examples in the notebook to try and detect some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_path = \"./Figures/\"\n",
    "timeseries_path = \"timeseries/\"\n",
    "hists_path = \"hists/\"\n",
    "total_views_path = \"all_views/\"\n",
    "covid_views_path = \"covid_views/\"\n",
    "topic_views_path = \"topic_views/\"\n",
    "\n",
    "\n",
    "def make_sub_dirs(main_dir):\n",
    "    os.mkdir(main_dir + total_views_path)\n",
    "    os.mkdir(main_dir + covid_views_path)\n",
    "    os.mkdir(main_dir + topic_views_path)\n",
    "    os.mkdir(main_dir + topic_views_path + total_views_path)\n",
    "    os.mkdir(main_dir + topic_views_path + covid_views_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(figures_path):\n",
    "    os.mkdir(figures_path)\n",
    "    os.mkdir(figures_path + timeseries_path)\n",
    "    os.mkdir(figures_path + hists_path)\n",
    "    make_sub_dirs(figures_path + timeseries_path)\n",
    "    make_sub_dirs(figures_path + hists_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_language_views_timeseries(data, country, covid_views=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = data[data[\"language\"] == country]\n",
    "    x = filtered_data.index\n",
    "    y = filtered_data.views\n",
    "    g = sns.lineplot(x=x, y=y)\n",
    "    plt.xticks(fontsize=8)\n",
    "    g.set(xlabel=\"Dates\")\n",
    "    if covid_views:\n",
    "        title = \"Wikipedia page views for articles related to Covid-19 for {}\".format(\n",
    "            country\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia page views for {}\".format(country)\n",
    "    g.set(ylabel=\"Page views\", title=title)\n",
    "    # ax.set(xscale=\"log\")\n",
    "    if covid_views:\n",
    "        plt.savefig(figures_path + timeseries_path + covid_views_path + title + \".jpg\")\n",
    "    else:\n",
    "        plt.savefig(figures_path + timeseries_path + total_views_path + title + \".jpg\")\n",
    "    plt.close(fig)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_language_views(data, country, covid_views=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = data[data[\"language\"] == country]\n",
    "    g = sns.histplot(data=filtered_data, x=\"views\", bins=50)\n",
    "    if covid_views:\n",
    "        title = (\n",
    "            \"Wikipedia views distribution for articles related to Covid-19 {}\".format(\n",
    "                country\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia views distribution for {}\".format(country)\n",
    "    g.set(title=title)\n",
    "    if covid_views:\n",
    "        plt.savefig(figures_path + hists_path + covid_views_path + title + \".jpg\")\n",
    "    else:\n",
    "        plt.savefig(figures_path + hists_path + total_views_path + title + \".jpg\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_code in sum_data_df.language.unique():\n",
    "    lineplot_language_views_timeseries(sum_data_df, country_code)\n",
    "    hist_language_views(sum_data_df, country_code)\n",
    "    lineplot_language_views_timeseries(covid_sum_data_df, country_code, True)\n",
    "    hist_language_views(covid_sum_data_df, country_code, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f235c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_topic_views_timeseries(\n",
    "    topic_data, country, covid_views=False, topic=\"environment\"\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = topic_data[topic_data[\"language\"] == country]\n",
    "    x = filtered_data.index\n",
    "    y = filtered_data.environment_views\n",
    "    g = sns.lineplot(x=x, y=y)\n",
    "    plt.xticks(fontsize=8)\n",
    "    g.set(xlabel=\"Dates\")\n",
    "    if covid_views:\n",
    "        title = \"Wikipedia page views for articles related to Covid-19 for {0} for the {1} topic\".format(\n",
    "            country, topic\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia page views for {0} for the {1} topic\".format(country, topic)\n",
    "    g.set(ylabel=\"Page views\", title=title)\n",
    "    if covid_views:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + timeseries_path\n",
    "            + topic_views_path\n",
    "            + covid_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "    else:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + timeseries_path\n",
    "            + topic_views_path\n",
    "            + total_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_topic_views(topic_data, country, covid_views=False, topic=\"environment\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    filtered_data = topic_data[topic_data[\"language\"] == country]\n",
    "    g = sns.histplot(data=filtered_data, x=\"environment_views\", bins=50)\n",
    "    if covid_views:\n",
    "        title = \"Wikipedia views distribution for articles related to Covid-19 for {0} for the {1} topic\".format(\n",
    "            country, topic\n",
    "        )\n",
    "    else:\n",
    "        title = \"Wikipedia views distribution for {0} for the {1} topic\".format(\n",
    "            country, topic\n",
    "        )\n",
    "    g.set(title=title)\n",
    "    if covid_views:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + hists_path\n",
    "            + topic_views_path\n",
    "            + covid_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "    else:\n",
    "        plt.savefig(\n",
    "            figures_path\n",
    "            + hists_path\n",
    "            + topic_views_path\n",
    "            + total_views_path\n",
    "            + title\n",
    "            + \".jpg\"\n",
    "        )\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941554ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_code in sum_environment_df.language.unique():\n",
    "    lineplot_topic_views_timeseries(sum_environment_df, country_code)\n",
    "    hist_topic_views(sum_environment_df, country_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488e9bb",
   "metadata": {},
   "source": [
    "Let us plot some of the environment views timeseries here, to try and see if some interesting patterns can be seen already. We will choose the Serbian and Danish languages for this part. Note that we're not comparing the values themselves but whether or not there are patterns in either of them, so sharing the y-axis isn't really needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd05c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_chosen = [\"sr\", \"da\"]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(countries_chosen), figsize=(20, 10), dpi=100)\n",
    "for i in range(len(countries_chosen)):\n",
    "    filename = \"Wikipedia page views for {} for the environment topic.jpg\".format(\n",
    "        countries_chosen[i]\n",
    "    )\n",
    "    ax[i].imshow(\n",
    "        plt.imread(fname=\"./Figures/timeseries/topic_views/all_views/\" + filename)\n",
    "    )\n",
    "    ax[i].axis(\"off\")\n",
    "# _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e63156",
   "metadata": {},
   "source": [
    "We can see a slight difference in behavior between these languages around the beginning of 2020:\n",
    "- first, we can see that while the environment views are pretty low before 2020 for the serbian wikipedia, these views go up quickly around the end of March 2020, which when looking at the intervention data seems to coincide with the first serbian lockdown. It is still a bit early however to affirm that one of these elements caused the other.\n",
    "- moving to the danish wikipedia however, there doesn't seem to be a particular rise or fall in the number of environment views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a2de1",
   "metadata": {},
   "source": [
    "We can also see however that for both languages, there seems to be a pattern of evolution of views between the years: for serbian, there is clear decrease between June and Septembre of both 2018 and 2019, which seems to have happened in 2020 as well if we look at the left plot. For danish the same phenomena can be observed as well, this time between July and August of both 2018 and 2019; again, this seems to have happened in 2020 as well if we were to look at the right plot. Looking at the other languages, patterns can also be found accross years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763db0f0",
   "metadata": {},
   "source": [
    "It appears that indeed, the number of views for environmental articles did change in the same timeframe as when Covid-19 first arrived in some of the countries. It's equally important, however, to state that some don't display a change in pattern, and that at this point in the analysis we can't say that Covid-19 indeed caused more or less environment awareness on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57d040",
   "metadata": {},
   "source": [
    "Let us now do another, more statistical analysis; we will test the hypothesis that, in average, and for every language, the average number of environmental views is the same between 2019 (pre-Covid) and 2020 (during the first, biggest wave of Covid). We will test these hypotheses using the $\\alpha = 0.05 $ significance level, as well as using the Bonferonni correction $\\alpha_{c} = \\frac{\\alpha}{n}$, with n = 14 .\n",
    "\n",
    "We match the same date between 2019 and 2020 (up to the 31st of July, as that's when the Coronawiki data ends) for every language, and conduct a t-test for the null hypothesis described above. Note that we consider the samples to be related, as they come from the same language and for the same range of dates. In this setup, a negative t-statistic means that for that language, there are more environment views on average in 2020, and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463964f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_covid_env_topic_views = sum_environment_df[\n",
    "    (sum_environment_df.date < \"2020-01-01\") & (sum_environment_df.date >= \"2019-01-01\")\n",
    "]\n",
    "during_covid_env_topic_views = sum_environment_df[\n",
    "    sum_environment_df.date >= \"2020-01-01\"\n",
    "]\n",
    "for language in sum_environment_df.language.unique():\n",
    "    language_before = before_covid_env_topic_views[\n",
    "        before_covid_env_topic_views.language == language\n",
    "    ].copy()\n",
    "    language_during = during_covid_env_topic_views[\n",
    "        during_covid_env_topic_views.language == language\n",
    "    ].copy()\n",
    "\n",
    "    language_before.date = language_before.date.apply(\n",
    "        lambda date: str(date.month) + \"-\" + str(date.day)\n",
    "    )\n",
    "    language_during.date = language_during.date.apply(\n",
    "        lambda date: str(date.month) + \"-\" + str(date.day)\n",
    "    )\n",
    "    matching = pd.merge(\n",
    "        language_before,\n",
    "        language_during,\n",
    "        on=[\"date\", \"language\"],\n",
    "        suffixes=[\"_before\", \"_during\"],\n",
    "    )\n",
    "    stat, pvalue = stats.ttest_rel(\n",
    "        matching[\"environment_views_before\"], matching[\"environment_views_during\"]\n",
    "    )\n",
    "    print(\"p-value for {0}: {1}\".format(language, pvalue))\n",
    "    if pvalue >= 0.05:\n",
    "        print(\"Don't reject null when alpha = 0.05\")\n",
    "    if pvalue >= (0.05 / 14):\n",
    "        print(\"Don't reject null when using the banferoni correction\")\n",
    "    print(\"Ttest statistic value for {0}: {1}\".format(language, stat))\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e78ea3",
   "metadata": {},
   "source": [
    "It appears that for 11 out of 14 languages, there is indeed a change in how people visit these pages; the null hypothesis is rejected even after applying the Bonferonni correction, seeing how small their p-values are.. Out of these 11 languages, 6 observe a negative t-statistic, i.e. an average increase in 2020, while the other 5 observe a positive t-statistic, i.e an average decrease in 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8f339",
   "metadata": {},
   "source": [
    "## Mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3fec3",
   "metadata": {},
   "source": [
    "The second type of data we have are mobility data that come from two different sources. The first one is from Apple, who stopped giving out the data in April 2022, and the second one is from Google, which is still available, and more up-to-date (17th of October)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f15486",
   "metadata": {},
   "source": [
    "### Apple mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66246be",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_mobility = pd.read_csv(\"Data/applemobilitytrends-2020-04-20.csv.gz\")\n",
    "apple_mobility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53aba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apple_mobility.transportation_type.unique())  # Three types of transportation\n",
    "print(apple_mobility.geo_type.unique())  # Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630f2e4",
   "metadata": {},
   "source": [
    "The mobility data from Apple we have begins in mid-January 2020, and ends that same year in April. This isn't a big time window, and it doesn't appear that there is earlier data as it has been collected specifically for Covid-19 mobility tracking. We could however try to look for newer information (post-April 2020) on the web.\n",
    "\n",
    "Three types of transportation have been tracked here: driving, walking, and transit. We also have two different granularities about the collected data: either country/world region level, or city level, which are often country capitals.\n",
    "\n",
    "Per day and region, we have the pourcentage of the usage of every transportation mode according to some pre-pandemic baseline computed in early 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4570a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_mobility[apple_mobility[\"region\"] == \"Turkey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4655c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_mobility.isnull().any().any()  # There doesn't appear to be null data, but some countries don't have all the transportation types (transit, mostly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates strings to date times\n",
    "time_columns = pd.to_datetime(apple_mobility.columns[3:])\n",
    "apple_mobility.columns = apple_mobility.columns[:3].append(time_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a642c",
   "metadata": {},
   "source": [
    "### Global mobility from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87265bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mobility_report = pd.read_csv(\"Data/Global_Mobility_Report.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    min(global_mobility_report.date.unique()), max(global_mobility_report.date.unique())\n",
    ")\n",
    "global_mobility_report.date = pd.to_datetime(global_mobility_report.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801ab29",
   "metadata": {},
   "source": [
    "The mobility data from Google we have begins in mid-February 2020, and ends that same year in August. This is more than the given Apple data, despite the fact that both collections happened in the context of Covid-19. The full data can be found in the *Outside data* folder.\n",
    "\n",
    "There are more levels of granularity with this data: for example, for the United Arab Emirates, we might simply talk about the whole country, or it could be specified in the column *sub_region_1* that the row is actually focused on the city of Abu Dhabi. This granularity can be made finer with *sub_region_2*.\n",
    "\n",
    "Per day and region, we have the **difference** in pourcentage usage of various location types (workplaces, etc) according to some pre-pandemic baseline computed in the early weeks of 2020. The baseline was computed *per day* , as people can have different behaviors depending on whether it's the weekend or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mobility_report.isnull().sum() / global_mobility_report.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb0512",
   "metadata": {},
   "source": [
    "As expected, we have more coarse grained data (no missing data) than finer grained (many sub_region_1 fields are null, and even more sub_region_2 as well). The metropolitan area is very rarely defined, as almost 99.4% of the field is empty values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa175ea",
   "metadata": {},
   "source": [
    "Looking at the differences from baseline, we remark scarcity as well; apart from workplace locations which has a missing rate of only around 5.04%, others go from 36.4% (for retail) to 53.7% (for parks).\n",
    "\n",
    "We can look in more details at the entries which have the missing values for the differences from baseline; let's check the intersection of these missing values, to see for example if the absence of one field implies the absence of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_missing = (\n",
    "    global_mobility_report.retail_and_recreation_percent_change_from_baseline.isnull()\n",
    ")\n",
    "grocery_pharmecy_missing = (\n",
    "    global_mobility_report.grocery_and_pharmacy_percent_change_from_baseline.isnull()\n",
    ")\n",
    "park_missing = global_mobility_report.parks_percent_change_from_baseline.isnull()\n",
    "transit_stations_missing = (\n",
    "    global_mobility_report.transit_stations_percent_change_from_baseline.isnull()\n",
    ")\n",
    "workplace_missing = (\n",
    "    global_mobility_report.workplaces_percent_change_from_baseline.isnull()\n",
    ")\n",
    "residential_missing = (\n",
    "    global_mobility_report.residential_percent_change_from_baseline.isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing = (\n",
    "    retail_missing\n",
    "    & grocery_pharmecy_missing\n",
    "    & park_missing\n",
    "    & transit_stations_missing\n",
    "    & workplace_missing\n",
    "    & residential_missing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457c6e8",
   "metadata": {},
   "source": [
    "From the above result, we can conclude that there doesn't appear to be a feature such that if that one is missing, then all the others are missing; this also means that for each entry, there's always at least one feature available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098d6e6",
   "metadata": {},
   "source": [
    "## Interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca2dd4",
   "metadata": {},
   "source": [
    "In this data, each language is mapped to the main country of usage except for English, where the language's usage is very high in multiple countries such that it couldn't be mapped to a single country. As such, for that language, we have most of the data missing.\n",
    "\n",
    "Per country, the pandemic timeline is represented, such as the first registered case, the first death, etc.\n",
    "\n",
    "Note that the paper says that nine languages are spoken in a single language, but we have more than that here (reason for that unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions = pd.read_csv(\"Data/interventions.csv\")\n",
    "\n",
    "\n",
    "def transform_column(column_name):\n",
    "    interventions[column_name] = pd.to_datetime(interventions[column_name])\n",
    "\n",
    "\n",
    "for intervention in interventions.columns[1:]:\n",
    "    transform_column(intervention)\n",
    "interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c18484",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e1bc7",
   "metadata": {},
   "source": [
    "Simply maps each considered article to the topics it is related to. A single article can be mapped to multiple topics. The number of articles per topic can be found in the [timeseries](#timeseries) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data takes a lot of time, and it's not really used right now, so there is no real point to load it for now\n",
    "# topics_linked = pd.read_csv(\"topics_linked.csv.xz\")\n",
    "# topics_linked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caec16f",
   "metadata": {},
   "source": [
    "## Data merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e716c70",
   "metadata": {},
   "source": [
    "First of all, let us keep from both mobility datasets only the countries that were considered in the original timeseries. We mapped the english language to Washington DC, the catalan language to Barcelona, and the Korean language both to Seoul and South Korea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_wiki_code = {\n",
    "    \"Japan\": \"ja\",\n",
    "    \"Italy\": \"it\",\n",
    "    \"Denmark\": \"da\",\n",
    "    \"Turkey\": \"tr\",\n",
    "    \"Norway\": \"no\",\n",
    "    \"Serbia\": \"sr\",\n",
    "    \"Sweden\": \"sv\",\n",
    "    \"Netherlands\": \"nl\",\n",
    "    \"Germany\": \"de\",\n",
    "    \"France\": \"fr\",\n",
    "    \"Barcelona\": \"ca\",\n",
    "    \"South Korea\": \"ko\",\n",
    "    \"Finland\": \"fi\",\n",
    "    \"District of Columbia\": \"en\",\n",
    "    \"Washington DC\": \"en\",\n",
    "    \"Seoul\": \"ko\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d915046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_countries_to_fine_grained(x):\n",
    "    if x == \"Spain\":\n",
    "        return \"ca\"\n",
    "    if x == \"United States\":\n",
    "        return \"en\"\n",
    "    if x == \"Seoul\":\n",
    "        return \"ko\"\n",
    "    return country_to_wiki_code[x]\n",
    "\n",
    "\n",
    "filtered_global_mobility_report = global_mobility_report[\n",
    "    global_mobility_report[\"country_region\"].isin(country_to_wiki_code)\n",
    "    | global_mobility_report[\"sub_region_1\"].isin(country_to_wiki_code)\n",
    "    | global_mobility_report[\"sub_region_2\"].isin(country_to_wiki_code)\n",
    "].copy()\n",
    "filtered_global_mobility_report.loc[\n",
    "    :, \"country_region_code\"\n",
    "] = filtered_global_mobility_report.country_region.apply(\n",
    "    lambda x: map_countries_to_fine_grained(x)\n",
    ")\n",
    "filtered_global_mobility_report.index = range(len(filtered_global_mobility_report))\n",
    "filtered_global_mobility_report.country_region.unique()  # There is data about all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to keep for each country only the capital/concerned city data from the\n",
    "cities_global_mobility_report = pd.DataFrame()\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_1\"] == \"Tokyo\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Copenhagen Municipality\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_1\"] == \"Berlin\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_1\"] == \"Oslo\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Barcelona\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Helsinki\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Paris\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"]\n",
    "            == \"Metropolitan City of Rome\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"metro_area\"] == \"Seoul Metropolitan Area\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Government of Amsterdam\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"metro_area\"]\n",
    "            == \"Belgrade Metropolitan Area\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Stockholm Municipality\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            (filtered_global_mobility_report[\"sub_region_1\"] == \"Ankara\")\n",
    "            & (filtered_global_mobility_report[\"sub_region_2\"].isnull())\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_2\"] == \"Barcelona\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = pd.concat(\n",
    "    [\n",
    "        cities_global_mobility_report,\n",
    "        filtered_global_mobility_report[\n",
    "            filtered_global_mobility_report[\"sub_region_1\"] == \"District of Columbia\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "cities_global_mobility_report = cities_global_mobility_report[\n",
    "    [\n",
    "        \"country_region_code\",\n",
    "        \"date\",\n",
    "        \"retail_and_recreation_percent_change_from_baseline\",\n",
    "        \"grocery_and_pharmacy_percent_change_from_baseline\",\n",
    "        \"parks_percent_change_from_baseline\",\n",
    "        \"transit_stations_percent_change_from_baseline\",\n",
    "        \"workplaces_percent_change_from_baseline\",\n",
    "        \"residential_percent_change_from_baseline\",\n",
    "    ]\n",
    "]\n",
    "cities_global_mobility_report = cities_global_mobility_report.rename(\n",
    "    columns={\"country_region_code\": \"language\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_apple_mobility = apple_mobility[\n",
    "    apple_mobility[\"region\"].isin(country_to_wiki_code)\n",
    "].copy()\n",
    "print(filtered_apple_mobility.region.unique())\n",
    "print(\"-----\")\n",
    "filtered_apple_mobility.region = filtered_apple_mobility.region.apply(\n",
    "    lambda x: country_to_wiki_code[x]\n",
    ")\n",
    "filtered_apple_mobility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54075540",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_apple_mobility = pd.DataFrame()\n",
    "for language in sum_data_df.language.unique():\n",
    "    tmp = (\n",
    "        filtered_apple_mobility[filtered_apple_mobility[\"region\"] == language]\n",
    "        .T.iloc[2:]\n",
    "        .copy()\n",
    "    )\n",
    "    tmp.columns = tmp.iloc[0]\n",
    "    tmp = tmp.iloc[1:]\n",
    "    tmp[\"language\"] = language\n",
    "    tmp[\"date\"] = tmp.index\n",
    "    new_apple_mobility = pd.concat([new_apple_mobility, tmp], axis=0)\n",
    "\n",
    "tmp_merge1 = sum_data_df.merge(new_apple_mobility, on=[\"language\", \"date\"])\n",
    "tmp_merge2 = tmp_merge1.merge(sum_environment_df, on=[\"language\", \"date\"])\n",
    "final_merge = tmp_merge2.merge(cities_global_mobility_report, on=[\"date\", \"language\"])\n",
    "final_merge[[\"driving\", \"transit\", \"walking\"]] = final_merge[\n",
    "    [\"driving\", \"transit\", \"walking\"]\n",
    "].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1104446",
   "metadata": {},
   "source": [
    "There is still some null data however, so we have to manage these features, either with interpolation or filling these values with the mean of the corresponding countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge[final_merge.transit.isnull()].groupby(\"language\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e4f4a",
   "metadata": {},
   "source": [
    "It appears that transit data from Apple is totally missing for the Korean, Serbian and Turkish parts; as such there is no way to replace them. Let us check for the other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96014d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge[\n",
    "    final_merge.grocery_and_pharmacy_percent_change_from_baseline.isnull()\n",
    "].groupby(\"language\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge[final_merge.grocery_and_pharmacy_percent_change_from_baseline.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993eef8",
   "metadata": {},
   "source": [
    "We will simply interpolate these 2 missing values with the nearest value; because it's both on the same day, and that day isn't the last one per language, the nearest value will be one for the same language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge[[\"grocery_and_pharmacy_percent_change_from_baseline\"]] = final_merge[\n",
    "    [\"grocery_and_pharmacy_percent_change_from_baseline\"]\n",
    "].interpolate(method=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41102b1b",
   "metadata": {},
   "source": [
    "## Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183db598",
   "metadata": {},
   "source": [
    "We can now try to do some regression analysis, with the number of environment views as output, and using the columns of *final_merge* as covariates, to see if we can fit some hyperplane to this data. First, let us transform the categorical language data using dummy variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge = pd.get_dummies(final_merge, drop_first=True)\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ef0cf",
   "metadata": {},
   "source": [
    "Then, we will normalize the input variables, so that each column has mean 0 and standard deviation 1 (note: maybe don't do this, use a different scaling, as the data isn't necessarily gaussian, but hey we'll see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_columns = list(final_merge.columns[2:])\n",
    "all_data_columns.remove(\"environment_views\")\n",
    "for column in all_data_columns:\n",
    "    final_merge[column] = (\n",
    "        final_merge[column] - final_merge[column].mean()\n",
    "    ) / final_merge[column].std()\n",
    "\n",
    "#final_merge[\"environment_views\"] = (\n",
    "#    final_merge[\"environment_views\"] - final_merge[\"environment_views\"].mean()\n",
    "#) / final_merge[\"environment_views\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277bb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"\"\n",
    "for covariate in all_data_columns:\n",
    "    formula = covariate + \"+\" + formula\n",
    "\n",
    "\n",
    "formula = formula[: len(formula) - 1]\n",
    "# formula = formula[120:]\n",
    "# formula = formula[72:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5f501",
   "metadata": {},
   "source": [
    "First we will use all covariates as predictors, with no interactions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=\"\"\"environment_views ~ \"\"\" + formula, data=final_merge)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029cf70",
   "metadata": {},
   "source": [
    "We're being told in the warnings that there either, there are strong multicollinearity problems, or the design matrix isn't invertible. This might be because of the \"dummy variable trap\" where, after using dummy encoding, many of the created features can be explained by the others. The condition number is also pretty high, which means that the matrix is easily perturbed by variation in the input data. We will try to fit a different model with less of the dummy variables to see if we can get rid of this issue.\n",
    "\n",
    "We also use less data points than we have; these correspond to the three languages that don't have the Apple transit data, i.e Turkish, Serbian and Korean. We will first try to get rid of that feature for the next regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45751f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_exclude_all = [\n",
    "    \"language_da\",\n",
    "    \"language_de\",\n",
    "    \"language_en\",\n",
    "    \"language_fi\",\n",
    "    \"language_fr\",\n",
    "    \"language_it\",\n",
    "    \"language_ja\",\n",
    "    \"language_ko\",\n",
    "    \"language_nl\",\n",
    "    \"language_no\",\n",
    "    \"language_sr\",\n",
    "    \"language_sv\",\n",
    "    \"language_tr\",\n",
    "]\n",
    "features_to_exclude = [\n",
    "    \"transit\",\n",
    "]\n",
    "data_columns = list(set(all_data_columns) - set(features_to_exclude))\n",
    "formula = \"\"\n",
    "for covariate in data_columns:\n",
    "    formula = covariate + \"+\" + formula\n",
    "\n",
    "\n",
    "formula = formula[: len(formula) - 1]\n",
    "# formula = formula[120:]\n",
    "# formula = formula[72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=\"\"\"environment_views ~ \"\"\" + formula, data=final_merge)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31922d31",
   "metadata": {},
   "source": [
    "Here, we got rid of the Apple transit feature, which makes us able to use all the data points we have. This gets rid of the warning that we used to have, meaning that we don't have multicollinearity problems anymore, and the data matrix is invertible. We can then now try to interpret the model.\n",
    "\n",
    "The R-squared is the same as before, i.e the part of explained variance is equal to 98.7%. Some of the features are not singificant, as their p-value is bigger than 0.05, so one can try to fit the model without these features.\n",
    "\n",
    "Looking at the covariates themselves, we see that it is useful to have the language data, as obviously the range of views from one country to another can change significantly as Wikipedia's importance isn't the same everywhere.\n",
    "\n",
    "From the Apple mobility data, it appears that the driving feature isn't statistically significant (0.221), while the walking one is. Surprisingly, it appears that walking is associated in a decrease of environmental views, which can be a bit counter-intuitive.\n",
    "\n",
    "From the statistically significant features from Global mobility data, it appears that a positive change in the residential pourcentage results in more environmental views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209b9f0-7f34-43a6-b637-ca9eb80c9706",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "With this first analysis, we can already extract some interesting insight about people's environmental (or lack thereof) awareness during the first Covid crisis: \n",
    "\n",
    "* we have shown that statistically speaking, there appears indeed to be a difference in how people visit these pages between 2019 and 2020, from January to July. \n",
    "* plotting these views as timeseries, we can sometimes see this change of pattern clearly, with an increase generally around March/April 2020\n",
    "* performing a regression analysis explains the data quiet well, and we can try different interactions between features to see if we can gain more insight that way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23f73e-ba2d-4cac-83d8-cdbfa29dad7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2 : The external datasets which we added by ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91089e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Precise Wikipedia Views Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c13ea7",
   "metadata": {},
   "source": [
    "The goal of this section is to precisely analyse the views of different wikipedia pages (Air pollution, Plastic pollution, and plastic production) from 01/01/2019 to 01/09/2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16ac57",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We restrict the page to the language studied in coronawiki\n",
    "languages = [\n",
    "    \"ja\",\n",
    "    \"it\",\n",
    "    \"da\",\n",
    "    \"tr\",\n",
    "    \"no\",\n",
    "    \"en\",\n",
    "    \"sr\",\n",
    "    \"sv\",\n",
    "    \"nl\",\n",
    "    \"de\",\n",
    "    \"fr\",\n",
    "    \"ca\",\n",
    "    \"ko\",\n",
    "    \"fi\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_langviews_data(path, columns_to_keep=None):\n",
    "    \"\"\"\n",
    "    Convert the csv generated by langviews for a particular wikipedia page into a usable dataframe keeping only the pages in the specified languages.\n",
    "    The resulting dataframe has a column per language and a new total column giving the total number of views across all the studied languages.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(path).transpose()\n",
    "    data.columns = data.iloc[0]\n",
    "    titles = data.iloc[1]\n",
    "    data.drop([\"Language\", \"Title\", \"Badges\"], inplace=True)\n",
    "\n",
    "    if columns_to_keep is None:\n",
    "        columns_to_keep = data.columns\n",
    "\n",
    "    columns = data.columns.intersection(columns_to_keep)\n",
    "    data = data[columns]\n",
    "\n",
    "    data[\"total\"] = data.sum(axis=1)\n",
    "    data[\"date\"] = data.index\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    data[\"year\"] = data.apply(lambda x: x.date.year, axis=1)\n",
    "\n",
    "    return titles, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64b7da",
   "metadata": {},
   "source": [
    "#### Air Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a203e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "airpol_titles, airpol_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-airpol.csv\", languages\n",
    ")\n",
    "airpol_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173be22",
   "metadata": {},
   "source": [
    "#### Plastic Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plastin_titles, plastin_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-plastin.csv\", languages\n",
    ")\n",
    "plastin_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5cf02",
   "metadata": {},
   "source": [
    "#### Plastic Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plastpol_titles, plastpol_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-plastpol.csv\", languages\n",
    ")\n",
    "plastpol_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc8125",
   "metadata": {},
   "source": [
    "### Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a562875",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_titles, covid_data = process_langviews_data(\n",
    "    \"Data/Additional/wikiviews/page-views-covid19.csv\", languages\n",
    ")\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042b51b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e878f",
   "metadata": {},
   "source": [
    "### Air Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972468db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(y=airpol_views.en, x=airpol_views.date)\n",
    "# sns.lineplot(y=airpol_views.da, x=airpol_views.date)\n",
    "sns.lineplot(y=airpol_data.total, x=airpol_data.date)\n",
    "plt.title(\"Air Pollution page views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4062ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=airpol_data.total, x=airpol_data.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dea3e0",
   "metadata": {},
   "source": [
    "It seems that during and after COVID, interest in air pollution decreases significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56328239",
   "metadata": {},
   "source": [
    "### Plastic Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac211b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y=plastin_data.total, x=plastin_data.date)\n",
    "plt.title(\"Plastic Industry page views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa769f",
   "metadata": {},
   "source": [
    "There are not enough views to conclude anything about awareness on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c35db",
   "metadata": {},
   "source": [
    "### Plastic Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y=plastpol_data.total, x=plastpol_data.date)\n",
    "plt.title(\"Plastic Pollution page views count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=plastpol_data.total, x=plastpol_data.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3e6bd-c89c-4b10-b844-eec5aa1a5c98",
   "metadata": {},
   "source": [
    "This concludes the extra data from Wikipedia that we add to the original Coronawiki dataset.\n",
    "Let us now present the next one : NO2 air pollution data by country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddb8bf",
   "metadata": {},
   "source": [
    "## Analysis of nitrogen dioxide levels by capital city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69346f8",
   "metadata": {},
   "source": [
    "Nitrogen dioxide is a toxic molecule that is typically emitted by industrial activities and car engines. It is therefore a very good indicator of human-induced air pollution.\n",
    "\n",
    "Here, we will focus on understanding the link between Covid and NO2 air pollution, i.e. between Covid and human-induced air pollution. To do so, we will make the following assumptions :\n",
    "- 2019 is the year that represents the \"usual activities\" before Covid.\n",
    "- The year of lockdowns and restrictions is then obviously 2020. We also consider that 2021-22 are the beginning of efforts towards going back to the normal, pre-Covid life. We do not assume that 2021 and 2022 have succeeded at being \"back to normal\".\n",
    "\n",
    "This might be a simplification of reality, but it has the two following advantages :\n",
    "- Covid began spreading worldwide during the very beginning of 2020. We can exploit this to our advantage by splitting years at the beginning of January, which is easy and fits our assumptions well.\n",
    "- It makes for very easy cross-coutries, cross-years comparisons.\n",
    "- Using instead lockdown data instead of year-by-year separations would weaken the possibility of comparing pollution by country, as pollution emissions are very dependent on the period of the year.\n",
    "\n",
    "The goal of this section is then to establish (or not) the fact that during Covid, the air got significantly cleaner, and to quantify the margin of improvement. We also consider the recovery phase after the initial Covid wave, i.e. after 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98c700",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd29fc",
   "metadata": {},
   "source": [
    "We use air pollution data from the different capitals from the World Air Quality Index dataset for the Covid-19 period (https://aqicn.org/data-platform/covid19/). This is a reliable dataset that was used in various large-scale studies about worldwide air pollution. This dataset is fairly lage, it includes the data for the years 2018 to 2022 for many cities of the world, and for various air polluting molecules.\n",
    "\n",
    "We reduce this to NO2 and only the fourteen capital cities in the notebook \"air_quality_by_capital.ipynb\", reducing the dataset to a much more manageable size, but not to the point of losing all interesting information.\n",
    "\n",
    "Note : we don't consider country-wide pollution because the dataset does not provide us with a way to merge all cities into one large blob for the country : we don't have the city size, the geographic proportion of the city in the country, etc. We therefore study capitals only, which the dataset certainly provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_datasets = \"Data/Additional/waqi/no2_capital/\"\n",
    "capitals = [\n",
    "    \"tokyo\",\n",
    "    \"rome\",\n",
    "    \"copenhagen\",\n",
    "    \"ankara\",\n",
    "    \"oslo\",\n",
    "    \"washington\",\n",
    "    \"stockholm\",\n",
    "    \"belgrade\",\n",
    "    \"amsterdam\",\n",
    "    \"berlin\",\n",
    "    \"paris\",\n",
    "    \"barcelona\",\n",
    "    \"seoul\",\n",
    "    \"helsinki\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_datasets = []\n",
    "for capital in capitals:\n",
    "    full_datasets.append((capital, pd.read_csv(path_to_datasets + capital + \".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_datasets[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc18fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2019 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    # keep year 2019\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2019\")].copy()\n",
    "    # remove year from date string to allow for inner merge on date with other years later on\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    # remove all other columns\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2019.append((capital, ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing as above, but for 2020\n",
    "ds_2020 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2020\")].copy()\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2020.append((capital, ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bce8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2021 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2021\")].copy()\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2021.append((capital, ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cef886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2022 = []\n",
    "for (capital, dataset) in full_datasets:\n",
    "    ds = dataset[dataset.Date.str.startswith(\"2022\")].copy()\n",
    "    ds[\"yearlessDate\"] = ds.Date.str[5:]\n",
    "    ds = ds[[\"yearlessDate\", \"median\"]]\n",
    "    ds_2022.append((capital, ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb1bfc",
   "metadata": {},
   "source": [
    "## Data viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c038b",
   "metadata": {},
   "source": [
    "We will then plot the SO3 measurements in all 14 cities every day for the years 2019, 2020 and 2021.\n",
    "\n",
    "We could also easily plot the current data for 2022, but we found that the resulting graph was fairly overloaded.\n",
    "\n",
    "We use a log scale because these cities have strong differences in air quality, and we want to show the full detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(14, sharex=True, sharey=True)\n",
    "fig.set_size_inches(18.5, 35.5)\n",
    "for i in range(14):\n",
    "    ds1 = ds_2019[i][1]\n",
    "    ds2 = ds_2020[i][1]\n",
    "    ds3 = ds_2021[i][1]\n",
    "    # not 2022, see above\n",
    "\n",
    "    ax[i].set_yscale(\"log\")\n",
    "    ax[i].set_title(ds_2019[i][0])\n",
    "    # always 2020 because we want all three graphs on one year, and 2020 is a leap year\n",
    "    (p2019,) = ax[i].plot_date(\n",
    "        matplotlib.dates.datestr2num(\"2020-\" + ds1[\"yearlessDate\"]),\n",
    "        ds1[\"median\"],\n",
    "        tz=\"UTC+1\",\n",
    "        fmt=\"-\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    (p2020,) = ax[i].plot_date(\n",
    "        matplotlib.dates.datestr2num(\"2020-\" + ds2[\"yearlessDate\"]),\n",
    "        ds2[\"median\"],\n",
    "        tz=\"UTC+1\",\n",
    "        fmt=\"-\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    (p2021,) = ax[i].plot_date(\n",
    "        matplotlib.dates.datestr2num(\"2020-\" + ds3[\"yearlessDate\"]),\n",
    "        ds3[\"median\"],\n",
    "        tz=\"UTC+1\",\n",
    "        fmt=\"-\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    ax[i].legend([p2019, p2020, p2021], [\"2019\", \"2020\", \"2021\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191d1d9",
   "metadata": {},
   "source": [
    "Some trends emerge :\n",
    "- 2019 (in blue) is usually a little bit above the others. When considering that this is a log scale, this is actually a fairly impressive difference.\n",
    "- 2020 is typically lower. We can almost always find a drop in March 2020, where the international community initially reacted to the virus.\n",
    "- 2020 and 2021 are somewhat more difficult to discern. It could be that these years are similar in terms of NO2 pollution.\n",
    "\n",
    "We can also check the evolution of mean pollution per country per year :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means2019 = [df[\"median\"].mean() for (capital, df) in ds_2019]\n",
    "means2020 = [df[\"median\"].mean() for (capital, df) in ds_2020]\n",
    "means2021 = [df[\"median\"].mean() for (capital, df) in ds_2021]\n",
    "means2022 = [df[\"median\"].mean() for (capital, df) in ds_2022]\n",
    "\n",
    "\n",
    "plt.title(\"Yearly average of daily pollution in capital cities\")\n",
    "\n",
    "for i in range(len(capitals)):\n",
    "    plt.plot(\n",
    "        [\"2019\", \"2020\", \"2021\", \"2022\"],\n",
    "        [means2019[i], means2020[i], means2021[i], means2022[i]],\n",
    "        linewidth=0.8,\n",
    "        label=capitals[i],\n",
    "    )\n",
    "\n",
    "plt.legend(loc=(1.05, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd77a05",
   "metadata": {},
   "source": [
    "Perhaps surprisingly, the graph is not U-shaped at all ! This means that on average, air pollution in the capital cities goes down steadily year after year. There is a caveat : the year 2022 is not over as we write these words. This means that the end of autumn and beginning of winter 2022 are not accounted for in this graph. If winter is more polluted than summer in 2022, then the graph above underestimates the pollution average for 2022.\n",
    "\n",
    "We can then quantify this evolution exactly, by using statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df659987",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf77fd",
   "metadata": {},
   "source": [
    "The testing we will perform consists of the following :\n",
    "- First, we merge the pollution data of two years\n",
    "- Then, for each capital city, we do a paired test comparing pairs of (day, day) of both years\n",
    "- The two days are always on the same date, except for the year. This enables us to compare pollution day by day, ignoring the effects of recurring seasonal pollution. This analysis works for 2022, as the missing data for the end of the year will be ignored in the other dataset, possibly making the compromise of increasing the variance of test statistics.\n",
    "\n",
    "We first merge years we want to compare. We will compare the following pairs : (2019-2020) to check if covid had an impact against the 2019 baseline, (2020-2021) to see if 2021 was a rebounce/drop from Covid, and (2020, 2022) for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697eb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged19_20 = []\n",
    "for index in range(len(capitals)):\n",
    "    (capital1, df1) = ds_2019[index]\n",
    "    (capital2, df2) = ds_2020[index]\n",
    "    assert capital1 == capital2\n",
    "    mergeTwoYears = pd.merge(df1, df2, on=\"yearlessDate\", how=\"inner\")\n",
    "    merged19_20.append((capitals[index], mergeTwoYears))\n",
    "\n",
    "merged20_21 = []\n",
    "for index in range(len(capitals)):\n",
    "    (capital2, df2) = ds_2020[index]\n",
    "    (capital3, df3) = ds_2021[index]\n",
    "    assert capital2 == capital3\n",
    "    mergeTwoYears = pd.merge(df2, df3, on=\"yearlessDate\", how=\"inner\")\n",
    "    merged20_21.append((capitals[index], mergeTwoYears))\n",
    "\n",
    "merged20_22 = []\n",
    "for index in range(len(capitals)):\n",
    "    (capital2, df2) = ds_2020[index]\n",
    "    (capital4, df4) = ds_2022[index]\n",
    "    assert capital2 == capital4\n",
    "    mergeTwoYears = pd.merge(df2, df4, on=\"yearlessDate\", how=\"inner\")\n",
    "    merged20_22.append((capitals[index], mergeTwoYears))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f6140",
   "metadata": {},
   "source": [
    "We can then do the daily paired test for the year pairs :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad37aa1",
   "metadata": {},
   "source": [
    "2019 to 2020 comparison :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_capital = []\n",
    "for capital, merger in merged19_20:\n",
    "    ttest = stats.ttest_rel(merger.median_x, merger.median_y, alternative=\"greater\")\n",
    "    alone_sig = ttest.pvalue < 0.05\n",
    "    bonf_sig = ttest.pvalue < (0.05 / 14)\n",
    "    evolution = (\n",
    "        \"{:+.2f}\".format(\n",
    "            ((np.mean(merger.median_y) / np.mean(merger.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "\n",
    "    test_by_capital.append((capital, len(merger), evolution, alone_sig, bonf_sig))\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_by_capital,\n",
    "    columns=[\n",
    "        \"Capital\",\n",
    "        \"numpoints\",\n",
    "        \"pollution_evolution\",\n",
    "        \"better_during_covid_significant\",\n",
    "        \"bonferroni_significant\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcae5e6",
   "metadata": {},
   "source": [
    "Here, we learn that for almost every capital city, the air was significantly cleaner in during Covid times than before. There is an exception for Ankara in Turkey, which is the only capital city that polluted more during Covid than before. Tokyo is not Bonferroni-significant, but also shows a drop in average pollution during Covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_capital = []\n",
    "for capital, merger in merged20_21:\n",
    "    # NOTE : we switch to two-sided testing because we are interested in both kinds of changes.\n",
    "    # Before, we were only interested in knowing whether covid was an improvement or not\n",
    "    ttest = stats.ttest_rel(merger.median_x, merger.median_y, alternative=\"two-sided\")\n",
    "    alone_sig = ttest.pvalue < 0.05\n",
    "    bonf_sig = ttest.pvalue < (0.05 / 14)\n",
    "    evolution = (\n",
    "        \"{:.2f}\".format(\n",
    "            ((np.mean(merger.median_y) / np.mean(merger.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "\n",
    "    test_by_capital.append((capital, len(merger), evolution, alone_sig, bonf_sig))\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_by_capital,\n",
    "    columns=[\n",
    "        \"Capital\",\n",
    "        \"numpoints\",\n",
    "        \"pollution_evolution\",\n",
    "        \"alone_significant\",\n",
    "        \"bonferroni_significant\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71ca84",
   "metadata": {},
   "source": [
    "Here, the results are much less significant, as the test could not manage to find a lot of cities where 2020 and 2021 were somehow different. We only have Belgrade (Serbia) and Helsinki (Finland) that showed a significant boost in pollution between 2020 and 2021. In this sense, the years 2020 and 2021 and very much alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_capital = []\n",
    "for capital, merger in merged20_22:\n",
    "    ttest = stats.ttest_rel(merger.median_x, merger.median_y, alternative=\"two-sided\")\n",
    "    alone_sig = ttest.pvalue < 0.05\n",
    "    bonf_sig = ttest.pvalue < (0.05 / 14)\n",
    "    evolution = (\n",
    "        \"{:.2f}\".format(\n",
    "            ((np.mean(merger.median_y) / np.mean(merger.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "\n",
    "    test_by_capital.append((capital, len(merger), evolution, alone_sig, bonf_sig))\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_by_capital,\n",
    "    columns=[\n",
    "        \"Capital\",\n",
    "        \"numpoints\",\n",
    "        \"pollution_evolution\",\n",
    "        \"alone_significant\",\n",
    "        \"bonferroni_significant\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965904f",
   "metadata": {},
   "source": [
    "This is perhaps the surprise of this study. We find that between 2020 and 2022, the only significant changes show that the cities are doing better in 2022 in terms of air pollution, the only exception being Barcelona (Spain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec46bc",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c0d7d",
   "metadata": {},
   "source": [
    "Let us then conclude about the NO2 air pollution. We have established the following :\n",
    "- Air pollution typically goes down with time in the fourteen capital cities we studied.\n",
    "- The year 2020 is special, as it shows a massive drop worldwide. We attribute this to Covid.\n",
    "- The air pollution typically does not go back up after Covid. This is surprising, as one might expect air pollution to go back up after the lockdowns are finished. This is not the case.\n",
    "\n",
    "There are a few limitations in this study :\n",
    "- We do not study pollution by country, but by capital city. If we want to draw nationwide conclusions, we need to assume that the pollution of the capital city is a good proxy for the country. This may or may not be the case.\n",
    "- We assumed that we could split the timeline (before, during, after) as (2019, 2020, 2021+). This is a decent compromise in order to be able to compare the evolution country-wise, but it can be a bit rough.\n",
    "\n",
    "This concludes this study about air pollution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3f0f4-42c9-400a-95e5-42ed4bd7c91b",
   "metadata": {},
   "source": [
    "## Plastic pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_plastic = pd.read_csv(\"Data/Additional/plastic/global-plastics-production.csv\")\n",
    "# source : https://ourworldindata.org/plastic-pollution\n",
    "global_plastic = global_plastic.rename(\n",
    "    columns={\"Global plastics production\": \"Global_plastics_production\"}\n",
    ")\n",
    "global_plastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb19e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd = pd.read_excel(\"Data/Additional/plastic/oecd_source.xlsx\")\n",
    "# source: https://www.oecd.org/newsroom/plastic-pollution-is-growing-relentlessly-as-waste-management-and-recycling-fall-short.htm\n",
    "oecd = oecd.dropna().rename(\n",
    "    columns={\n",
    "        \"Unnamed: 0\": \"Region\",\n",
    "        \"Unnamed: 1\": \"2020\",\n",
    "        \"Unnamed: 2\": \"2019\",\n",
    "        \"Unnamed: 3\": \"pre-COVID 2020 projection\",\n",
    "        \"Unnamed: 4\": \"Absolute 2020 versus 2019\",\n",
    "    }\n",
    ")\n",
    "oecd = oecd.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102583f-7859-45f6-90f9-50be03d7d751",
   "metadata": {},
   "source": [
    "## Looking at the difference between the amount of plastic produced in 2020 and 2019 per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f22e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=oecd, x=\"Absolute 2020 versus 2019\", y=\"Region\").set(title=\"difference between the amount of plastic produced in 2020 and 2019 per region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718608e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont have 2020 data here\n",
    "sns.lineplot(data=global_plastic, x=\"Year\", y=\"Global_plastics_production\").set(title=\"global plastic production per year from 1950 to 2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dc183-bca8-47b0-a8c2-df2ffd20cb9d",
   "metadata": {},
   "source": [
    "Now we want to fit a regression model in order to predict what will be the global plastic production in 2020 given the data from 1950 to 2019, the goal is to see how much higher that number would have been if covid was not there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(\n",
    "    formula=\"Global_plastics_production ~ +np.exp(Year/1950) +Year \",\n",
    "    data=global_plastic,\n",
    ")\n",
    "model = mod.fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69712b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = global_plastic[\"Year\"]\n",
    "plt.plot(X, model.predict(X), color=\"k\", label=\"Regression model\")\n",
    "\n",
    "plt.xlabel('year') \n",
    "plt.ylabel('model prediction') \n",
    "plt.title('The regression model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00041b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(Year=2020)\n",
    "\n",
    "d2020 = pd.DataFrame(data, index=[0])\n",
    "plastic_prediction_2020_if_not_covid = model.predict(d2020)\n",
    "real_plastic_prodction_2020=oecd['2020'][49]*10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poucentage_de_reduction=((real_plastic_prodction_2020 - plastic_prediction_2020_if_not_covid) / plastic_prediction_2020_if_not_covid) * 100\n",
    "print(poucentage_de_reduction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9f13b-e8db-45dd-a89a-ae2942228169",
   "metadata": {},
   "source": [
    "We can see that compared to our prediction the amount of plastic produced in 2020 is 0.06% smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244540ad-3007-47f2-add6-358b4601f89c",
   "metadata": {},
   "source": [
    "## Studying the plastic production in the EU\n",
    "In the first part we study the plastic produced for food packaging, Then the whole amount of plastic produced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_plastic_packaging = pd.read_csv(\n",
    "    \"Data/Additional/plastic/sts_inpr_m__custom_3782744_linear.csv\"\n",
    ")\n",
    "# source= https://ec.europa.eu/eurostat/databrowser/view/STS_INPR_M/default/table?lang=en production of plastic packages in europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_nace_r2_c222 = eu_plastic_packaging[eu_plastic_packaging[\"nace_r2\"] == \"C2222\"].sort_values(\"TIME_PERIOD\")\n",
    "\n",
    "for region in set(eu_nace_r2_c222[\"geo\"]):\n",
    "    df = eu_nace_r2_c222[eu_nace_r2_c222[\"geo\"] == region]\n",
    "    if len(df[\"OBS_VALUE\"].dropna()) == 0:\n",
    "        continue\n",
    "\n",
    "    df.plot.line(\n",
    "        x=\"TIME_PERIOD\", y=\"OBS_VALUE\", title=\"plastic produced for packaging in \" +region, rot=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51cd4b-6e74-4d82-8b92-62954f12b322",
   "metadata": {},
   "source": [
    "Notice the drop around 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9083727",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = eu_plastic_packaging[\n",
    "    (eu_plastic_packaging[\"nace_r2\"] == \"C2222\")\n",
    "    & (eu_plastic_packaging[\"TIME_PERIOD\"] >= \"2020-01\")\n",
    "]\n",
    "old = eu_plastic_packaging[\n",
    "    (eu_plastic_packaging[\"nace_r2\"] == \"C2222\")\n",
    "    & (eu_plastic_packaging[\"TIME_PERIOD\"] < \"2020-01\")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ae4ce50",
   "metadata": {},
   "source": [
    "EL is greece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb3ba9-4948-4e8c-9a55-013b7fbcd664",
   "metadata": {},
   "source": [
    "for each region we compare the production before and after covid to see how the pandemic affected the production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in set(\n",
    "    eu_plastic_packaging[eu_plastic_packaging[\"nace_r2\"] == \"C2222\"][\"geo\"]\n",
    "):\n",
    "    if region=='IE' or region=='UK' or region=='EU28': # we have no data for these \n",
    "        continue\n",
    "    print(\n",
    "        \"difference in the mean ammount of plastic produced before and after covid in \"+region,\n",
    "        stats.ttest_ind(\n",
    "            new[new[\"geo\"] == region][\"OBS_VALUE\"].dropna(),\n",
    "            old[old[\"geo\"] == region][\"OBS_VALUE\"].dropna(),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a81d79-7a37-4d35-90e0-9561aca4def8",
   "metadata": {},
   "source": [
    "it seems for France and Germany the data shows a significant drop in production, while for spain Greece and turkey we see an increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacture of plastics products by month by country\n",
    "full_plastic_production = pd.read_csv(\n",
    "    \"Data/Additional/plastic/sts_inpr_m__custom_3857183_linear.csv\"\n",
    ")\n",
    "# source: https://ec.europa.eu/eurostat/databrowser/view/STS_INPR_M__custom_3857183/default/table?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eceb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_plastic_production = full_plastic_production[\n",
    "    full_plastic_production[\"TIME_PERIOD\"] >= \"2018-01\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cda3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_c222 = full_plastic_production[full_plastic_production[\"nace_r2\"] == \"C222\"].sort_values(\"TIME_PERIOD\")\n",
    "for region in set(r2_c222[\"geo\"]):\n",
    "    df = r2_c222[r2_c222[\"geo\"] == region]\n",
    "    if len(df[\"OBS_VALUE\"].dropna()) == 0:\n",
    "        continue\n",
    "\n",
    "    df.plot.line(x=\"TIME_PERIOD\", y=\"OBS_VALUE\", title=\"plastic produced in \" +region, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56478d4",
   "metadata": {},
   "source": [
    "notice how the drop during 2020 usually wider than the others and slightly deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff860e-300b-457c-bab2-39ed755b585d",
   "metadata": {},
   "source": [
    "## Study of the recycling rates and recovery rates\n",
    "The goal here is to see how the recycling and recovery rates were affected by covid, since if we see a drop in plastic production it would be hard to state that it would lead to a drop in plastic pollution without studying first how covid impacted plastic management facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b9cff",
   "metadata": {},
   "source": [
    "source: https://ec.europa.eu/eurostat/databrowser/view/TEN00063__custom_3793752/default/table?lang=en\n",
    "\n",
    "Recycling rates for packaging waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eee22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recycling_rates_eu = pd.read_csv(\n",
    "    \"Data/Additional/plastic/ten00063__custom_3793752_linear.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62242c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we filter for plastic waste\n",
    "Recycling_rates_eu = Recycling_rates_eu[Recycling_rates_eu[\"waste\"] == \"W150102\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(\n",
    "    Recycling_rates_eu[Recycling_rates_eu[\"TIME_PERIOD\"] == 2020][\"OBS_VALUE\"],\n",
    "    Recycling_rates_eu[Recycling_rates_eu[\"TIME_PERIOD\"] == 2019][\"OBS_VALUE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fdb7c-b351-4bbb-bc4c-4206f8983469",
   "metadata": {},
   "source": [
    "The data shows that the recycling rates are unaffected by covid, thus we can conclude that the recycling rates did not change from 2019 to 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c787dc",
   "metadata": {},
   "source": [
    "Rate of recovery or incineration at waste incineration plants with energy recovery’ for the purposes of Article 6(1) of Directive 94/62/EC means the total quantity of packaging waste recovered or incinerated at waste incineration plants with energy recovery, divided by the total quantity of generated packaging waste https://ec.europa.eu/eurostat/databrowser/view/ten00062/default/table?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_rates_eu = pd.read_csv(\"Data/Additional/plastic/ten00062_linear.csv\")\n",
    "recovery_rates_eu = recovery_rates_eu[recovery_rates_eu[\"waste\"] == \"W150102\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(\n",
    "    recovery_rates_eu[recovery_rates_eu[\"TIME_PERIOD\"] == 2020][\"OBS_VALUE\"],\n",
    "    recovery_rates_eu[recovery_rates_eu[\"TIME_PERIOD\"] == 2019][\"OBS_VALUE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360170d-3e64-467f-9052-46c641decfda",
   "metadata": {},
   "source": [
    "The data shows that the recovery rates are unaffected by covid, thus we can conclude that the recovery rates did not change from 2019 to 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c9bb3-86e7-4669-b92d-3ed036c2a01a",
   "metadata": {},
   "source": [
    "## Study of the waste generated by households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste = pd.read_csv(\"Data/Additional/plastic/ten00110_linear.csv\")\n",
    "# https://ec.europa.eu/eurostat/databrowser/view/TEN00110/default/table?lang=en&category=env.env_was.env_wasgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste = eu_waste[eu_waste[\"waste\"] == \"TOTAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste[eu_waste[\"TIME_PERIOD\"] == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=eu_waste, x=\"TIME_PERIOD\", y=\"OBS_VALUE\").set(title=\"Household waste generated per year in the EU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56945501-9d5f-447d-b4b6-e799ccf54535",
   "metadata": {},
   "source": [
    "We can see a drop of 0.5 in 2020, however, due to the low amount of data the confidence intervals are quite large, so it would be hard to perform statistical tests and get a meaningful result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65314c1c-173c-481c-bf60-7bd034953900",
   "metadata": {},
   "source": [
    "## Study of the total amount of waste generated by households and businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_waste = pd.read_csv(\"Data/Additional/plastic/ten00108_linear.csv\")\n",
    "eu_waste = eu_waste[eu_waste[\"waste\"] == \"TOTAL\"]\n",
    "# https://ec.europa.eu/eurostat/databrowser/view/TEN00108/default/table?lang=en&category=env.env_was.env_wasgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=eu_waste, x=\"TIME_PERIOD\", y=\"OBS_VALUE\").set(title=\"Total amount of waste generated by the EU and buisnesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337db5f-5e58-4f9a-b0de-ef481ceb4e1b",
   "metadata": {},
   "source": [
    "We can see a drop of almost 1 in 2020, however, due to the low amount of data the confidence intervals are quite large, so it would be hard to perform statistical tests and get a meaningful result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0055a1b-b8a1-4f9d-b591-4289279adddb",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "We saw that in some countries the plastic production experienced a significant decrease during covid, especially around 2020 we saw quite a drop probably due to lockdown. However, recycling rates and recovery rates did not experience significant change, More over The amount of waste generated decreased between 2018 and 2020 although one might question the validity of this due to a low amount of data.\n",
    "\n",
    "Therefore with those results one might conclude that since less plastic was produced and with no impact on the recycling and recovery rates this might suggest that covid caused a drop plastic polution in the EU.\n",
    "\n",
    "\n",
    "This analysis can be improved since one main issue is that the EU data lacks the data for some countries like Switzerland Poland and so on.\n",
    "This causes some confidence intervals to inflate and thus makes some hypothesis hard to test.\n",
    "We mainly focused on the EU and therefore we can improve it by adding other regions of the world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bf5e7-faef-4a4b-a267-cdc16201d5e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 3 : Studying the awareness versus the actual pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849de983-5e1d-4856-89de-81f53f0562a5",
   "metadata": {},
   "source": [
    "The goal is to establish whether there is a link between awareness (i.e. Wikipedia views) and actual ground measurements about pollution. We will perform two experiments for each country : \n",
    "- **intervention analysis** : we find the peak of wikipedia views for a given wikipedia subject or page in 2020 (the peak of awareness, which we call the intervention) and check whether this peak translates to a significant change in empirical pollution. \n",
    "- **granger causality testing** : we test whether a given timeseries (wikipedia views) can be used to linearly predict the future of another timeseries (say, pollution). This gives us a hint about the temporal relationship between two observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbe441-1fb2-4344-a4e9-5471f247944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = sum_environment_df.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2986f1-4322-4082-858a-02c956405922",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e34ac-0504-417e-82d7-4ebad4aa8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_language_to_dataset = {\n",
    "    'ja' : full_datasets[0], \n",
    "    'it' : full_datasets[1], \n",
    "    'da' : full_datasets[2],\n",
    "    'tr' : full_datasets[3], \n",
    "    'no' : full_datasets[4], \n",
    "    'en' : full_datasets[5], \n",
    "    'sr' : full_datasets[7], #careful : we swap indices here \n",
    "    'sv' : full_datasets[6], \n",
    "    'nl' : full_datasets[8], \n",
    "    'de' : full_datasets[9], \n",
    "    'fr' : full_datasets[10], \n",
    "    'ca' : full_datasets[11], \n",
    "    'ko' : full_datasets[12], \n",
    "    'fi' : full_datasets[13]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f39d1c-4348-426e-86ed-d78de5289874",
   "metadata": {},
   "outputs": [],
   "source": [
    "{(k, v[0]) for (k, v) in wikipedia_language_to_dataset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4829d-d7c8-46bc-8eca-34926074c233",
   "metadata": {},
   "source": [
    "First, we will perform intervention analysis. The first thing we need is to find the peaks of awareness in all fourteen countries in the environment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ac122-2064-4d2a-8049-d557f7bb014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak2020 = {}\n",
    "for code in country_codes:\n",
    "    datapoints = sum_environment_df[sum_environment_df.language == code]\n",
    "    datapoints = datapoints[datapoints.date.dt.year == 2020]\n",
    "    peak = datapoints.environment_views.max()\n",
    "    peakline = datapoints[datapoints.environment_views == peak].iloc[0]\n",
    "    peak2020[code] = peakline['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38a1bf-de5a-479b-9e27-cbd9ad578bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d00ca-1c2c-4361-961b-22f227a39903",
   "metadata": {},
   "source": [
    "Now that we have all peaks, we can look at the 365 previous days and the 365 following years in air pollution and analyze whether there is a significant difference in air quality. This will give us an idea of the relationship between awareness of environment problems and the actual state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf3e1c-5ff9-4cc6-840a-8190f58acb34",
   "metadata": {},
   "source": [
    "#### Can wikipedia awareness be used to model an intervention in the air pollution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e35ce4-9a81-4996-9cf4-5826e84d51e9",
   "metadata": {},
   "source": [
    "At first, let us visualize the data which we analyze : we want to see whether there is a significant difference of air quality before and after the wikipedia views peak of 2020. We will look at both the full timeseries with a change of color at the date of the peak, along with a boxplot to show the difference in quartile-based statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d420217-5ff8-408c-b3db-fec04cac55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for code in country_codes:\n",
    "    dataset = wikipedia_language_to_dataset[code][1]\n",
    "    dataset[\"dt\"] = pd.to_datetime(dataset.Date)\n",
    "    dataset[\"yearless\"] = dataset.Date.str[5:]\n",
    "    peak = peak2020[code]\n",
    "    \n",
    "    before = dataset[(dataset[\"dt\"] >= peak - datetime.timedelta(weeks=52)) & (dataset[\"dt\"] < peak)][['Date', 'median']]\n",
    "    after  = dataset[(dataset[\"dt\"] < peak + datetime.timedelta(weeks=52)) & (dataset[\"dt\"] >= peak)][['Date', 'median']]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(code)\n",
    "    fig.set_size_inches(13, 5)\n",
    "    ax1.plot_date(matplotlib.dates.datestr2num(before[\"Date\"]), before['median'], tz=\"UTC+1\", fmt=\"-\")\n",
    "    ax1.plot_date(matplotlib.dates.datestr2num(after[\"Date\"]), after['median'], tz=\"UTC+1\", fmt=\"-\")\n",
    "    \n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    ax2.boxplot([before[\"median\"], after[\"median\"]])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f2ec0-f827-4991-a1d9-8eedc8a82c66",
   "metadata": {},
   "source": [
    "Here's a few interesting results by themselves : \n",
    "- First of all, most countries visibly have less air pollution after the peak of their environment Wikipedia page of 2020. We will analyze this further in the next cell. That alone is however not enough to explain the whole story : both periods of 365 days have strong similarities\n",
    "- Analyzing in that direction, there seems to be a group of countries that stand out : Japan, Italy, Turkey, Norway, the U.S., Serbia, the Netherlands, Norway, Korea and Finland all have a air pollution that is U-shaped, meaning they are highly seasonal and binary (summer = no pollution, winter = strong pollution). France and Germany also show a similar behavior, but with a seemingly higher variance. This is hardly the case for the last two : Danemark and Sweden are less well-behaved in terms a pollution seasonality. Looking at it globally, we can conclude that there is a strong seasonality of pollution wordlwide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d424d-85f2-4344-8f18-eff683960ab2",
   "metadata": {},
   "source": [
    "Then, we can dig into the significance of these results : is it generally true that the peak of awareness marks a significant difference in air quality in a given country ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5d5e2-3876-4c22-aa32-197122187446",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for code in country_codes:\n",
    "    dataset = wikipedia_language_to_dataset[code][1]\n",
    "    dataset[\"dt\"] = pd.to_datetime(dataset.Date)\n",
    "    dataset[\"yearless\"] = dataset.Date.str[5:]\n",
    "    peak = peak2020[code]\n",
    "    \n",
    "    before = dataset[(dataset[\"dt\"] >= peak - datetime.timedelta(weeks=52)) & (dataset[\"dt\"] < peak)][['yearless', 'median']]\n",
    "    after  = dataset[(dataset[\"dt\"] < peak + datetime.timedelta(weeks=52)) & (dataset[\"dt\"] >= peak)][['yearless', 'median']]\n",
    "    \n",
    "    yearComp = before.merge(after, on='yearless')\n",
    "    pval = stats.ttest_rel(yearComp['median_x'], yearComp['median_y']).pvalue\n",
    "    evolution = (\n",
    "        \"{:+.2f}\".format(\n",
    "            ((np.mean(yearComp.median_y) / np.mean(yearComp.median_x)) - 1) * 100\n",
    "        )\n",
    "        + \"%\"\n",
    "    )\n",
    "    sig_alone = pval < 0.05\n",
    "    sig_bonf = pval < 0.05/14\n",
    "    results.append((code, pval, evolution, sig_alone, sig_bonf))\n",
    "    \n",
    "pd.DataFrame(results, columns = ['country', 'pvalue', 'evolution', 'sig_alone', 'sig_bonferonni'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f52b60-0492-4cc0-88e1-5d858fb50841",
   "metadata": {},
   "source": [
    "These results are in line with the rest of the air pollution analysis earlier : countries significantly reduced their air pollution between these two periods. The only exception is again Turkey. This [Wikipedia page](https://en.wikipedia.org/wiki/Air_pollution_in_Turkey#Nitrogen_oxides) gives a bit of an explanation : NOx car pollution and lack of pollution regulation are a large part of the problem. We however could not explain why the pollution goes up instead of stagnating, for example. Serbia also shows a slight increase in air pollution, but this is not a significant change from before the peak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b66c7-0db1-4d28-85b8-1ecdd4006e6b",
   "metadata": {},
   "source": [
    "#### What can we conclude from this analysis of view peak vs. air pollution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c27fc3-f9a0-49ec-951e-53ff880aa845",
   "metadata": {},
   "source": [
    "We can establish that for the capitals of most countries studied here, **air pollution can reasonably be used as an intervention in air pollution**. However, this is not a causal analysis : we cannot really claim that awareness made the pollution go down, because of two related remarks. First, there is a strong confounder : Covid probably made people read the wikipedia page about air pollution, and at the same time made them pollute less. Second, we cannot establish causality in such sequential data, as we would need to also witness an alternative universe where covid did not happen. It is however not out of the question that awareness causes a drop in pollution : being locked down during Covid, people may have had more time to learn and think about their environment.\n",
    "\n",
    "Bad news aside, we did learn some facts : air pollution gets significantly better after people are most aware about the problem of air pollution in their respective country. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613edef-2e0b-472b-8324-cb1d6f5d55f7",
   "metadata": {},
   "source": [
    "### Daily views vs actual pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53e94e-89d2-427c-adb3-4b9b0640a8ae",
   "metadata": {},
   "source": [
    "We can also ask a related but different question : is it true that the value of wikipedia views of the environment topic can linearly predict the air pollution ? \n",
    "\n",
    "Here, the Granger test can help us. The [Granger causality test](https://en.wikipedia.org/wiki/Granger_causality) is a hypothesis testing tool that takes two time-series as input and checks whether the first one is a good linear predictor for the values of the next. It enables us to do some lag analysis, for example verifying whether the past of the first time series can predict the value of the other at a future day. Here's the implementation we will use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd453ee7-d55c-469c-8355-35f51633f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "grangercausalitytests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dd9e7-2af7-4c43-b96d-ee8b7c64ad78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "802e5aad-2ed6-4e67-9d05-2acb80d2f6d8",
   "metadata": {},
   "source": [
    "Since we are concerned with data that goes beyond what the original Coronawiki dataset gives us, we can load the extended version that goes further than 2020.\n",
    "\n",
    "We loaded all wikipedia views on the environment topic from 2015-07 to the end of 2022. This extends the data in both directions, giving us more datapoints to merge with the air quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c602fa-b123-46db-9328-45ff447d8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_extended = pd.read_csv(\"Data/Additional/langviews/final/aggregated_views.csv\")\n",
    "sum_environment_extended.date = pd.to_datetime(sum_environment_extended.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341c43b-984c-4c0f-960c-bf5455c37d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_environment_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8015487-dc75-4e68-b5ff-b7a41062d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for code in country_codes:\n",
    "    \n",
    "    env = sum_environment_extended[[code, 'date']]\n",
    "    air = wikipedia_language_to_dataset[code][1]\n",
    "    merged = env.merge(air, left_on=\"date\", right_on=\"dt\")\n",
    "    \n",
    "    print(code)\n",
    "                                                           #jesus the access patterns are annoying\n",
    "    pval = grangercausalitytests(merged[['median', code]], [1])[1][0]['ssr_chi2test'][1]\n",
    "    alone_sig = pval < 0.05\n",
    "    bonf_sig = pval < 0.05/14\n",
    "    \n",
    "    res.append((code, pval, alone_sig, bonf_sig))\n",
    "    \n",
    "    print(\"\\n\\n===================\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d5d90-fbbf-4740-bb35-e8b0c2f44102",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_poll_granger = pd.DataFrame(res)\n",
    "air_poll_granger.columns = [\"code\", \"pvalue\", \"alone_sig\", \"bonf_sig\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1595f-c846-45c8-aaa9-8584b9dff4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_poll_granger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2cd53-2072-47ee-b8fe-13643aacda21",
   "metadata": {},
   "source": [
    "The p-values are very binary : either something is not significant alone, or it is significant even under the Bonferonni correction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d42c0-905b-479f-b400-7e0b9c244574",
   "metadata": {},
   "outputs": [],
   "source": [
    "(air_poll_granger.alone_sig == air_poll_granger.bonf_sig).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2034f-ebaf-4d02-8c83-957d0554f5c1",
   "metadata": {},
   "source": [
    "In other words, there is no need for a finer correction like Benjamin-Hochberg FDR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee2285-d99b-4a49-8e67-b2f319ed3520",
   "metadata": {},
   "source": [
    "We find that for most countries, past wikipedia views make for a good linear predictor of the future of air pollution. This even holds for Turkey where the air pollution got worse during Covid.\n",
    "\n",
    "An interesting case is that of Japan and South Korea which have very insignificant p-values (>.2), suggesting that day-to-day linear prediction is not very convincing for these two countries. We note that these are the only Eastern-Asian countries in the dataset. An interesting extension to this project could be to check whether this extends further to other countries in the area.\n",
    "\n",
    "The odd one out is then Serbia, with a p-value of 0.21. There is an explanation which we find satisfying : while air pollution follows the same patterns as similar European countries, the wikipedia views on the environment topic in Serbia are fairly unusual when compared to neighboring countries. Indeed, there is a massive spike around 2020-04, the time at which Covid hit the country. Previously, the article had significnatly fewer views. For most other countries, the process was much more continuous. We see no particular reason as to why the awareness differs, but this explains why the Granger test sees no significant use of wikipedia views for predicting the air pollution.\n",
    "\n",
    "For all others, the linear prediction works out fine. The model is confident that the past of the wikipedia views is a useful tool to predict the air pollution of the next day in the capital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c63e5-095a-4bee-b31a-b1c829b7db67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc4f5c91-b457-452e-ae1a-1389a0d3bcf2",
   "metadata": {},
   "source": [
    "# Part 4 : What if ? and what happens next ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184b6a8-2329-48c9-9c97-cd580eac0872",
   "metadata": {},
   "source": [
    "In this final part, we want to create a hypothetical scenario of 2022 using statistical forecasting without the data from that year. The idea is to show whether the direction air pollution is taking is predictable, and where it is headed.\n",
    "\n",
    "For the statistical forecasting, we will use the [SARIMA model](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average#Variations_and_extensions) which enables the prediction of the future of a timeseries by using the previous data points and accounting for seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abcd27e-0776-45a4-b93d-52543da76d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e0184-f67c-4eab-96ef-687adb4ab358",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_list = []\n",
    "for country in country_codes : \n",
    "    try:\n",
    "        print(\"=========================================================\")\n",
    "        print(country, \"\\n\") \n",
    "        \n",
    "        dataset = wikipedia_language_to_dataset[country][1].copy()\n",
    "        dataset = dataset[dataset[\"dt\"].dt.year <= 2021]\n",
    "        dataset = dataset.set_index(\"dt\")\n",
    "        dataset = dataset[\"median\"]\n",
    "        decomposition = sm.tsa.seasonal_decompose(dataset.resample('MS').mean(), model='additive')\n",
    "        fig = decomposition.plot()\n",
    "        plt.show()\n",
    "\n",
    "        mod = sm.tsa.statespace.SARIMAX(dataset.resample('MS').mean(),\n",
    "                                        order=(0, 1, 1),\n",
    "                                        seasonal_order=(1, 1, 1, 12))\n",
    "        results = mod.fit(method = 'powell')\n",
    "\n",
    "        pred = results.get_prediction(start=pd.to_datetime('2022-01-01'), end = pd.to_datetime('2023-01-01'), dynamic=False)\n",
    "        pred.predicted_mean.plot()\n",
    "        dataset = wikipedia_language_to_dataset[country][1].copy()\n",
    "        dataset = dataset.set_index(\"dt\")\n",
    "        dataset = dataset[\"median\"]\n",
    "        dataset = dataset.rename(\"actual_mean\")\n",
    "        dataset.resample('MS').mean().plot()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        dataset_2022 = wikipedia_language_to_dataset[country][1]\n",
    "        dataset_2022 = dataset_2022[dataset_2022[\"dt\"].dt.year > 2021]\n",
    "        dataset_2022 = dataset_2022.set_index(\"dt\")\n",
    "        dataset_2022 = dataset_2022[\"median\"].resample('MS').mean()\n",
    "        \n",
    "        joined_true_and_pred = pd.concat([pred.predicted_mean, dataset_2022], axis = 1, join = 'inner')\n",
    "        mae = mean_absolute_error(joined_true_and_pred[\"median\"], joined_true_and_pred[\"predicted_mean\"])\n",
    "        \n",
    "        expectedMean = dataset_2022.mean()\n",
    "        predMean = pred.predicted_mean.mean()\n",
    "        relative_diff = (\n",
    "            \"{:+.2f}\".format(\n",
    "                ((predMean / expectedMean) - 1) * 100\n",
    "            )+ \"%\"\n",
    "        )\n",
    "        sarima_list.append((country, expectedMean, predMean, relative_diff, mae))\n",
    "        print(\"\\n\\n\\n\")\n",
    "    except:\n",
    "        print(\"not enough data for sarima!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77927fbb-1d4c-4163-8ee3-415996d6c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimaDf = pd.DataFrame(sarima_list, columns = [\"Country\", \"expected\", \"prediction\", \"relative_diff\", \"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab9fd8-377f-4bca-a15f-b85cefff2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f613e71-97bd-4841-aed5-cfdaf1e06ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimaDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76adf20b-2d58-47ed-a27b-3621145c83ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
